"""Tests for memory optimization features."""

import gc
import os
import resource

import psutil
import pytest
import torch


def get_rss_mb() -> float:
    """Get current RSS in MB."""
    return psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)


def get_maxrss_mb() -> float:
    """Get current process max RSS (Resident Set Size) in MB."""
    # resource.getrusage returns maxrss in bytes on Linux, KB on macOS
    import platform

    usage = resource.getrusage(resource.RUSAGE_SELF)
    if platform.system() == "Darwin":
        # macOS returns bytes
        return usage.ru_maxrss / (1024 * 1024)
    else:
        # Linux returns KB
        return usage.ru_maxrss / 1024


class TestTrellisMoEMLPEagerBuffers:
    """Tests for TrellisMoEMLP eager buffer creation."""

    @pytest.fixture
    def mock_moe_layer(self):
        """Create a small mock MoE layer for testing."""
        from metal_marlin.trellis.testing import create_mock_moe_mlp

        return create_mock_moe_mlp(
            hidden_dim=256,
            intermediate_dim=512,
            num_experts=4,
            num_experts_per_tok=2,
            bits=3,
            device="mps",
        )

    @pytest.mark.skipif(not torch.backends.mps.is_available(), reason="MPS not available")
    def test_eager_buffers_creates_metal_buffers(self, mock_moe_layer):
        """Test that _get_cached_buffers creates Metal buffers."""
        # Initially no cached buffers
        assert mock_moe_layer._cached_weight_buffers is None

        # Force buffer creation via _get_cached_buffers
        mock_moe_layer._get_cached_buffers()

        assert mock_moe_layer._cached_weight_buffers is not None

    @pytest.mark.skipif(not torch.backends.mps.is_available(), reason="MPS not available")
    def test_eager_buffers_frees_stacked_tensors(self, mock_moe_layer):
        """Test that stacked tensors remain available after buffer creation.

        Note: The current implementation keeps stacked tensors for the slow path
        fallback. This test verifies they exist and are properly formed.
        """
        mock_moe_layer._get_cached_buffers()

        # Stacked tensors should still exist (for slow path fallback)
        assert hasattr(mock_moe_layer, "gate_weights_stacked")
        assert mock_moe_layer.gate_weights_stacked is not None


class TestMemoryBaseline:
    """Baseline memory tests (without model loading)."""

    @pytest.mark.skipif(not torch.backends.mps.is_available(), reason="MPS not available")
    def test_cpu_to_metal_buffer_no_mps_copy(self):
        """Test that CPUâ†’Metal doesn't create MPS copy."""
        import Metal

        from metal_marlin.metal_dispatch import mps_tensor_to_metal_buffer

        device = Metal.MTLCreateSystemDefaultDevice()

        gc.collect()
        torch.mps.empty_cache()
        mps_before = torch.mps.current_allocated_memory()

        # Create CPU tensor, move to MPS, then convert to Metal buffer
        t_cpu = torch.randn(1000, 1000, dtype=torch.float32)
        t_mps = t_cpu.to("mps")
        torch.mps.synchronize()

        mps_after_move = torch.mps.current_allocated_memory()

        # Convert MPS tensor to Metal buffer - this should NOT create another MPS copy
        buf = mps_tensor_to_metal_buffer(t_mps, device)
        torch.mps.synchronize()

        mps_after_buf = torch.mps.current_allocated_memory()

        # The buffer conversion should not significantly increase MPS allocation
        # Allow small overhead but not a full 4MB copy
        delta = mps_after_buf - mps_after_move
        assert delta < 1024 * 1024, (  # < 1MB increase
            f"MPS allocation increased by {delta / (1024*1024):.2f}MB during buffer creation"
        )


@pytest.mark.skipif(
    not os.path.exists("models/GLM-4.7-Flash-Trellis-3bpw"),
    reason="Model not available"
)
class TestRealModelMemory:
    """Integration tests with real model."""

    def test_model_load_memory_bounded(self):
        """Test that model loading stays within memory bounds."""
        from metal_marlin.trellis.model import TrellisForCausalLM

        gc.collect()
        torch.mps.empty_cache()

        before_rss = get_rss_mb()

        # Load with memory optimization
        model = TrellisForCausalLM.from_pretrained(
            "models/GLM-4.7-Flash-Trellis-3bpw",
            device="mps",
            optimize_memory=True,
        )

        gc.collect()
        torch.mps.synchronize()
        torch.mps.empty_cache()
        gc.collect()

        after_rss = get_rss_mb()
        delta_gb = (after_rss - before_rss) / 1024

        # Should be under 8 GB increase for 3-bit model
        # (4 GB weights + 2-3 GB buffers + overhead)
        assert delta_gb < 10, f"Memory increased by {delta_gb:.2f} GB, expected < 10 GB"

        # Verify model works
        x = torch.randn(1, 2048, dtype=torch.float16, device="mps")
        with torch.no_grad():
            # Just run first layer as smoke test
            layer = model.model.layers[2]  # First MoE layer
            out = layer(x)
            assert not torch.isnan(out).any()


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
