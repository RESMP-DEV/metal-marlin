{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM-4.7-Flash-MoE Quickstart\n",
    "\n",
    "This notebook demonstrates end-to-end inference with **GLM-4.7-Flash** (Mixture of Experts model) using Metal Marlin's FP4 quantization on Apple Silicon.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Model**: `zai-org/GLM-4.7-Flash` (MoE architecture)\n",
    "- **Quantization**: 4-bit FP4 with group size 128\n",
    "- **Device**: Apple Metal (MPS)\n",
    "- **Features**: Streaming inference, memory tracking, token metrics\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- macOS 13.0+ with Apple Silicon\n",
    "- Python 3.11 or 3.12\n",
    "- Installed via: `uv sync --extra all`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from metal_marlin.inference.pipeline_v2 import TransformersMarlinPipeline\n",
    "from metal_marlin.transformers_loader import load_and_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Quantize Model\n",
    "\n",
    "This step:\n",
    "1. Downloads the GLM-4.7-Flash model from HuggingFace\n",
    "2. Replaces `nn.Linear` layers with `MetalQuantizedLinear`\n",
    "3. Quantizes weights to FP4 format (4 bits per parameter)\n",
    "\n",
    "**Expected**: ~3-5 GB memory usage after quantization (vs ~9 GB for BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"zai-org/GLM-4.7-Flash\"\n",
    "BITS = 4\n",
    "GROUP_SIZE = 128\n",
    "FORMAT = \"fp4\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model, stats = load_and_quantize(\n",
    "    MODEL_NAME,\n",
    "    bits=BITS,\n",
    "    group_size=GROUP_SIZE,\n",
    "    format=FORMAT,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "print(\"\\n‚úÖ Quantization Complete\")\n",
    "print(f\"  Quantized layers: {stats.get('quantized_count', 'N/A')}\")\n",
    "print(f\"  Skipped layers: {stats.get('skipped_count', 'N/A')}\")\n",
    "print(f\"  Compression ratio: {stats.get('compression_ratio', 0):.2f}x\")\n",
    "print(f\"  Original size: {stats.get('original_bytes', 0) / 1024**3:.2f} GB\")\n",
    "print(f\"  Quantized size: {stats.get('quantized_bytes', 0) / 1024**3:.2f} GB\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(f\"  MPS memory: {torch.mps.current_allocated_memory() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "\n",
    "The pipeline handles tokenization, generation, and streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TransformersMarlinPipeline(model, tokenizer)\n",
    "print(\"‚úÖ Pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Prompt Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts model is in 2 sentences.\"},\n",
    "]\n",
    "\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "# Stream tokens\n",
    "start = time.perf_counter()\n",
    "response = \"\"\n",
    "for token in pipeline.chat(messages, max_tokens=256, temperature=0.7, stream=True):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    response += token\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "token_count = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "\n",
    "print(f\"\\n\\nüìä Metrics: {token_count} tokens in {elapsed:.2f}s ({token_count/elapsed:.1f} tok/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Conversation\n",
    "\n",
    "Demonstrate context retention across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "]\n",
    "\n",
    "print(\"Turn 1\")\n",
    "print(\"User:\", history[-1][\"content\"])\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "response1 = \"\"\n",
    "for token in pipeline.chat(history, max_tokens=128, temperature=0.7, stream=True):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    response1 += token\n",
    "print()\n",
    "\n",
    "history.append({\"role\": \"assistant\", \"content\": response1})\n",
    "history.append({\"role\": \"user\", \"content\": \"What's a famous landmark there?\"})\n",
    "\n",
    "print(\"\\nTurn 2\")\n",
    "print(\"User:\", history[-1][\"content\"])\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "response2 = \"\"\n",
    "for token in pipeline.chat(history, max_tokens=128, temperature=0.7, stream=True):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    response2 += token\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference (Non-Streaming)\n",
    "\n",
    "Generate multiple completions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Write a haiku about AI.\",\n",
    "    \"Name 3 programming languages.\",\n",
    "    \"What is 15 * 23?\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    print(f\"\\nPrompt {i}: {prompt}\")\n",
    "    print(\"Response: \", end=\"\", flush=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for token in pipeline.chat(messages, max_tokens=128, temperature=0.7, stream=True):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        response += token\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    current = torch.mps.current_allocated_memory() / 1024**3\n",
    "    driver = torch.mps.driver_allocated_memory() / 1024**3\n",
    "    print(f\"MPS Current: {current:.2f} GB\")\n",
    "    print(f\"MPS Driver: {driver:.2f} GB\")\n",
    "else:\n",
    "    print(\"MPS not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "Measure throughput across different token lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_configs = [\n",
    "    {\"prompt\": \"Hi\", \"max_tokens\": 64, \"label\": \"Short (64 tokens)\"},\n",
    "    {\"prompt\": \"Explain quantum computing\", \"max_tokens\": 128, \"label\": \"Medium (128 tokens)\"},\n",
    "    {\"prompt\": \"Write a story about a robot\", \"max_tokens\": 256, \"label\": \"Long (256 tokens)\"},\n",
    "]\n",
    "\n",
    "print(\"\\nüèÅ Benchmark Results\\n\")\n",
    "print(f\"{'Config':<25} {'Tokens':<10} {'Time (s)':<12} {'Throughput (tok/s)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for config in test_configs:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": config[\"prompt\"]},\n",
    "    ]\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    response = \"\"\n",
    "    for token in pipeline.chat(messages, max_tokens=config[\"max_tokens\"], temperature=0.7, stream=True):\n",
    "        response += token\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    token_count = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "    throughput = token_count / elapsed\n",
    "    \n",
    "    print(f\"{config['label']:<25} {token_count:<10} {elapsed:<12.2f} {throughput:<20.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try different quantization formats**: `fp8`, `int4`, `int3`, `int2`\n",
    "2. **Experiment with group sizes**: 64, 128, 256\n",
    "3. **Test other models**: Any HuggingFace model with `AutoModelForCausalLM`\n",
    "4. **Deploy as API server**: See `examples/perf_dashboard.py` for OpenAI-compatible serving\n",
    "\n",
    "## CLI Alternative\n",
    "\n",
    "For command-line usage:\n",
    "\n",
    "```bash\n",
    "# Single prompt\n",
    "python examples/glm4_flash_inference.py --prompt \"Hello, how are you?\"\n",
    "\n",
    "# Interactive mode\n",
    "python examples/glm4_flash_inference.py --interactive\n",
    "\n",
    "# Custom parameters\n",
    "python examples/glm4_flash_inference.py \\\n",
    "  --prompt \"Explain AI\" \\\n",
    "  --max-tokens 512 \\\n",
    "  --temperature 0.9 \\\n",
    "  --top-p 0.95\n",
    "```\n",
    "\n",
    "## Documentation\n",
    "\n",
    "- **README**: `README.md`\n",
    "- **API Reference**: `docs/api_reference.md`\n",
    "- **Benchmarks**: `benchmarks/`\n",
    "- **Tests**: `tests/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
