# Metal Marlin Status

**Last Updated:** 2026-02-08

## Summary

| Component | Status |
|-----------|--------|
| Test Suite | **5427 tests collected** âœ… |
| GEMM Kernel | **Working + Optimized** âœ… (2.4x speedup) |
| MoE Infrastructure | **Complete** âœ… (batched expert kernels wired) |
| EXL3 Quantization | **Complete** âœ… (trellis + viterbi pipeline) |
| **Trellis Inference** | **Complete** âœ… (11 modules, 3500 LOC, fused GEMM ~50x speedup) |
| **Async Dispatch Batching** | **Complete** âœ… (LayerBatchContext, 1.29x speedup) |
| Qwen3-4B FP4 Inference | **PyTorch MPS fallback** ~27 tok/s |
| GLM-4.7-Flash MoE | **Verified** âœ… (end-to-end generation working, ~0.28 tok/s) |
| OpenAI Server | **Complete** âœ… (30 tests, paged attention via CLI) |
| Metal Shaders | **65 shaders** âœ… (precompiled metallib) |
| Vision Preprocessing | **Complete** âœ… (16 kernels wired) |
| Phase 80 Cleanup | **Complete** âœ… (int16â†’uint8, dead code removal) |
| ASR/ANE Modules | **Removed** âœ… (cleanup complete; legacy ASR benchmarks fail-fast) |
| Top-level API Compatibility | **Patched** âœ… (`MetalQuantizedLinear` export restored) |
| Ruff Linting | **101 warnings** âš ï¸ |
| Pyright Errors | **0 errors, 215 warnings** âœ… |
| **C++ Extension** | âœ… **Working** (needs HeapAllocator bindings) |

---

## Remaining Work to Complete

> This section tracks what's needed before the project is production-ready.

### C++ Extension Status: âœ… Working

The `_cpp_ext` nanobind module now builds and imports successfully.

**Available exports:**
- `BatchDispatch`, `BufferHandle`, `BufferPool`, `EncoderCache`
- `LibraryManager`, `ManagedBuffer`, `MetalContext`, `MetalDevice`
- `QueueManager`, `TokenGroupManager`, `TransientRingBuffer`
- `create_buffer`, `dispatch_kernel`, etc.

**Build:**
```bash
cd contrib/metal_marlin/build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j
cp _cpp_ext.cpython-312-darwin.so ../metal_marlin/
```

**Fixes applied (2026-02-02):**
| Issue | Resolution |
|-------|------------|
| Duplicate `python_bindings.cpp/mm` | Deleted `.cpp`, kept `.mm` (ObjC++) |
| `MTLFunctionRef` undefined | Changed to `MTL::Function*` (Metal-cpp type) |
| Missing ObjC Metal types | Added `#import <Metal/Metal.h>` |
| `BufferPtr` methods undefined | Removed `inline` keyword for external linkage |
| `HeapAllocator` type mismatch | Temporarily disabled (use Python fallback) |

**Minor remaining work:**
- Re-enable `HeapAllocator` bindings with proper type bridging
- Pure-Python `MetalHeapAllocator` works as drop-in replacement

### High Priority

| Task | Description | Impact |
|------|-------------|--------|
| **Unify MoE dispatch paths** | Route all dispatches through AsyncCommandBufferManager | Consistent batching |
| **Flash Attention v3** | Chunked prefill for long contexts | 2-3x prefill speedup at >4K |
| **Continuous batching** | KV cache sharing across requests | 5-10x throughput |
| **Speculative decoding** | Draft-verify architecture | 2-3x decode speed |

### Medium Priority

| Task | Description | Impact |
|------|-------------|--------|
| **Wire C++ extension to inference** | Use `_cpp_ext` for kernel dispatch | 5-10x dispatch speedup |
| **INT8 KV cache** | Quantized attention cache | 2x context length |
| **MoE kernel fusion** | Fuse router + expert GEMM | Reduce per-layer latency |
| **Decode GEMV optimization** | Single-token decode kernel | Target: 10+ tok/s |
| **Auto-detect metallib staleness** | Warn if shaders modified | Prevent silent bugs |

### Low Priority (Developer Experience)

| Task | Description | Impact |
|------|-------------|--------|
| **Profiling dashboard** | Real-time kernel timing | Easier optimization |
| **Model registry** | Centralized model configs | Less hardcoding |
| **Codebase consolidation** | Reduce benchmark/task sprawl | Cleaner repo |

### Files Requiring Attention

**Integration (wire C++ extension to inference):**
- [metal_marlin/metal_dispatch.py](metal_marlin/metal_dispatch.py) - Use `_cpp_ext.dispatch_kernel` when available
- [metal_marlin/kernels.py](metal_marlin/kernels.py) - Replace PyObjC dispatch with C++ path
- [metal_marlin/trellis_dispatch.py](metal_marlin/trellis_dispatch.py) - Add fast path for C++ extension

**Tests:**
- [tests/test_cpp_extension.py](tests/test_cpp_extension.py) - Tests verify extension loads and functions work

---

## Trellis Inference Pipeline

**Status:** âœ… Complete (Phase 50-57)

Standalone inference for trellis-quantized models with Metal acceleration.

### Components (11 modules, ~3500 LOC)

| Module | Lines | Purpose | Status |
|--------|-------|---------|--------|
| `trellis_config.py` | 71 | Model configuration (GLM-4.7-Flash defaults) | âœ… |
| `trellis_attention.py` | 275 | MLA with KV compression | âœ… |
| `kv_cache.py` (`TrellisKVCache`) | - | Compressed KV cache (8x memory savings) | âœ… |
| `trellis_layer.py` | 125 | Dense MLP with SwiGLU | âœ… |
| `trellis_linear.py` | 356 | Quantized linear with Metal dequant | âœ… |
| `trellis_loader.py` | 505 | Layer-wise model loading | âœ… |
| `trellis_model.py` | 473 | Complete model (MoE + dense) | âœ… |
| `trellis_lm.py` | 192 | Language model wrapper | âœ… |
| `trellis_generate.py` | 881 | Generation with streaming/sampling | âœ… |
| `trellis_moe.py` | 98 | MoE routing module | âœ… |
| `trellis_dispatch.py` | 364 | Metal dequantization dispatch | âœ… |

### Test Coverage (7 files, 50 tests)

| Test File | Tests | Status |
|-----------|-------|--------|
| `test_trellis_attention.py` | 3 | âœ… Pass |
| `test_trellis_generate.py` | 6 | âš ï¸ 4 pass, 2 annotation issues |
| `test_trellis_loader.py` | 16 | âš ï¸ Needs safetensors |
| `test_trellis_model.py` | 4 | âœ… Pass |
| `test_trellis_moe.py` | 19 | â­ï¸ Skip (needs model files) |
| `test_trellis_quality.py` | 4 | âš ï¸ Needs safetensors |
| `test_trellis.py` | ~100 | âœ… Core tests pass |

### Features

- **MLA (Multi-head Latent Attention)**: Compresses KV cache via low-rank decomposition
- **MoE Support**: 64 routed + 1 shared expert (GLM-4.7-Flash architecture)
- **Metal Acceleration**: On-the-fly dequantization via custom shaders
- **Streaming**: Token-by-token generation with `stream_generate()`
- **HuggingFace Tokenizers**: Native compatibility via transformers

### Loading Trellis Models

```python
from metal_marlin.trellis_model import TrellisModel
from metal_marlin.trellis_generate import TrellisGenerator, GenerationConfig

model = TrellisModel.from_pretrained("models/GLM-4.7-Flash-3bpw")
tokenizer = AutoTokenizer.from_pretrained("zai-org/GLM-4.7-Flash")
generator = TrellisGenerator(model, tokenizer)

output = generator.generate("Hello!", GenerationConfig(max_new_tokens=100))
```

---

## Metal Acceleration Status

### Complete
- [x] GEMM (marlin_gemm.metal) - FP4/INT4/INT8 dequant + matmul
- [x] Attention (flash_attention.metal) - Flash attention v1/v2
- [x] RoPE (rope.metal) - YaRN and standard RoPE
- [x] Sampling (sampling.metal) - argmax, top-k, top-p
- [x] Trellis Dequantization (dequant_trellis.metal) - Packed uint8 unpacking to FP16
- [x] Fused Trellis GEMM (gemm_trellis.metal) - Combined dequant+GEMM with on-the-fly unpacking

### Phase 70: Metallib Precompilation âœ…

**Status:** Complete

Precompiled Metal shaders for 100-1000x faster kernel dispatch:

| Component | Status |
|-----------|--------|
| `build_metallib.sh` | âœ… Compiles 65 shaders to metallib |
| `metallib_loader.py` | âœ… Python loader with caching |
| Version tracking | âœ… `.metallib_version` file |
| Incremental builds | âœ… Only recompile changed shaders |
| Documentation | âœ… `docs/internals/metallib_architecture.md` |

**Build output:**
- File: `metal_marlin/lib/metal_marlin.metallib` (2.7 MB)
- Shaders: 65 kernels
- Metal version: 3.0

**Performance:**
| Dispatch Method | Latency |
|-----------------|---------|
| Precompiled metallib | ~0.01 ms |
| JIT (first call) | 50-100 ms |
| JIT (cached) | ~0.1 ms |

### Remaining Work
- [ ] Flash attention v3 (chunked prefill)
- [ ] Speculative decoding kernels
- [ ] Continuous batching optimization

### Trellis Inference (Phase 70)

| Kernel | Purpose | Status |
|--------|---------|--------|
| `dequant_trellis.metal` | Weight dequantization | âœ… Working |
| `gemm_trellis.metal` | Fused dequant+GEMM | âœ… Working |

**Performance (M4 Max, GLM-4.7-Flash experts):**

| Shape | Bits | Reference | Fused | Speedup |
|-------|------|-----------|-------|---------|
| 1x2048x1536 | 3 | 145.2ms | 2.8ms | 51.9x |
| 32x2048x1536 | 3 | 162.4ms | 4.2ms | 38.7x |
| 128x2048x1536 | 3 | 189.6ms | 12.5ms | 15.2x |

**End-to-end (GLM-4.7-Flash Trellis mixed-precision):**
- Decode: ~0.28 tok/s (after async batch fix, up from 0.22 tok/s)
- Prefill throughput: TBD
- Memory: ~8-9 GB (mixed 2-6 bit quantized)

### Remaining on CPU/MPS
- Image preprocessing (scipy.ndimage)
- ONNX graph execution
- Model loading (safetensors)

---

## Async Dispatch Batching

**Status:** âœ… Complete (2026-02-08)

Layer batching reduces command buffer commits by grouping dispatches across multiple
transformer layers into a single Metal command buffer.

### Components

| Module | Purpose | Status |
|--------|---------|--------|
| `AsyncCommandBufferManager` | Batches kernel dispatches into shared command buffer | âœ… Working |
| `LayerBatchContext` | Groups N layers per commit (default: 4) | âœ… Working |
| `MixedBPWMoEDispatcher` | Grouped dispatch for mixed-precision experts | âœ… Working |
| `dispatch_immediate()` | Fallback unbatched dispatch | âœ… Working |
| `ensure_batch_active()` | Recovers batch state if lost | âœ… Working |
| `has_active_batch()` | Query batch state | âœ… Working |

### Performance

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Decode (tok/s) | 0.22 | 0.28 | **1.29x** |
| Commits per forward | ~58 | ~12 | **4.8x reduction** |

### Analysis Reports

- [Batch Dispatch Analysis](docs/reports/batch_dispatch_analysis.md) â€” lib.dispatch() interference analysis
- [Attention Batch Analysis](docs/reports/attention_batch_analysis.md) â€” CopyBackBuffer and commit() tracing
- [Batch Fix Benchmark](docs/reports/batch_fix_benchmark.md) â€” Before/after performance data

---

## Metal MoE Kernels

| Kernel | Status | Notes |
|--------|--------|-------|
| `moe_trellis_swiglu` | âœ… Working | Fused MoE GEMM with SwiGLU |
| Slow fallback | âœ… Working | Memory-optimized sequential path |

### Performance (GLM-4.7-Flash mixed-precision)

| Metric | Before Batching | After Batching | Improvement |
|--------|----------------|----------------|-------------|
| Decode (tok/s) | 0.22 | 0.28 | **1.29x** |
| GPU commits/fwd | ~58 | ~12 | **4.8x fewer** |

**Notes:**
- Fast path uses `moe_trellis_swiglu` kernel with batched expert dispatch via `AsyncCommandBufferManager`
- `MixedBPWMoEDispatcher` groups experts by BPW (2/3/4-bit) and dispatches in batched Metal command buffers
- `LayerBatchContext` accumulates dispatches across 4 consecutive MoE layers before committing
- MoE compute remains ~98.5% of forward-pass time â€” kernel fusion is the next optimization frontier
- Memory usage dominated by model weights (~15 GB for 3 BPW), constant across context lengths

---

## Model Compatibility (Detailed)

| Model | Size | Memory | Speed | Status |
|-------|------|--------|-------|--------|
| **Qwen/Qwen3-4B** | 4B | ~2GB FP4 | ~27 tok/s | âœ… Fully Working |
| **zai-org/GLM-4.7-Flash** | 30B-A3B MoE | ~15GB FP4 | ~0.28 tok/s | âœ… MoE + MLA Verified |
| Llama-3.1-8B | 8B | ~4GB FP4 | ~20 tok/s (est.) | âœ… Working |
| Mixtral-8x7B | 47B | ~24GB FP4 | MoE optimized | âœ… Working |

---

## Test Results

**Last verified:** 2026-02-06
**Test files:** 126

| Category | Count |
|----------|-------|
| Collected | 5427 |
| Collection Errors | 0 |
| Collection Warnings | 2 (`benchmark`, `vision` unknown marks) |

**Optional dependency tests:** (skipped when dependencies missing)
- `test_all_models.py` - requires `transformers`
- `test_trellis.py` LayerStreamer section - requires `safetensors`

**Recent additions:**
- EXL3 quantization pipeline (trellis + viterbi)
- Kernel optimization scripts (`optimize_kernel.py`, `optimize_all_kernels.py`)
- Streaming quantization (`quantize_streaming.py`)
- New test files for trellis, viterbi, scatter/gather, sampling

**Test suite changes:**
- Removed legacy/ directory and all legacy model implementations
- Cleaned up test_inference.py (removed MetalGLM47Model tests)
- Added EXL3 quantization tests

---

## Test Suite Structure

**Test files:** 126
**Tests collected:** 5427

### Recent Additions

| Test File | Purpose |
|-----------|---------|
| `test_trellis.py` | Trellis quantization algorithms |
| `test_viterbi_quant.py` | Viterbi optimal quantization |
| `test_scatter_gather_metal.py` | Metal scatter/gather kernels |
| `test_sampling_metal.py` | Metal sampling kernels |
| `test_moe_dispatch_metal.py` | MoE dispatch kernels |

### Completed Cleanup

- âœ… Consolidated MoE kernel/integration tests into `test_moe.py`
- âœ… Merged Hadamard kernel tests into `test_hadamard.py`
- âœ… Deprecated legacy model tests in favor of Transformers integration
- âœ… Standardized pytest markers across all files
- âœ… Eliminated duplicate test coverage

---

## EXL3 Quantization Pipeline

**Status:** âœ… Complete

EXL3-style trellis quantization with Viterbi optimal search:

### Components

| Module | Purpose | Status |
|--------|---------|--------|
| `exl3_pipeline.py` | End-to-end quantization workflow | âœ… |
| `exl3_quantizer.py` | Core quantizer with LDLQ | âœ… |
| `ldl_decomp.py` | LDL decomposition (faster than Cholesky) | âœ… |
| `ldlq.py` | LDLQ tile quantization | âœ… |
| `trellis_codebook.py` | Trellis state machine codebook | âœ… |
| `trellis_tile.py` | Tensor-core permutation | âœ… |
| `viterbi_quant.py` | Viterbi optimal quantization | âœ… |
| `hadamard_preprocess.py` | Outlier dispersal rotation | âœ… |
| `hessian_streaming.py` | Memory-efficient Hessian collection | âœ… |
| `calibration_streamer.py` | Batched calibration data | âœ… |
| `layer_streamer.py` | Layer-wise weight streaming | âœ… |

### Features

- **Layer-wise streaming**: One layer in memory at a time
- **Parallel tile quantization**: Multi-threaded Viterbi search
- **RAM-aware calibration**: Automatic batch sizing
- **LDL decomposition**: 2x faster than Cholesky
- **Hadamard rotation**: Outlier dispersal for better quantization

### Usage

```bash
# Quantize a model with EXL3 pipeline
uv run python3 -m metal_marlin.quantization.exl3_pipeline \
    --model Qwen/Qwen3-4B \
    --output ./qwen3_4b_exl3 \
    --bits 4 \
    --calibration wikitext
```

---

## GLM-4.7-Flash Integration

**Status:** âœ… Working via Transformers integration.

GLM-4.7-Flash is a **Mixture-of-Experts (MoE)** model with:
- Layer 0: Dense MLP
- Layers 1-46: MoE (64 routed experts + 1 shared expert per layer)
- Multi-Latent Attention (MLA) throughout

**Current State:**
- âœ… Quantization works correctly (9024 expert weights quantized)
- âœ… MLA attention dimensions fixed
- âœ… MoE infrastructure wired (`MetalQuantizedMoE`, `replace_moe_layers()`)
- âœ… Shared expert kernel (`moe_shared_expert_fp4`) integrated
- âœ… Inference working via Transformers + layer replacement

**How it works:** Transformers 5.0+ provides `Glm4MoeLiteForCausalLM` which handles
MoE routing natively. We swap `nn.Linear` â†’ `MetalQuantizedLinear` for quantized GEMM.

```bash
# Requires transformers 5.0.0+
RUN_GLM47_TRANSFORMERS=1 uv run pytest tests/test_glm47_transformers.py -v
```

---

## Inference Pipeline

Inference uses PyTorch MPS fallback (not fused Metal kernels):

```bash
cd contrib/metal_marlin
uv run python3 -c "
from metal_marlin.inference.pipeline import MarlinPipeline
pipe = MarlinPipeline.from_pretrained('benchmarks/results/qwen3_4b_fp4', device='mps')
print(pipe('The capital of France is', max_tokens=20))
"
```

**Requires:** `transformers` package installed.

| Metric | Value |
|--------|-------|
| Backend | PyTorch MPS (fallback) |
| Target | Fused Metal kernels (~100 tok/s) |

---

## Architecture

**Metal Marlin integrates with HuggingFace Transformers, not reimplements it.**

Model structure (MoE routing, MLA attention, layer ordering) comes from Transformers.
We swap `nn.Linear` â†’ `MetalQuantizedLinear` to use optimized Metal kernels.

```python
from transformers import AutoModelForCausalLM
from metal_marlin import replace_linear_layers, MetalQuantizedLinear

# Transformers handles architecture (MoE, MLA, everything)
model = AutoModelForCausalLM.from_pretrained("zai-org/GLM-4.7-Flash", device_map="mps")

# Our only job: swap Linear layers with quantized versions
replace_linear_layers(model, MetalQuantizedLinear, bits=4, group_size=128)

# Inference uses Transformers' battle-tested code
output = model.generate(input_ids)
```

**Requires:** `transformers>=5.0.0` for GLM-4.7-Flash native support.

### Fused Trellis Inference

TrellisLinear uses fused Metal kernels that perform dequantization during GEMM:

| Phase | Kernel | Description |
|-------|--------|-------------|
| Prefill (M>16) | `gemm_trellis_packed` | Fused dequant+GEMM, 64x64 tiles |
| Decode (Mâ‰¤16) | `gemm_trellis_packed_decode` | Decode-optimized, 32x128 tiles |

**Memory Efficiency**: Weights are never fully materialized. Dequantization happens
in simdgroup registers during the GEMM mainloop.

**Expected Performance** (M3 Max):
- Model load: ~15GB GPU memory
- Decode: 5-10 tok/s (vs 0.1 tok/s with dequant+matmul)
- Prefill: 40-80 tok/s at 2K context

---

## Implementation Progress

### Metal Kernels (Core Value)

| Kernel | Purpose | Status |
|--------|---------|--------|
| `marlin_gemm_fp4` | FP4 dequant + GEMM fused | âœ… Working |
| `gemm_fp4_optimized` | Tuned GEMM variant | âœ… 2.4x speedup |
| `flash_attention_v2` | Memory-efficient attention | âœ… Working |
| `moe_shared_expert_fp4` | Shared expert GEMM (GLM-4.7) | âœ… Wired |
| `moe_expert_gemm_fp4` | Batched routed expert GEMM | âœ… Wired |
| `viterbi_quant` | Optimal quantization search | âœ… Working |
| `hessian` | Hessian computation | âœ… Working |
| `cholesky` | Cholesky decomposition | âœ… Working |
| `hadamard` | Hadamard transform | âœ… Working |
| `dense_gemm` | BF16/FP16 GEMM | âœ… Working |
| `moe_dispatch_optimized` | GPU token grouping | âœ… Available |
| `moe_router_fused` | Fused routing kernel | âœ… Available |

### MoE Infrastructure (Phase 42)

| Component | Status | Notes |
|-----------|--------|-------|
| `MetalQuantizedMoE` | âœ… Done | Holds packed expert weights |
| `find_moe_layers()` | âœ… Done | Auto-detect MoE in any model |
| `quantize_moe_experts()` | âœ… Done | Quantize 3D expert weight tensors |
| `replace_moe_layers()` | âœ… Done | Swap MoE layers with quantized versions |
| `moe_shared_expert_fp4()` | âœ… Done | Metal kernel for shared expert |
| `transformers_loader.py` | âœ… Done | MoE-aware model loading |
| Batched expert dispatch | âœ… Done | `moe_expert_gemm_fp4_grouped` Metal kernel wired |

### EXL3 Quantization (Phase 44)

| Component | Status | Notes |
|-----------|--------|-------|
| `EXL3Quantizer` | âœ… Done | Main quantizer class |
| `ldlq_quantize_layer` | âœ… Done | LDLQ layer quantization |
| `TrellisCodebook` | âœ… Done | State machine codebook |
| `viterbi_quant` | âœ… Done | Optimal search with Metal kernel |
| `hadamard_preprocess` | âœ… Done | Outlier dispersal |
| `StreamingHessianCollector` | âœ… Done | Memory-efficient calibration |
| Metal `viterbi_quant` kernel | âœ… Done | GPU-accelerated search |

See [docs/metal_kernel_audit.md](docs/metal_kernel_audit.md) for full kernel inventory.

### Legacy Model Layers (Removed)

> **Note:** Legacy model implementations have been removed. All models should use
> Transformers + `replace_linear_layers()` for inference.

**Deleted:**
- `metal_marlin/legacy/` - All files
- `metal_marlin/models/llama.py` - QuantizedLlama* classes
- `metal_marlin/models/qwen3.py` - QuantizedQwen3* classes
- `metal_marlin/models/mixtral.py` - Mixtral* classes
- `metal_marlin/models/moe.py` - QuantizedMoELayer

**Remaining in `metal_marlin/models/`:**
- `deepseek.py` - DeepSeekQwenModel (working)

---

## OpenAI-Compatible Server

**Status:** âœ… Complete (30 tests passing)

FastAPI-based OpenAI-compatible server in `metal_marlin/serving/`:

```bash
# Start server
metal-marlin serve benchmarks/results/qwen3_4b_fp4 --port 8000

# Or with Python
python -m metal_marlin serve benchmarks/results/qwen3_4b_fp4

# Test with mock model
METAL_MARLIN_MOCK_MODEL=1 python -m metal_marlin serve /tmp/any --port 8000
```

**Endpoints:**

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/v1/models` | GET | List available models |
| `/v1/models/{id}` | GET | Model info (capabilities, config) |
| `/v1/chat/completions` | POST | Chat completions (streaming supported) |
| `/v1/completions` | POST | Text completions |
| `/metrics` | GET | Prometheus metrics |

**Test Coverage (30 tests):**
- Basic functionality: health, models, chat, completions
- Streaming responses with SSE
- Concurrent requests (10 requests, 5 workers)
- Input validation (missing fields, wrong types â†’ 422)
- Error handling (wrong model â†’ 404, no model loaded â†’ 503)
- Paged attention mode (2 tests)

**Current behavior (non-paged attention):**
- Each request runs sequentially through the model pipeline
- No KV cache sharing between requests
- Good for single-user or low-concurrency scenarios

**Paged attention (CLI-enabled with `--enable-batching`):**
- Enable via: `metal-marlin serve MODEL --enable-batching`
- Tune KV cache: `--num-kv-blocks 512 --block-size 16`
- `continuous_batch.py`: BatchScheduler, KVCacheManager
- `runner.py`: BatchedModelRunner with paged KV execution
- `paged/`: BlockAllocator, PageTable, paged_attention kernels
- Enables continuous batching, KV cache reuse, higher throughput

**Usage:** See [serving guide](docs/guides/serving.md) for full API reference and OpenAI SDK examples.

---

## Metal Shader Status

Verified via `scripts/verify_kernels.py`:

### âœ… All Shaders (65 total, precompiled)

| Category | Shaders |
|----------|---------|
| **GEMM** | marlin_gemm, gemm_fp4_optimized, dense_gemm, batched_gemm, decode_gemv |
| **Attention** | flash_attention, flash_attention_v2, simdgroup_attention, paged_attention, sliding_window_attention, tree_attention, diff_attention, mla_proj |
| **MoE** | moe_dispatch (3 variants), moe_expert_gemm, moe_router, moe_shared_expert |
| **Quantization** | dequant (4 variants), viterbi_quant, hessian, cholesky |
| **Other** | rope, sampling, layernorm, hadamard, sparse (2), scatter_gather, all_reduce, rwkv_wkv, vision_preprocess, bf16_compat |

### Kernel Optimization

Auto-tuning via `scripts/optimize_kernel.py`:

| Variant | Speedup | Notes |
|---------|---------|-------|
| `tile_n_32` | **2.4x** | Best current variant |
| `tile_m_128` | 1.8x | Larger M tiles |
| `simdgroups_2` | 1.5x | More parallelism |

### Known Metal Compiler Bugs (Documented)

See [docs/metal_array_parameter_bugs.md](docs/metal_array_parameter_bugs.md) for two Metal compiler bugs affecting simdgroup operations:
1. Functions receiving 2D `simdgroup_matrix` arrays require `__attribute__((always_inline))`
2. 3D threadgroup array slices should use pointers instead of 2D references

---

## Stubs & Incomplete Implementations

The following are intentional stubs or need optimization:

| Location | Function/Class | Status |
|----------|---------------|--------|
| kernels.py | flash_attention_fp4_kv | Stub - needs fused kernel |
| kernels.py | moe_expert_gemm_fp4 | âœ… Batched Metal dispatch |
| kernels.py | moe_router_topk | Works - uses PyTorch ops |
| kernels.py | moe_shared_expert_fp4 | âœ… Fully wired to Metal |
| quantization/ | viterbi_quant | âœ… Fully implemented with Metal |
| speculative/engine.py | TargetModel.__call__ | Protocol - by design |
| speculative/engine.py | TargetModel.create_kv_cache | Protocol - by design |

---

## Code Quality

### Ruff Warnings (101)

Mostly whitespace and unused variable warnings:
- `W293` - Blank line contains whitespace
- `F841` - Local variable assigned but never used

```bash
uv run ruff check . --fix  # Auto-fix available
```

### Pyright Status

| Category | Count |
|----------|-------|
| Errors | 0 |
| Warnings | 215 |

Fixed `safe_open` import issue in `layer_streamer.py` (was 6 errors).

---

## Blockers

### 1. GEMM Kernel Dispatch (Resolved âœ…)

Fixed two Metal compiler bugs:
1. **Array Parameter Bug**: Functions receiving 2D `simdgroup_matrix` arrays need `__attribute__((always_inline))`
2. **Tile Coverage Bug**: Simdgroup configuration only covered 32 of 64 rows

See [docs/metal_array_parameter_bugs.md](docs/metal_array_parameter_bugs.md) for details.

### 2. Qwen3 LayerNorm Device (Resolved âœ…)

`RuntimeError: Expected all tensors to be on the same device (mps:0 vs cpu)`
- Fixed by defaulting RMSNorm device to None/cpu
- Test now passing: `test_qwen3_layer_forward`

### 3. GLM-4.7-Flash MLA (Resolved âœ…)

MLA implementation now working - all GLM-4.7 model tests pass.

---

## Quantization Formats

| Format | Bits | Status | Use Case |
|--------|------|--------|----------|
| FP4 E2M1 | 4.0 | âœ… Working | Default weights |
| INT4 U4/S4 | 4.0 | âœ… Working | GPTQ compat |
| FP8 E4M3 | 8.0 | âœ… Working | Higher precision |
| INT3/INT2 | 3/2 | âœ… Working | Cold experts |
| 2:4 Sparse | var | âœ… Working | Sparsity |

---

## Task Queue

Current swarm status:

| Phase | Tasks | Status |
|-------|-------|--------|
| 32 | Buffer cache fix, INT4 export, linting | âœ… Complete |
| 33 | Qwen3/GLM4 layer implementations | âœ… Complete |
| 34 | Test failures, kernel integration | âœ… Complete |
| 35 | Kernel compilation, device mismatch, ZeroModule | âœ… Complete |
| 36 | GEMM dispatch debugging, Metal compiler bugs | âœ… Complete |
| 37 | FP4 reference fixes, Hadamard kernel, LayerNorm | âœ… Complete |
| 42 | MoE Pipeline (GLM-4.7-Flash) | âœ… Complete |
| 43 | Legacy Code Cleanup | âœ… Complete |
| 44 | EXL3 Quantization Pipeline | âœ… Complete |
| 45 | Kernel Optimization | âœ… Complete |
| 50-57 | Trellis Inference Pipeline | âœ… Complete |
| 68 | Codebase Consolidation | ðŸ“‹ Queued |
| **70** | **Metallib Precompilation** | âœ… **Complete** |
| **74R** | **Cython LAPACK Acceleration** | âœ… **Complete** |
| **75** | **Metal PSD + Dynamic Bit Allocation** | âœ… **Complete** |
| 71 | **Optimization Swarm v1** | ðŸ”„ **In Progress** (100 tasks queued) |

### Phase 75: Metal PSD Projection + Dynamic Bit Allocation âœ…

**Status:** Complete (2026-02-04)

Accelerated quantization with Metal PSD projection and sensitivity-aware 2-8 bit allocation.

#### Components

| Module | Purpose | Status |
|--------|---------|--------|
| `_psd_dispatch.mm` | Metal PSD projection with embedded shader | âœ… Working |
| `psd_project_metal()` | Iterative Cholesky with diagonal regularization | âœ… Working |
| `is_likely_psd()` | O(NÂ²) Gershgorin fast check (skip projection) | âœ… Working |
| `start_prefetch()` / `get_prefetched()` | Background prefetch pipeline | âœ… Working |
| `sensitivity_to_bits()` | Maps layer sensitivity to 2-8 bit precision | âœ… Working |
| `quantize_moe_experts_dynamic()` | Dynamic expert bit allocation | âœ… Working |

#### Performance

| Operation | Before (NumPy) | After (Metal) | Speedup |
|-----------|----------------|---------------|---------|
| PSD projection 128x128 | ~15ms | ~0.5ms | **30x** |
| PSD projection 256x256 | ~85ms | ~2ms | **42x** |
| Gershgorin check 128x128 | N/A | ~0.01ms | Fast path |

#### Dynamic Bit Allocation

Maps expert sensitivity to bit precision using sqrt scaling:

| Sensitivity | Bits | Use Case |
|-------------|------|----------|
| 0.0 (min) | 2 | Cold/rarely-used experts |
| 0.25 | 5 | Below-average experts |
| 0.50 | 6 | Average experts |
| 0.75 | 7 | Above-average experts |
| 1.0 (max) | 8 | Critical experts |

**Example (64 experts):**
- Average bits: 5.05 bits/weight
- Distribution: 2-bit (8), 3-bit (12), 4-bit (14), 5-bit (10), 6-bit (8), 7-bit (7), 8-bit (5)

#### Usage

```bash
# Dynamic bit allocation for Qwen3-30B MoE
uv run python scripts/quantize_qwen3_30b.py \
    --model Qwen/Qwen3-30B-A3B \
    --dynamic-experts \
    --expert-min-bits 2 \
    --expert-max-bits 8

# Check if Hessian needs PSD projection (fast path)
from metal_marlin._psd_dispatch import is_likely_psd, psd_project_metal
if not is_likely_psd(H):
    H = psd_project_metal(H, sigma_reg=0.01, max_iters=10)
```

### Phase 74R: Cython LAPACK Acceleration âœ…

**Status:** Complete (2026-02-04)

Cython wrappers for Apple Accelerate LAPACK (LDL, eigendecomposition).

| Module | Purpose | Status |
|--------|---------|--------|
| `_ldl_fast.pyx` | Cython LDL decomposition | âœ… Working |
| `_eigh_fast.pyx` | Cython eigendecomposition | âœ… Working |

**Note:** Both use Apple Accelerate's LAPACK underneath, so speedup vs NumPy is ~1.0x
(both already call the same BLAS routines). The main benefit is reduced Python overhead
for small matrices and future ability to batch multiple decompositions.
### Phase 68: Codebase Consolidation

**Goal:** Reduce sprawl without losing capability.

| Category | Before | Target |
|----------|--------|--------|
| Benchmark files | 42 (15K LOC) | ~15 active + archive |
| Task YAML files | 90+ | ~10 active + archive |
| Hardcoded model IDs | ~20 files | Centralized registry |
| Duplicate tokenizers | ~5 copies | 0 (load from HF) |
| MLX remnants | 2-3 files | 0 |

**Task file:** `tasks/phase68_codebase_consolidation.yaml`

### Phase 50-57 Results (Trellis Inference)

**Completed:**
- âœ… `TrellisModel` with MoE + dense layer support
- âœ… `TrellisMLAttention` for MLA with KV compression
- âœ… `TrellisKVCache` for compressed KV storage
- âœ… `TrellisLinear` with Metal dequantization dispatch
- âœ… `TrellisModelLoader` with format auto-detection
- âœ… `TrellisGenerator` with streaming and sampling
- âœ… `TrellisDenseMLP` and `TrellisMoEMLP` layers
- âœ… `TrellisForCausalLM` language model wrapper
- âœ… Tests: 50 tests across 7 test files

### Phase 44 Results (EXL3 Quantization)

**Completed:**
- âœ… `EXL3Quantizer` class with LDLQ algorithm
- âœ… `TrellisCodebook` for state machine encoding
- âœ… `viterbi_quant` Metal kernel for optimal search
- âœ… `hadamard_preprocess` for outlier dispersal
- âœ… `StreamingHessianCollector` for memory-efficient calibration
- âœ… `ldl_decomp` (2x faster than Cholesky)
- âœ… Tests: `test_trellis.py`, `test_viterbi_quant.py`

### Phase 45 Results (Kernel Optimization)

**In Progress:**
- âœ… `optimize_kernel.py` script for auto-tuning
- âœ… `optimize_all_kernels.py` for batch optimization
- âœ… `tile_n_32` variant: 2.4x speedup
- ðŸ”„ Additional variant exploration

### Phase 42 Results

**Completed:**
- âœ… `MetalQuantizedMoE` class for quantized expert weights
- âœ… `moe_shared_expert_fp4()` wired to Metal kernel
- âœ… `find_moe_layers()`, `quantize_moe_experts()`, `replace_moe_layers()`
- âœ… `transformers_loader.py` with automatic MoE detection
- âœ… Tests: `test_moe_accuracy.py`, `test_glm47_transformers.py`, `test_qwen3_moe_transformers.py`
- âœ… Legacy model deprecation warnings
- âœ… `moe_expert_gemm_fp4` batched Metal dispatch (replaced Python loop)
- âœ… End-to-end GLM-4.7-Flash generation verified

### Phase 43 Results (Legacy Cleanup)

**Completed:**
- âœ… Deleted `metal_marlin/legacy/` directory
- âœ… Deleted `metal_marlin/models/llama.py`, `qwen3.py`, `mixtral.py`, `moe.py`
- âœ… Removed legacy exports from `__init__.py` files
- âœ… Cleaned `test_inference.py` (removed MetalGLM47Model tests)
- âœ… Removed root directory junk files

---

## Commands

```bash
# Build precompiled metallib (required for fast dispatch)
cd contrib/metal_marlin
./scripts/build_metallib.sh

# Run tests
uv run pytest tests/ -v --tb=short

# Run tests (excluding optional dependency tests)
uv run pytest tests/ -v --ignore=tests/test_all_models.py \
    --ignore=tests/test_trellis.py --ignore=tests/test_viterbi_quant.py

# Verify kernel compilation
uv run python3 scripts/verify_kernels.py

# Optimize kernel parameters
uv run python3 scripts/optimize_kernel.py --shapes 4096x4096x4096 --iterations 100

# Apply best optimization
uv run python3 scripts/optimize_kernel.py --apply-best <session_id>

# Quantize a new model
uv run python3 -m metal_marlin.hf_loader Qwen/Qwen3-4B ./output --bits 4

# EXL3 quantization
uv run python3 -m metal_marlin.quantization.exl3_pipeline \
    --model Qwen/Qwen3-4B --output ./qwen3_exl3 --bits 4

# Run linting
uv run ruff check .
uv run pyright metal_marlin/

# Fix ruff warnings
uv run ruff check . --fix
```

---

## Next Steps (Roadmap)

### Priority 1: Performance Optimization

| Task | Description | Impact |
|------|-------------|--------|
| **Flash Attention v3** | Chunked prefill for long contexts | 2-3x prefill speedup at >4K |
| **MoE kernel fusion** | Fuse router + expert GEMM | Reduce 125ms/layer â†’ <50ms |
| **Decode GEMV** | Optimized single-token decode | Target: 10+ tok/s |

### Priority 2: Production Readiness

| Task | Description | Impact |
|------|-------------|--------|
| **Continuous batching** | Share KV cache across requests | 5-10x throughput |
| **Speculative decoding** | Draft-verify architecture | 2-3x decode speed |
| **INT8 KV cache** | Quantized attention cache | 2x context length |

### Priority 3: Developer Experience

| Task | Description | Impact |
|------|-------------|--------|
| **Auto-detect metallib staleness** | Warn if shaders modified | Prevent silent bugs |
| **Profiling dashboard** | Real-time kernel timing | Easier optimization |
| **Model registry** | Centralized model configs | Less hardcoding |

### Known Issues

1. **Import error with torch.float32**
   - Fixed: Lazy initialization in `kv_cache.py` avoids module-level initialization issues. âœ…

2. **bf16_kernels verification failed**  
   - Task `verify-bf16-kernels-separated` hit tier 7 (Cursor CLI unavailable)
   - bf16 kernels ARE separated - verification task needs retry

### Files Moved (2026-02-02)

Cleaned up metal_marlin root by moving debug/profiling scripts:
- `profile_overhead.py` â†’ `scripts/`
- `test_perf.py` â†’ `scripts/`
- `verify_decode_fix.py` â†’ `scripts/`
- `verify_moe_fix.py` â†’ `scripts/`
- `all_kernel_names.txt` â†’ `scripts/`
- `check_circular_includes.py` â†’ `scripts/` (from AlphaHENG root)
