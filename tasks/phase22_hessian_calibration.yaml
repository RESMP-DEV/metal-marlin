# yaml-language-server: $schema=
# Phase 22: Advanced Hessian Calibration & Layer-Aware Quantization
#
# Goals:
# 1. Implement proper Hessian calibration with Bartowski v3 dataset
# 2. ExllamaV3-style calibration for low-bit MoE quantization
# 3. Layer-aware mixed precision (FP4/FP8/FP16 per layer type)
# 4. Memory-aware batch processing
# 5. Test on MoE (Qwen3-30B-A3B), Dense (Qwen3-32B), MLA (GLM-4.7-Flash)

tasks:
  # =========================================================================
  # HESSIAN CALIBRATION INFRASTRUCTURE
  # =========================================================================

  - name: hessian-collector-base
    prompt: |
      Create `metal_marlin/calibration/hessian_collector.py`:

      Implement HessianCollector class for accumulating X^T @ X during forward passes:

      ```python
      @dataclass
      class HessianCollector:
          """Collect Hessian approximation H = X^T X from activations."""
          in_features: int
          H: np.ndarray  # Running sum [in_features, in_features]
          n_samples: int
          dtype: np.dtype = np.float64  # Use float64 for accumulation precision
          
          def accumulate(self, x: np.ndarray) -> None:
              """Add batch of activations. x: [batch, seq_len, in_features]"""
              x_flat = x.reshape(-1, self.in_features)
              self.H += x_flat.T @ x_flat
              self.n_samples += x_flat.shape[0]
          
          def get_hessian(self, damp: float = 0.01) -> np.ndarray:
              """Return damped Hessian for GPTQ."""
              H = self.H / self.n_samples
              # Damping for numerical stability
              diag_mean = np.mean(np.diag(H))
              H += damp * diag_mean * np.eye(H.shape[0])
              return H.astype(np.float32)
      ```

      Key features:
      - Memory-efficient streaming accumulation
      - Configurable damping factor
      - Per-layer collection with hooks
      - Support for batched updates

      Include HessianManager for coordinating collection across model layers.
    priority: P0
    dependencies: []

  - name: bartowski-calibration-loader
    prompt: |
      Create `metal_marlin/calibration/bartowski.py`:

      Load and process Bartowski calibration_v3 dataset from `examples/calibration_datav3.txt`.

      ```python
      class BartowskiCalibration:
          """Bartowski v3 multi-domain calibration dataset."""
          
          @classmethod
          def v3(cls, max_samples: int | None = None) -> "BartowskiCalibration":
              """Load calibration_v3 from examples folder."""
              
          @classmethod  
          def from_local(cls, path: str | Path) -> "BartowskiCalibration":
              """Load custom calibration file."""
          
          def tokenize(self, tokenizer, max_length: int = 2048) -> list[np.ndarray]:
              """Tokenize samples for model forward passes."""
          
          def get_batches(self, batch_size: int = 4) -> Iterator[list[str]]:
              """Yield batches for calibration."""
      ```

      The dataset contains ~800 samples across domains:
      - Scientific/medical text
      - Code snippets
      - Conversational/chat
      - Mathematical reasoning

      This multi-domain coverage produces better Hessian estimates than single-domain
      datasets like WikiText-2.
    priority: P0
    dependencies: []

  - name: forward-hook-integration
    prompt: |
      Create `metal_marlin/calibration/hooks.py`:

      Implement forward hooks for Hessian collection during calibration passes.

      ```python
      class CalibrationHooks:
          """Manage forward hooks for Hessian collection."""
          
          def __init__(self):
              self.collectors: dict[str, HessianCollector] = {}
              self.handles: list[torch.utils.hooks.RemovableHandle] = []
          
          def register_linear_hooks(self, model, layer_filter: Callable | None = None):
              """Register hooks on all Linear layers."""
              
          def collect_from_module(self, module_name: str, module: nn.Module):
              """Create collection hook for a specific module."""
              
          def remove_hooks(self):
              """Clean up all registered hooks."""
              
          def get_hessians(self) -> dict[str, np.ndarray]:
              """Return all collected Hessians."""
      ```

      Support both:
      - PyTorch models (via register_forward_hook)
      - MLX models (via custom wrapper approach)

      Handle GQA/MQA attention where Q has different dims than K/V.
    priority: P0
    dependencies:
      - hessian-collector-base

  # =========================================================================
  # EXLLAMAV3-STYLE CALIBRATION
  # =========================================================================

  - name: exllamav3-calibration-research
    prompt: |
      Research ExllamaV3 calibration methodology and document findings in
      `docs/exllamav3_calibration.md`.

      Key aspects to investigate:
      1. How ExllamaV3 handles per-layer sensitivity analysis
      2. The "error measurement" approach for determining bits-per-weight
      3. How it handles MoE routed vs shared experts differently
      4. The iterative quantization refinement process
      5. Row-wise vs column-wise error propagation differences

      Reference ExllamaV3 GitHub: https://github.com/turboderp/exllamav3

      Create a comparison table:
      | Feature | GPTQ | AWQ | ExllamaV3 | Our MR-GPTQ |

      Document which techniques we should adopt for Metal Marlin.
    priority: P1
    dependencies: []

  - name: sensitivity-analysis-module
    prompt: |
      Create `metal_marlin/calibration/sensitivity.py`:

      Implement per-layer sensitivity analysis inspired by ExllamaV3.

      ```python
      @dataclass
      class LayerSensitivity:
          name: str
          hessian_trace: float      # tr(H) - activation magnitude
          hessian_condition: float  # max(eig)/min(eig) - numerical sensitivity
          weight_variance: float    # σ²(W) - weight distribution spread
          outlier_ratio: float      # |W| > 3σ - outlier prevalence
          recommended_bits: int     # 4, 8, or 16
          recommended_format: str   # "fp4", "fp8", "fp16"

      def analyze_layer_sensitivity(
          weight: np.ndarray,
          hessian: np.ndarray | None,
          activations_sample: np.ndarray | None = None,
      ) -> LayerSensitivity:
          """Determine optimal quantization for a layer."""

      def compute_model_sensitivity_profile(
          model_path: Path,
          calibration_data: BartowskiCalibration,
      ) -> dict[str, LayerSensitivity]:
          """Generate full model sensitivity analysis."""
      ```

      Decision rules:
      - Router layers: Always FP16 (critical for MoE)
      - High condition number: FP8 or FP16
      - High outlier ratio: Apply Hadamard first, then re-evaluate
      - Low variance layers: Can use aggressive FP4
    priority: P1
    dependencies:
      - hessian-collector-base
      - forward-hook-integration

  - name: adaptive-bits-quantizer
    prompt: |
      Create `metal_marlin/calibration/adaptive_quant.py`:

      Implement adaptive bits-per-weight selection like ExllamaV3.

      ```python
      class AdaptiveQuantizer:
          """Adaptively choose bits per layer based on error budget."""
          
          def __init__(
              self,
              error_budget: float = 0.01,  # Target reconstruction error
              min_bits: int = 4,
              max_bits: int = 8,
          ):
              pass
          
          def quantize_layer_adaptive(
              self,
              weight: np.ndarray,
              hessian: np.ndarray,
              target_error: float,
          ) -> tuple[np.ndarray, int, QuantizationFormat]:
              """
              Try quantization at different bit widths.
              Return the lowest bits that meets error target.
              """
          
          def iterative_refinement(
              self,
              weight: np.ndarray,
              hessian: np.ndarray,
              initial_quant: np.ndarray,
              iterations: int = 3,
          ) -> np.ndarray:
              """
              ExllamaV3-style iterative error correction.
              Re-quantize using residual errors.
              """
      ```

      Key insight: Some MoE expert layers can use 3-bit or even 2-bit
      because they see less diverse inputs (sparse activation).
    priority: P1
    dependencies:
      - sensitivity-analysis-module

  # =========================================================================
  # LAYER-AWARE MIXED PRECISION
  # =========================================================================

  - name: moe-aware-precision-config
    prompt: |
      Update `metal_marlin/mixed_precision.py` with MoE-specific precision rules:

      ```python
      @dataclass
      class MoEPrecisionConfig:
          """Precision configuration optimized for MoE models."""
          
          # Router: Always high precision
          router_precision: Precision = Precision.FP16
          router_group_size: int = 0  # No quantization
          
          # Shared expert: Moderate precision (sees all tokens)
          shared_expert_precision: Precision = Precision.FP4
          shared_expert_group_size: int = 64
          
          # Routed experts: Can be aggressive (sparse activation)
          routed_expert_precision: Precision = Precision.FP4
          routed_expert_group_size: int = 128
          routed_expert_min_bits: int = 3  # Allow 3-bit for redundant experts
          
          # Attention: Layer-specific
          attention_qkv_precision: Precision = Precision.FP4
          attention_qkv_group_size: int = 64
          attention_o_precision: Precision = Precision.FP4
          attention_o_group_size: int = 128
          
          # Embeddings
          embed_precision: Precision = Precision.FP8

      class LayerPrecisionSelector:
          """Select precision based on layer sensitivity analysis."""
          
          def __init__(self, sensitivity_profile: dict[str, LayerSensitivity]):
              pass
          
          def get_precision(self, layer_name: str) -> tuple[Precision, int]:
              """Return (precision, group_size) for layer."""
      ```

      Add presets:
      - `MoEPrecisionConfig.quality()` - Prioritize accuracy
      - `MoEPrecisionConfig.balanced()` - Default trade-off
      - `MoEPrecisionConfig.size()` - Minimize memory
    priority: P0
    dependencies:
      - sensitivity-analysis-module

  - name: fp8-quantization-support
    prompt: |
      Add FP8 E4M3/E5M2 support to `metal_marlin/quantize.py`:

      ```python
      # FP8 E4M3 grid (for weights)
      FP8_E4M3_GRID = compute_fp8_e4m3_values()  # 240 distinct values

      # FP8 E5M2 grid (for activations/gradients)  
      FP8_E5M2_GRID = compute_fp8_e5m2_values()  # 256 distinct values

      def pack_fp8_weights(
          weight: np.ndarray,
          group_size: int = 128,
          format: str = "e4m3",
      ) -> tuple[np.ndarray, np.ndarray]:
          """Pack weights to FP8 format."""

      def dequant_fp8_e4m3(packed: np.ndarray, scales: np.ndarray) -> np.ndarray:
          """Dequantize FP8 E4M3 weights."""
      ```

      FP8 provides 2x storage vs FP4 but much better precision for sensitive layers.
      Use for:
      - Embedding layers
      - First/last transformer blocks
      - Layers with high Hessian condition number
    priority: P1
    dependencies: []

  - name: mixed-format-packing
    prompt: |
      Create `metal_marlin/packing/mixed_format.py`:

      Support mixed FP4/FP8/FP16 within a single model file.

      ```python
      @dataclass
      class MixedFormatHeader:
          """Header describing mixed-format quantized model."""
          layer_formats: dict[str, str]  # layer_name -> "fp4"/"fp8"/"fp16"
          layer_group_sizes: dict[str, int]
          total_params: int
          quantized_params: int
          average_bits: float

      def pack_mixed_format_model(
          weights: dict[str, np.ndarray],
          precision_map: dict[str, tuple[Precision, int]],
          output_path: Path,
      ) -> MixedFormatHeader:
          """Save model with per-layer precision."""

      def load_mixed_format_model(
          model_path: Path,
      ) -> tuple[dict[str, np.ndarray], MixedFormatHeader]:
          """Load mixed-format model."""
      ```

      Store format metadata in safetensors header for runtime dispatch.
    priority: P1
    dependencies:
      - fp8-quantization-support
      - moe-aware-precision-config

  # =========================================================================
  # MEMORY-AWARE PROCESSING
  # =========================================================================

  - name: memory-query-system
    prompt: |
      Create `metal_marlin/utils/memory.py`:

      Query available system memory for adaptive batch sizing.

      ```python
      import psutil
      import subprocess

      @dataclass
      class MemoryInfo:
          total_ram_gb: float
          available_ram_gb: float
          gpu_total_gb: float       # Metal unified memory
          gpu_available_gb: float
          recommended_batch_tensors: int

      def get_system_memory() -> MemoryInfo:
          """Query system memory (macOS/Linux)."""
          # Use psutil for RAM
          mem = psutil.virtual_memory()
          
          # Query Metal GPU memory via system_profiler or Metal API
          # On Apple Silicon, GPU shares unified memory
          
      def estimate_tensor_memory(shape: tuple, dtype: np.dtype) -> int:
          """Estimate memory for tensor in bytes."""
          
      def compute_optimal_batch_size(
          tensor_shapes: list[tuple],
          available_memory_gb: float,
          safety_factor: float = 0.7,
      ) -> int:
          """Compute batch size that fits in memory."""
      ```

      For Apple Silicon:
      - Query unified memory via `sysctl hw.memsize`
      - Check Metal memory pressure
      - Leave headroom for MLX/Metal allocations
    priority: P1
    dependencies: []

  - name: adaptive-prefetcher
    prompt: |
      Create `metal_marlin/utils/prefetch.py`:

      Memory-aware tensor prefetching for quantization.

      ```python
      class AdaptivePrefetcher:
          """Prefetch tensors based on available memory."""
          
          def __init__(
              self,
              st_files: list[Path],
              target_memory_gb: float | None = None,
          ):
              if target_memory_gb is None:
                  mem_info = get_system_memory()
                  target_memory_gb = mem_info.available_ram_gb * 0.6
              
          def __iter__(self) -> Iterator[tuple[str, np.ndarray]]:
              """Yield tensors with prefetching."""
              
          def prefetch_batch(self, batch_size: int) -> list[tuple[str, np.ndarray]]:
              """Load batch of tensors into memory."""
      ```

      Strategy:
      1. Query available memory
      2. Estimate tensor sizes from safetensors metadata
      3. Prefetch N tensors that fit in memory budget
      4. Process while loading next batch
    priority: P1
    dependencies:
      - memory-query-system

  # =========================================================================
  # MODEL-SPECIFIC TESTING
  # =========================================================================

  - name: test-qwen3-30b-moe
    prompt: |
      Create comprehensive test for Qwen3-30B-A3B (MoE) in
      `benchmarks/eval_qwen3_30b_full.py`:

      Test matrix:
      1. RTN FP4 (baseline)
      2. MR-GPTQ FP4 (Hadamard only)
      3. MR-GPTQ FP4 with Hessian (Bartowski v3)
      4. Mixed precision: Router FP16, Experts FP4
      5. Adaptive bits: 3-bit experts, 4-bit attention

      Metrics:
      - Perplexity (WikiText-2 for comparison)
      - Layer-wise RMSE
      - Routing accuracy (do quantized routers pick same experts?)
      - Inference throughput
      - Memory usage

      Use `examples/calibration_datav3.txt` for Hessian collection.
    priority: P0
    dependencies:
      - hessian-collector-base
      - bartowski-calibration-loader
      - forward-hook-integration
      - moe-aware-precision-config

  - name: test-qwen3-32b-dense
    prompt: |
      Create test for Qwen3-32B (dense) in `benchmarks/eval_qwen3_32b_dense.py`:

      Dense models have different characteristics:
      - All params active every forward pass
      - More uniform layer sensitivity
      - Higher memory pressure during inference

      Test configurations:
      1. Uniform FP4 g128
      2. MR-GPTQ FP4 with Hessian
      3. First/last layers FP8, middle FP4
      4. Attention FP4/g64, MLP FP4/g128

      Compare against:
      - BF16 baseline
      - GGUF Q4_K_M
      - MLX native 4-bit
    priority: P1
    dependencies:
      - test-qwen3-30b-moe

  - name: test-glm4-flash-mla
    prompt: |
      Create test for GLM-4.7-Flash (MLA architecture) in
      `benchmarks/eval_glm4_flash.py`:

      GLM-4.7 uses Multi-head Latent Attention (MLA):
      - Compressed KV cache via learned projections
      - Different attention structure than standard MHA/GQA
      - Requires special handling for latent projections

      Key differences:
      - `kv_lora_rank` dimension for compressed KV
      - Separate Q/K/V projections with latent bottleneck
      - `rope_ratio` for position embedding scaling

      Test:
      1. Identify which MLA layers are most sensitive
      2. Compare Hadamard benefit on latent projections
      3. Determine optimal precision for KV compression layers

      Reference: model config has `kv_lora_rank: 512`, `q_lora_rank: 1536`
    priority: P1
    dependencies:
      - sensitivity-analysis-module
      - test-qwen3-30b-moe

  # =========================================================================
  # FULL QUANTIZATION PIPELINE
  # =========================================================================

  - name: full-mrgptq-pipeline
    prompt: |
      Update `metal_marlin/mr_gptq.py` with full Hessian-aware pipeline:

      ```python
      class MRGPTQQuantizer:
          def quantize_model_with_calibration(
              self,
              model_path: Path,
              calibration: BartowskiCalibration,
              tokenizer,
              output_path: Path,
              precision_config: MoEPrecisionConfig | None = None,
              num_calibration_batches: int = 128,
              batch_size: int = 4,
              verbose: bool = True,
          ) -> QuantizationReport:
              """
              Full MR-GPTQ pipeline:
              1. Load model for forward passes
              2. Register Hessian collection hooks
              3. Run calibration forward passes (Bartowski v3)
              4. Apply Hadamard rotation to weights
              5. Run GPTQ with collected Hessians
              6. Pack and save quantized model
              """
      ```

      This is the main entry point for production quantization.
      Should support:
      - Streaming large models (don't load all at once)
      - Checkpointing progress
      - Resume from interruption
    priority: P0
    dependencies:
      - hessian-collector-base
      - bartowski-calibration-loader
      - forward-hook-integration
      - moe-aware-precision-config

  - name: cli-quantize-command
    prompt: |
      Update CLI in `metal_marlin/__main__.py`:

      ```bash
      # Full MR-GPTQ with Hessian calibration
      python -m metal_marlin quantize \
          --input models/Qwen3-30B-A3B \
          --output models/Qwen3-30B-MR-GPTQ \
          --method mr-gptq \
          --calibration bartowski-v3 \
          --precision-config moe-balanced \
          --workers 12

      # Quick RTN (no calibration)
      python -m metal_marlin quantize \
          --input models/Qwen3-32B \
          --output models/Qwen3-32B-RTN \
          --method rtn \
          --format fp4

      # Sensitivity analysis
      python -m metal_marlin analyze \
          --input models/GLM-4.7-Flash \
          --calibration bartowski-v3 \
          --output sensitivity_report.json
      ```

      Add proper argparse with subcommands for:
      - `quantize` - Main quantization flow
      - `analyze` - Sensitivity analysis
      - `convert` - Format conversion
      - `eval` - Perplexity evaluation
    priority: P1
    dependencies:
      - full-mrgptq-pipeline

  # =========================================================================
  # DOCUMENTATION & VALIDATION
  # =========================================================================

  - name: update-documentation
    prompt: |
      Update documentation with new calibration features:

      1. `docs/mr_gptq.md` - Add Hessian calibration section
      2. `docs/calibration.md` - New file for calibration guide
      3. `README.md` - Update quick start with calibration example

      Include:
      - Why Bartowski v3 > WikiText-2
      - How to create custom calibration datasets
      - Sensitivity analysis interpretation
      - MoE-specific quantization guide
      - ExllamaV3 comparison
    priority: P2
    dependencies:
      - full-mrgptq-pipeline
      - exllamav3-calibration-research

  - name: validation-test-suite
    prompt: |
      Create comprehensive validation in `tests/test_calibration.py`:

      Tests:
      1. HessianCollector accumulation accuracy
      2. Bartowski loader sample coverage
      3. Sensitivity analysis reproducibility
      4. Mixed precision packing/unpacking
      5. FP8 format correctness
      6. End-to-end quantize -> inference

      Use small synthetic models for fast CI runs.
      Add markers for slow tests that use real models.
    priority: P1
    dependencies:
      - full-mrgptq-pipeline
