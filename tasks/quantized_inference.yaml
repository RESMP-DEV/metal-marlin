# yaml-language-server: $schema=
# End-to-end Quantized Inference Implementation
# Run with: uv run alphaheng tasks add contrib/metal_marlin/tasks/quantized_inference.yaml

tasks:
  # =============================================================================
  # Phase 1: Fix Broken Shader (CRITICAL BLOCKER)
  # =============================================================================

  - name: fix-marlin-gemm-store-results
    prompt: |
      Fix marlin_gemm.metal shader compilation - CRITICAL BLOCKER.

      Error: 7 calls to `store_results` pass 10 arguments but function requires 11.

      The function signature at line 497:
      ```metal
      inline void store_results(
          thread simdgroup_matrix<half, 8, 8> acc[SG_M_TILES][SG_N_TILES],
          device half* C,
          uint M, uint N,
          uint tg_row, uint tg_col,
          uint sg_row_offset, uint sg_col_offset,
          uint simd_lane,
          uint simd_id [[maybe_unused]],
          threadgroup half (&staging)[8][8]  // <-- 11th parameter missing from calls
      )
      ```

      Broken call sites (lines ~905, 963, 1130, 1181, 1807, 2023, 2110):
      ```metal
      store_results(acc, C, M, N, tg_row, tg_col,
                    sg_row_offset, sg_col_offset, simd_lane, simd_id);
      // Missing: staging parameter
      ```

      Fix pattern - add staging buffer to each call:
      ```metal
      store_results(acc, C, M, N, tg_row, tg_col,
                    sg_row_offset, sg_col_offset, simd_lane, simd_id, staging);
      ```

      Each kernel should already have `threadgroup half staging[8][8];` declared.
      Check lines 713, 992 for existing declarations.

      For kernels with `staging[SIMDGROUPS_PER_TG][8][8]` (line 1400), use `staging[simd_id]`.

      After fixing, verify:
      ```bash
      cd contrib/metal_marlin
      uv run python -c "
      from metal_marlin.metal_dispatch import MetalKernelLibrary
      lib = MetalKernelLibrary.from_source_dir()
      print('All shaders compile successfully')
      "
      ```

      File: contrib/metal_marlin/src/marlin_gemm.metal
    priority: P0
    dependencies: []

  # =============================================================================
  # Phase 2: Quantized Weight Loader
  # =============================================================================

  - name: implement-quantized-model-loader
    prompt: |
      Create a quantized model loader that loads FP4/INT4 weights and creates a runnable model.

      The quantization pipeline saves to:
      - benchmarks/results/<config>/model.safetensors (quantized weights)
      - benchmarks/results/<config>/quantization_config.json (metadata)

      Create metal_marlin/quantized_loader.py:
      ```python
      from dataclasses import dataclass
      from pathlib import Path
      import torch
      from safetensors.torch import load_file

      @dataclass
      class QuantizedTensor:
          \"\"\"A quantized weight tensor with its metadata.\"\"\"
          data: torch.Tensor  # Packed quantized data (uint8/uint16)
          scales: torch.Tensor  # Per-group or per-channel scales
          format: str  # "fp4", "int4", "int2", etc.
          group_size: int
          original_shape: tuple[int, ...]
          needs_hadamard: bool = False
          hadamard_matrix: torch.Tensor | None = None

      @dataclass
      class QuantizedModel:
          \"\"\"Container for a fully quantized model.\"\"\"
          config: dict
          weights: dict[str, QuantizedTensor | torch.Tensor]
          # BF16 weights for things like router gates
          bf16_weights: dict[str, torch.Tensor]
          
          @classmethod
          def load(cls, path: Path | str) -> "QuantizedModel":
              \"\"\"Load a quantized model from safetensors.\"\"\"
              path = Path(path)
              config = json.loads((path / "quantization_config.json").read_text())
              tensors = load_file(path / "model.safetensors")
              
              weights = {}
              bf16_weights = {}
              
              for name, tensor in tensors.items():
                  # Parse metadata from config to determine format
                  if name.endswith("_quantized"):
                      base_name = name[:-10]
                      # Load associated scales, etc.
                      ...
                  elif tensor.dtype == torch.bfloat16:
                      bf16_weights[name] = tensor
                  else:
                      weights[name] = tensor
              
              return cls(config=config, weights=weights, bf16_weights=bf16_weights)
      ```

      Test with GLM-4.7-Flash quantized model:
      ```bash
      uv run python -c "
      from metal_marlin.quantized_loader import QuantizedModel
      model = QuantizedModel.load('benchmarks/results/glm47_sensitivity_fp8_int2')
      print(f'Loaded {len(model.weights)} quantized tensors')
      print(f'Loaded {len(model.bf16_weights)} BF16 tensors')
      "
      ```

      File: contrib/metal_marlin/metal_marlin/quantized_loader.py
    priority: P0
    dependencies:
      - fix-marlin-gemm-store-results

  # =============================================================================
  # Phase 3: Fused Dequant + GEMM Kernel Integration
  # =============================================================================

  - name: implement-fp4-linear-layer
    prompt: |
      Create a PyTorch-compatible Linear layer that uses quantized weights with Metal kernels.

      Create metal_marlin/quantized_linear.py:
      ```python
      import torch
      import torch.nn as nn
      from .metal_dispatch import MetalKernelLibrary
      from .quantized_loader import QuantizedTensor

      class QuantizedLinear(nn.Module):
          \"\"\"Linear layer with FP4/INT4 quantized weights using Metal kernels.\"\"\"
          
          def __init__(
              self,
              quantized_weight: QuantizedTensor,
              bias: torch.Tensor | None = None,
          ):
              super().__init__()
              self.weight_data = quantized_weight.data
              self.weight_scales = quantized_weight.scales
              self.format = quantized_weight.format
              self.group_size = quantized_weight.group_size
              self.out_features, self.in_features = quantized_weight.original_shape
              self.bias = bias
              
              # Optional Hadamard for QuaRot-style quantization
              self.needs_hadamard = quantized_weight.needs_hadamard
              self.hadamard = quantized_weight.hadamard_matrix
              
              # Get Metal kernel library
              self._lib = MetalKernelLibrary.get_instance()
          
          def forward(self, x: torch.Tensor) -> torch.Tensor:
              # x: [batch, seq, in_features]
              batch_shape = x.shape[:-1]
              x_flat = x.view(-1, self.in_features)
              
              # Apply Hadamard rotation if needed
              if self.needs_hadamard and self.hadamard is not None:
                  x_flat = x_flat @ self.hadamard.to(x.device)
              
              # Dispatch to fused dequant+GEMM kernel
              out = self._dispatch_quantized_gemm(x_flat)
              
              if self.bias is not None:
                  out = out + self.bias
              
              return out.view(*batch_shape, self.out_features)
          
          def _dispatch_quantized_gemm(self, x: torch.Tensor) -> torch.Tensor:
              \"\"\"Dispatch to appropriate Metal kernel based on format.\"\"\"
              if self.format == "fp4":
                  return self._lib.fp4_gemm(
                      x, self.weight_data, self.weight_scales,
                      self.out_features, self.in_features, self.group_size
                  )
              elif self.format == "int4":
                  return self._lib.int4_gemm(...)
              # etc.
      ```

      File: contrib/metal_marlin/metal_marlin/quantized_linear.py
    priority: P0
    dependencies:
      - implement-quantized-model-loader

  - name: implement-metal-gemm-dispatch
    prompt: |
      Add fp4_gemm and int4_gemm methods to MetalKernelLibrary for quantized GEMM.

      Update metal_marlin/metal_dispatch.py to add:
      ```python
      class MetalKernelLibrary:
          # ... existing code ...
          
          def fp4_gemm(
              self,
              input: torch.Tensor,      # [M, K] input activations
              weight: torch.Tensor,     # Packed FP4 weights
              scales: torch.Tensor,     # Per-group scales
              N: int,                   # Output features
              K: int,                   # Input features
              group_size: int = 128,
          ) -> torch.Tensor:
              \"\"\"Fused FP4 dequantize + GEMM using marlin_gemm kernel.\"\"\"
              M = input.shape[0]
              
              # Allocate output
              output = torch.empty(M, N, dtype=input.dtype, device=input.device)
              
              # Get kernel
              kernel = self.get_kernel("marlin_gemm", "marlin_gemm_fp4")
              
              # Get Metal buffers from MPS tensors
              input_buf = self._get_metal_buffer(input)
              weight_buf = self._get_metal_buffer(weight)
              scales_buf = self._get_metal_buffer(scales)
              output_buf = self._get_metal_buffer(output)
              
              # Compute grid dimensions
              tile_m, tile_n = 64, 128  # From marlin_gemm.metal
              grid = (div_ceil(M, tile_m), div_ceil(N, tile_n), 1)
              threadgroup = (256, 1, 1)  # 8 simdgroups
              
              # Dispatch
              self._dispatch(
                  kernel, grid, threadgroup,
                  input_buf, weight_buf, scales_buf, output_buf,
                  M, N, K, group_size
              )
              
              return output
      ```

      The key is using PyObjC to share MPS tensor buffers with Metal:
      ```python
      def _get_metal_buffer(self, tensor: torch.Tensor):
          \"\"\"Get MTLBuffer from MPS tensor (zero-copy).\"\"\"
          # Use tensor.data_ptr() to get buffer address
          # Then create MTLBuffer wrapping that memory
          ...
      ```

      File: contrib/metal_marlin/metal_marlin/metal_dispatch.py
    priority: P0
    dependencies:
      - fix-marlin-gemm-store-results

  # =============================================================================
  # Phase 4: Model Architecture Integration
  # =============================================================================

  - name: implement-quantized-glm-model
    prompt: |
      Create a quantized GLM-4.7-Flash model class that uses QuantizedLinear layers.

      Create metal_marlin/models/glm4.py:
      ```python
      from transformers import AutoConfig
      from ..quantized_linear import QuantizedLinear
      from ..quantized_loader import QuantizedModel
      import torch.nn as nn

      class QuantizedGLM4MoE(nn.Module):
          \"\"\"Quantized GLM-4.7-Flash for inference.\"\"\"
          
          def __init__(self, quantized_model: QuantizedModel):
              super().__init__()
              self.config = quantized_model.config
              
              # Build model structure matching HF architecture
              self.embed_tokens = nn.Embedding.from_pretrained(
                  quantized_model.bf16_weights["model.embed_tokens.weight"]
              )
              
              self.layers = nn.ModuleList()
              for i in range(self.config["num_hidden_layers"]):
                  layer = QuantizedGLM4Layer(quantized_model, i)
                  self.layers.append(layer)
              
              self.norm = nn.LayerNorm(self.config["hidden_size"])
              self.lm_head = QuantizedLinear(
                  quantized_model.weights["lm_head.weight"]
              )
          
          def forward(self, input_ids, attention_mask=None, past_key_values=None):
              hidden_states = self.embed_tokens(input_ids)
              
              for layer in self.layers:
                  hidden_states = layer(hidden_states, attention_mask, past_key_values)
              
              hidden_states = self.norm(hidden_states)
              logits = self.lm_head(hidden_states)
              
              return logits
          
          @classmethod
          def from_quantized(cls, path: str) -> "QuantizedGLM4MoE":
              quantized = QuantizedModel.load(path)
              return cls(quantized)
      ```

      File: contrib/metal_marlin/metal_marlin/models/glm4.py
    priority: P1
    dependencies:
      - implement-fp4-linear-layer

  - name: implement-quantized-moe-layer
    prompt: |
      Implement the MoE layer for quantized GLM-4.7-Flash.

      Create metal_marlin/models/moe.py:
      ```python
      class QuantizedMoELayer(nn.Module):
          \"\"\"MoE layer with quantized expert weights.\"\"\"
          
          def __init__(
              self,
              num_experts: int,
              num_experts_per_tok: int,
              quantized_experts: list[QuantizedLinear],  # 64 experts for GLM
              gate: nn.Linear,  # Router kept in BF16
              shared_expert: QuantizedLinear | None = None,
          ):
              super().__init__()
              self.num_experts = num_experts
              self.num_experts_per_tok = num_experts_per_tok
              self.experts = nn.ModuleList(quantized_experts)
              self.gate = gate
              self.shared_expert = shared_expert
          
          def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
              batch_size, seq_len, hidden_dim = hidden_states.shape
              hidden_flat = hidden_states.view(-1, hidden_dim)
              
              # Route tokens to experts (gate is BF16)
              router_logits = self.gate(hidden_flat)
              routing_weights, selected_experts = torch.topk(
                  router_logits.softmax(dim=-1),
                  self.num_experts_per_tok,
                  dim=-1
              )
              
              # Expert computation with quantized weights
              # Use batched dispatch for efficiency
              output = self._dispatch_experts(
                  hidden_flat, routing_weights, selected_experts
              )
              
              # Add shared expert if present
              if self.shared_expert is not None:
                  output = output + self.shared_expert(hidden_flat)
              
              return output.view(batch_size, seq_len, hidden_dim)
      ```

      File: contrib/metal_marlin/metal_marlin/models/moe.py
    priority: P1
    dependencies:
      - implement-quantized-glm-model

  # =============================================================================
  # Phase 5: Multi-Model Support
  # =============================================================================

  - name: implement-quantized-qwen-model
    prompt: |
      Create quantized model class for Qwen3 MoE models.

      Qwen3-30B-A3B has similar MoE structure to GLM-4.7-Flash:
      - 48 layers
      - 64 experts per layer
      - 8 experts active per token
      - Shared expert

      Create metal_marlin/models/qwen3.py following the same pattern as glm4.py.

      Key differences from GLM:
      - Uses YaRN RoPE for extended context
      - Different attention architecture details

      File: contrib/metal_marlin/metal_marlin/models/qwen3.py
    priority: P1
    dependencies:
      - implement-quantized-moe-layer

  - name: implement-quantized-llama-model
    prompt: |
      Create quantized model class for Llama 3.x dense models.

      Support models:
      - meta-llama/Llama-3.2-3B
      - meta-llama/Llama-3.2-90B

      Dense models are simpler than MoE - no routing, just stacked transformer layers.

      Create metal_marlin/models/llama.py:
      ```python
      class QuantizedLlama(nn.Module):
          \"\"\"Quantized Llama for inference.\"\"\"
          
          def __init__(self, quantized_model: QuantizedModel):
              # Standard transformer stack with QuantizedLinear
              ...
      ```

      File: contrib/metal_marlin/metal_marlin/models/llama.py
    priority: P1
    dependencies:
      - implement-fp4-linear-layer

  - name: implement-quantized-mixtral-model
    prompt: |
      Create quantized model class for Mixtral 8x7B MoE.

      Mixtral differences from GLM/Qwen MoE:
      - 8 experts (not 64)
      - 2 experts per token
      - No shared expert

      Create metal_marlin/models/mixtral.py.

      File: contrib/metal_marlin/metal_marlin/models/mixtral.py
    priority: P2
    dependencies:
      - implement-quantized-moe-layer

  - name: implement-quantized-deepseek-model
    prompt: |
      Create quantized model class for DeepSeek models.

      Support DeepSeek-R1-Distill-Qwen-32B (dense model based on Qwen architecture).

      Create metal_marlin/models/deepseek.py.

      File: contrib/metal_marlin/metal_marlin/models/deepseek.py
    priority: P2
    dependencies:
      - implement-quantized-qwen-model

  # =============================================================================
  # Phase 6: Inference Engine
  # =============================================================================

  - name: implement-inference-engine
    prompt: |
      Create a high-level inference engine for quantized models.

      Create metal_marlin/inference.py:
      ```python
      from transformers import AutoTokenizer
      from .quantized_loader import QuantizedModel
      from .models import QuantizedGLM4MoE, QuantizedQwen3, QuantizedLlama

      class MetalInferenceEngine:
          \"\"\"High-level inference API for quantized models.\"\"\"
          
          MODEL_CLASSES = {
              "glm4_moe_lite": QuantizedGLM4MoE,
              "qwen3_moe": QuantizedQwen3,
              "llama": QuantizedLlama,
          }
          
          def __init__(self, model_path: str, device: str = "mps"):
              self.quantized = QuantizedModel.load(model_path)
              self.model = self._load_model()
              self.tokenizer = AutoTokenizer.from_pretrained(
                  self.quantized.config.get("base_model_id")
              )
              self.device = device
          
          def _load_model(self):
              arch = self.quantized.config.get("architecture")
              model_class = self.MODEL_CLASSES.get(arch)
              if model_class is None:
                  raise ValueError(f"Unsupported architecture: {arch}")
              return model_class(self.quantized)
          
          def generate(
              self,
              prompt: str,
              max_tokens: int = 100,
              temperature: float = 0.7,
              top_p: float = 0.9,
          ) -> str:
              input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
              input_ids = input_ids.to(self.device)
              
              # Autoregressive generation loop
              for _ in range(max_tokens):
                  logits = self.model(input_ids)
                  next_token = self._sample(logits[:, -1], temperature, top_p)
                  if next_token == self.tokenizer.eos_token_id:
                      break
                  input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)
              
              return self.tokenizer.decode(input_ids[0])
      ```

      File: contrib/metal_marlin/metal_marlin/inference.py
    priority: P1
    dependencies:
      - implement-quantized-glm-model

  # =============================================================================
  # Phase 7: Quantization Pipeline for New Models
  # =============================================================================

  - name: quantize-qwen3-30b
    prompt: |
      Quantize Qwen/Qwen3-30B-A3B using the sensitivity-aware pipeline.

      ```bash
      cd contrib/metal_marlin
      uv run python benchmarks/bench_glm47_sensitivity_mixed.py \
          --model Qwen/Qwen3-30B-A3B \
          --output benchmarks/results/qwen3_30b_sensitivity \
          --samples 20 \
          -v
      ```

      Verify:
      1. Compression ratio ~9x
      2. Perplexity degradation <5%
      3. All weights saved to safetensors
    priority: P2
    dependencies:
      - implement-quantized-qwen-model

  - name: quantize-mixtral-8x7b
    prompt: |
      Quantize mistralai/Mixtral-8x7B-v0.1.

      Use similar sensitivity config but adapted for 8-expert MoE:
      - FP4 for expert weights
      - BF16 for router
      - FP4 for attention

      ```bash
      cd contrib/metal_marlin
      uv run python benchmarks/bench_glm47_sensitivity_mixed.py \
          --model mistralai/Mixtral-8x7B-v0.1 \
          --output benchmarks/results/mixtral_8x7b_sensitivity \
          --config mixtral_fp4_moe \
          --samples 20 \
          -v
      ```
    priority: P2
    dependencies:
      - implement-quantized-mixtral-model

  - name: quantize-llama-3-90b
    prompt: |
      Quantize meta-llama/Llama-3.2-90B-Instruct.

      Dense model - use uniform FP4 quantization with Hadamard pre-processing
      for layers with high kurtosis.
    priority: P3
    dependencies:
      - implement-quantized-llama-model

  # =============================================================================
  # Phase 8: Testing & Benchmarks
  # =============================================================================

  - name: test-quantized-inference-glm
    prompt: |
      Test end-to-end quantized inference for GLM-4.7-Flash.

      Create tests/test_quantized_inference.py:
      ```python
      def test_glm_quantized_generates():
          \"\"\"Test that quantized GLM generates coherent text.\"\"\"
          from metal_marlin.inference import MetalInferenceEngine
          
          engine = MetalInferenceEngine("benchmarks/results/glm47_sensitivity_fp8_int2")
          output = engine.generate("The capital of France is", max_tokens=20)
          
          assert "Paris" in output, f"Expected 'Paris' in output: {output}"
          assert len(output) > 30, "Output too short"

      def test_glm_quantized_perplexity():
          \"\"\"Test that quantized model perplexity is within 5% of BF16.\"\"\"
          # Compare against BF16 reference
          ...
      ```

      Run: `uv run pytest tests/test_quantized_inference.py -v`
    priority: P1
    dependencies:
      - implement-inference-engine

  - name: benchmark-throughput
    prompt: |
      Create throughput benchmark comparing quantized vs BF16 inference.

      Create benchmarks/bench_throughput.py:
      ```python
      def benchmark_model(
          model_path: str,
          prompt: str = "Explain quantum computing in simple terms.",
          num_tokens: int = 100,
          num_runs: int = 5,
      ) -> dict:
          \"\"\"Benchmark tokens/second for quantized model.\"\"\"
          ...
      ```

      Report:
      - Tokens/second (prefill)
      - Tokens/second (decode)
      - Memory usage (peak)
      - Latency (first token, p50, p99)

      File: contrib/metal_marlin/benchmarks/bench_throughput.py
    priority: P2
    dependencies:
      - test-quantized-inference-glm

  - name: benchmark-memory
    prompt: |
      Create memory benchmark comparing quantized vs BF16 models.

      Measure:
      - Model weight memory (expected: ~9x reduction)
      - KV cache memory
      - Peak activation memory
      - Total VRAM usage

      File: contrib/metal_marlin/benchmarks/bench_memory.py
    priority: P2
    dependencies:
      - test-quantized-inference-glm
