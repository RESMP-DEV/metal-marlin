# yaml-language-server: $schema=
# Phase 17: ONNX Support
# Load and convert ONNX models to Metal Marlin format

tasks:
  - name: onnx-weight-extractor
    prompt: |
      Create `metal_marlin/onnx_loader.py` - ONNX model weight extractor.

      Extract weights from ONNX models without running inference:
      ```python
      import onnx
      import numpy as np

      def extract_onnx_weights(
          onnx_path: str,
      ) -> Iterator[tuple[str, np.ndarray, dict]]:
          """
          Load ONNX model and yield (name, tensor, metadata) for each weight.

          Maps ONNX initializer names to HuggingFace-style names:
          - "/model/layers.0/self_attn/q_proj/MatMul" -> "model.layers.0.self_attn.q_proj.weight"
          - "/model/embed_tokens/Gather" -> "model.embed_tokens.weight"
          """
          model = onnx.load(onnx_path)

          for initializer in model.graph.initializer:
              name = initializer.name
              tensor = onnx.numpy_helper.to_array(initializer)

              # Normalize name to HF style
              hf_name = normalize_onnx_name(name)

              yield hf_name, tensor, {"onnx_name": name}


      def normalize_onnx_name(onnx_name: str) -> str:
          """Convert ONNX node path to HuggingFace weight name."""
          # Strip leading slash, replace / with .
          # Handle common patterns: MatMul, Gather, etc.

      def get_onnx_config(onnx_path: str) -> ModelConfig:
          """Extract model config from ONNX metadata or infer from shapes."""
      ```

      Dependencies: `onnx` (add to optional deps)
    priority: P0
    dependencies: []

  - name: onnx-to-marlin-converter
    prompt: |
      Add ONNX conversion support to `metal_marlin/hf_loader.py`.

      ```python
      def convert_onnx_to_fp4(
          onnx_path: str,
          output_path: str,
          group_size: int = 128,
          mixed_precision: MixedPrecisionConfig | None = None,
          calibration: CalibrationDataset | None = None,
      ) -> dict[str, Any]:
          """
          Convert ONNX model to Metal Marlin FP4 format.

          1. Extract weights from ONNX
          2. Apply mixed-precision config (detect MoE layers)
          3. Quantize to FP4 with optional calibration
          4. Save as safetensors + config
          """
      ```

      CLI:
      ```bash
      python -m metal_marlin.hf_loader convert-onnx \
          model.onnx ./model-fp4/ \
          --mixed-precision moe \
          --calibration bartowski-v3
      ```
    priority: P1
    dependencies:
      - onnx-weight-extractor

  - name: onnx-graph-parser
    prompt: |
      Create `metal_marlin/onnx_graph.py` - ONNX graph structure parser.

      For future inference support, parse ONNX graph to understand execution order:
      ```python
      @dataclass
      class ONNXOp:
          name: str
          op_type: str  # MatMul, Softmax, LayerNorm, etc.
          inputs: list[str]
          outputs: list[str]
          attributes: dict[str, Any]

      def parse_onnx_graph(onnx_path: str) -> list[ONNXOp]:
          """Parse ONNX graph into execution order."""

      def detect_architecture(ops: list[ONNXOp]) -> str:
          """Detect model architecture from op patterns."""
          # Llama: RMSNorm, SwiGLU, RoPE
          # GPT: LayerNorm, GELU, absolute position
          # MoE: Router + TopK + Expert dispatch
      ```

      This enables architecture-agnostic inference later.
    priority: P2
    dependencies:
      - onnx-weight-extractor

  - name: onnx-validation
    prompt: |
      Create `tests/test_onnx_loader.py` - validate ONNX loading.

      Tests:
      1. `test_extract_small_onnx()`: Use a tiny ONNX model (create one with torch.onnx.export)
      2. `test_name_normalization()`: Verify ONNX to HF name mapping
      3. `test_onnx_to_fp4_conversion()`: Full conversion pipeline on small model
      4. `test_onnx_cli()`: CLI smoke test

      Create a minimal test ONNX model:
      ```python
      import torch
      import torch.nn as nn

      class TinyModel(nn.Module):
          def __init__(self):
              super().__init__()
              self.linear1 = nn.Linear(64, 32)
              self.linear2 = nn.Linear(32, 16)

          def forward(self, x):
              return self.linear2(torch.relu(self.linear1(x)))

      model = TinyModel()
      torch.onnx.export(model, torch.randn(1, 64), "tests/fixtures/tiny.onnx")
      ```

      Run: `uv run pytest tests/test_onnx_loader.py -v`
    priority: P1
    dependencies:
      - onnx-to-marlin-converter
