# yaml-language-server: $schema=
# Fix All 60 Test Failures
#
# Test Status Before: 928 passed, 60 failed
# Target: 988+ passed, 0 failed
#
# ============================================================
# WHY AGENT SWARM vs DIRECT FIX
# ============================================================
#
# These failures require agent swarm because:
#
# 1. INVESTIGATION DEPTH: Each category requires reading 500-2000 lines of
#    Metal shader + Python binding code to understand the bug. A human or
#    single agent would take hours per category; 13 agents parallelize this.
#
# 2. INDEPENDENT WORKSTREAMS: The 7 failure categories are mostly independent:
#    - Stripe kernel syntax errors don't affect U4 GEMM accuracy
#    - FP4 M=1 indexing is separate from mixed precision overflow
#    - Agents can fix these simultaneously without merge conflicts
#
# 3. TEST ITERATION COST: Full test suite takes 13+ minutes. Each agent
#    can run targeted tests (e.g., `pytest -k "u4"`) in <30 seconds,
#    allowing rapid iteration within their specific domain.
#
# 4. CROSS-REFERENCING: Fixes require comparing:
#    - Metal kernel vs Python reference implementation
#    - Packed bit layout vs dequantization logic
#    - CUDA Marlin original vs Metal port
#    Multiple agents can read different files simultaneously.
#
# 5. RISK ISOLATION: Kernel changes are high-risk. If one agent's fix
#    breaks something, it's isolated to their PR branch. Direct editing
#    without this structure risks cascading breakage.
#
# Categories:
#   1. FP4 M=1 single token (5 failures) - P0
#   2. U4 GEMM accuracy (14 failures) - P0
#   3. Stripe partition kernel (24 failures) - P0
#   4. Speculative decoding (3 failures) - P1 [FIXED DIRECTLY - simple test fixes]
#   5. Mixed precision (3 failures) - P1
#   6. Regression/Layers (5 failures) - P1
#   7. ONNX (1 failure) - P2

tasks:
  # ============================================================
  # CATEGORY 1: FP4 M=1 SINGLE TOKEN (P0)
  # ============================================================
  # Issue: Output correlation ~0.07 instead of >0.85 for M=1
  # Root cause: Indexing bug in scalar accumulation path when M=1

  - name: fix-fp4-m1-indexing
    prompt: |
      Fix FP4 GEMM producing random output when M=1 (single token inference).

      **Symptoms:**
      - test_fp4_single_token_correctness: correlation 0.098 (expected >0.85)
      - test_fp4_single_token_shapes: mean diff 9.22 > tolerance 1.92

      **Investigation:**
      1. Run the failing test with debug output:
         ```bash
         cd contrib/iq-vs-k-bench/metal_marlin
         PYTHONPATH=. uv run python -c "
         import mlx.core as mx
         import numpy as np
         from metal_marlin import pack_fp4_weights, quantized_linear
         
         M, K, N = 1, 128, 128
         x = mx.random.normal((M, K)).astype(mx.float16)
         w = mx.random.normal((K, N)).astype(mx.float16)
         packed, scales = pack_fp4_weights(w, group_size=32)
         
         # Reference
         ref = x @ w
         
         # Metal
         out = quantized_linear(x, packed, scales, group_size=32)
         
         print('x[:5]:', x[0,:5])
         print('ref[:5]:', ref[0,:5])
         print('out[:5]:', out[0,:5])
         print('Correlation:', float(mx.corrcoef(mx.stack([ref.flatten(), out.flatten()]))[0,1]))
         "
         ```

      2. Check `metal_marlin/metal_marlin.py` in `quantized_linear()`:
         - The issue is likely in how threadgroup tiles are indexed for M=1
         - When M < TILE_M, the tiling logic may skip rows incorrectly
         - Check if `m_tiles = 0` when M=1 and TILE_M=64

      3. Fix the kernel dispatch or add M=1 special case:
         - Option A: Fix tile calculation to handle M < TILE_M
         - Option B: Add scalar fallback for M=1 (no tiling)

      4. Verify all 5 failing tests pass:
         ```bash
         PYTHONPATH=. uv run python -m pytest tests/test_accuracy.py::TestGEMMAccuracy::test_fp4_single_token_correctness tests/test_accuracy.py::TestGEMMAccuracy::test_fp4_single_token_shapes -v
         ```
    priority: P0
    dependencies: []

  # ============================================================
  # CATEGORY 2: U4 GEMM ACCURACY (P0)
  # ============================================================
  # Issue: Metal U4 GEMM differs from reference by mean 1-5, max 2-20
  # Root cause: INT4 dequant or scale application differs

  - name: fix-u4-gemm-dequant
    prompt: |
      Fix U4 (INT4) GEMM accuracy vs reference implementation.

      **Symptoms:**
      - test_metal_u4_gemm_vs_reference: max diff 2-20, mean diff 1-5
      - Reference roundtrip tests PASS, so packing is correct
      - Metal kernel produces slightly different values

      **Investigation:**
      1. Compare U4 dequant in Metal vs Python reference:
         
         Python reference (in tests/test_accuracy.py):
         ```python
         def _u4_dequant_ref(packed, scales, zeros, group_size):
             # INT4 unsigned: value in [0, 15]
             # Dequant: (u4_val - zero_point) * scale
         ```
         
         Metal kernel:
         ```metal
         // Find the U4 dequant in marlin_gemm.metal or inline kernel
         // Check: zero_point handling, scale indexing, nibble extraction order
         ```

      2. Debug a single element:
         ```bash
         cd contrib/iq-vs-k-bench/metal_marlin
         PYTHONPATH=. uv run python -c "
         import mlx.core as mx
         from metal_marlin import pack_u4_weights, quantized_linear_u4
         
         # Minimal test: 1x4 @ 4x1
         w = mx.array([[1.0], [2.0], [3.0], [4.0]], dtype=mx.float16)
         packed, scales, zeros = pack_u4_weights(w, group_size=4)
         x = mx.ones((1, 4), dtype=mx.float16)
         
         ref = x @ w  # Should be 10.0
         out = quantized_linear_u4(x, packed, scales, zeros, group_size=4)
         print(f'ref={ref.item()}, out={out.item()}, diff={abs(ref.item()-out.item())}')
         "
         ```

      3. Common U4 bugs:
         - Nibble order: low nibble first vs high nibble first
         - Zero point: signed vs unsigned interpretation
         - Scale broadcast: per-group vs per-channel indexing

      4. Fix the kernel and verify:
         ```bash
         PYTHONPATH=. uv run python -m pytest tests/test_accuracy.py::TestU4GEMMAccuracy -v
         ```
    priority: P0
    dependencies: []

  - name: fix-u4-scale-indexing
    prompt: |
      Fix U4 GEMM scale indexing for different group sizes.

      **Symptoms:**
      - test_metal_u4_gemm_group_sizes[32/64/128/256]: all fail
      - Different group sizes produce different errors

      **Root cause likely:**
      Scale index calculation in Metal:
      ```metal
      uint scale_k = k_idx / GROUP_SIZE;
      half s = scales[scale_k * N + out_col];
      ```

      vs Python reference which may use different layout.

      **Fix:**
      1. Check scale tensor shape from pack_u4_weights()
      2. Verify Metal indexing matches the packed layout
      3. Test each group size individually
    priority: P0
    dependencies:
      - fix-u4-gemm-dequant

  # ============================================================
  # CATEGORY 3: STRIPE PARTITION KERNEL (P0)
  # ============================================================
  # Issue: Metal kernel compilation fails with syntax errors
  # Root cause: Inline kernel in metal_marlin.py has broken syntax

  - name: fix-striped-kernel-syntax
    prompt: |
      Fix Metal kernel compilation errors in striped GEMM kernel.

      **Error messages:**
      ```
      mlx/backend/metal/kernels/utils.h:1064:3: error: function definition is not allowed here
      mlx/backend/metal/kernels/utils.h:2008:29: error: automatic variable qualified with an address space
      mlx/backend/metal/kernels/utils.h:2015:51: error: function definition is not allowed here
      ```

      **Root cause:**
      The inline kernel string in `metal_marlin/metal_marlin.py` (around line 824) is being
      concatenated with MLX's kernel template in a way that causes parsing errors.

      **Investigation:**
      1. Find the inline kernel:
         ```bash
         grep -n "striped_source" metal_marlin/metal_marlin.py | head -5
         ```

      2. Check how MLX compiles custom kernels. The kernel string gets inserted into
         a template that includes MLX headers. If our kernel body contains top-level
         declarations, they'll error because they're inside a function.

      3. The kernel body must be pure statements, no:
         - Function definitions
         - Top-level `constant constexpr` declarations  
         - Kernel entry point declarations

      **Fix options:**
      A) Move constants into the kernel body as local constants
      B) Use MLX's proper custom kernel API with separate source file
      C) Pass constants as kernel parameters instead of hardcoding

      4. Test fix:
         ```bash
         PYTHONPATH=. uv run python -m pytest tests/test_stripe_partition.py::TestStripedVs2D::test_striped_matches_2d -v -x
         ```
    priority: P0
    dependencies: []

  - name: fix-k-parallel-reduction
    prompt: |
      Fix K-parallel atomic reduction in striped kernel.

      **After fixing syntax errors**, the K-parallel tests may still fail due to:
      1. Race conditions in atomic reduction
      2. Incorrect partial sum indexing
      3. Missing memory barriers

      **Verify the reduction logic:**
      ```metal
      // In striped kernel, parallel > 1 path:
      // 1. Each K-slice writes to reduction_buf[k_slice * M * N + row * N + col]
      // 2. Last slice (determined by atomic counter) sums all slices
      // 3. Need device memory barrier before reading other slices
      ```

      **Test incrementally:**
      ```bash
      # First with parallel=1 (no reduction)
      PYTHONPATH=. uv run python -m pytest tests/test_stripe_partition.py::TestStripedVs2D::test_striped_matches_2d -v -k "1]"

      # Then parallel=2
      PYTHONPATH=. uv run python -m pytest tests/test_stripe_partition.py::TestKParallelReduction -v -k "2]"
      ```
    priority: P0
    dependencies:
      - fix-striped-kernel-syntax

  # ============================================================
  # CATEGORY 4: SPECULATIVE DECODING (P1)
  # ============================================================

  - name: fix-speculative-adaptive-cap
    prompt: |
      Fix speculative engine exceeding max speculation tokens.

      **Failure:**
      - test_adaptive_never_above_maximum: `assert 8 <= 4`
      - `engine.current_num_spec` is 8, but config has `num_speculative_tokens=4`

      **Root cause:**
      In `metal_marlin/speculative/engine.py`, the adaptive logic increases
      `current_num_spec` above the configured maximum.

      **Fix:**
      Find the adaptive update code:
      ```python
      def _update_num_spec(self, acceptance_rate: float):
          # Add cap to configured maximum
          self.current_num_spec = min(
              self.config.num_speculative_tokens,  # NOT max_speculative_tokens
              self.current_num_spec + 1
          )
      ```

      Actually, check if `num_speculative_tokens` vs `max_speculative_tokens` is confused.
      The test sets `num_speculative_tokens=4, max_speculative_tokens=8`.
      The engine should respect `num_speculative_tokens` as the cap.

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_speculative.py::TestSpeculativeEngine::test_adaptive_never_above_maximum -v
      ```
    priority: P1
    dependencies: []

  - name: fix-speculative-stats-import
    prompt: |
      Fix missing GenerationStats import in test.

      **Failure:**
      - test_generation_stats_accumulate: `NameError: name 'GenerationStats' is not defined`

      **Fix:**
      Add import to `tests/test_speculative.py`:
      ```python
      from metal_marlin.speculative.engine import SpeculativeEngine, GenerationStats
      ```

      Or if GenerationStats is defined elsewhere, find and import it.

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_speculative.py::TestSpeculativeEngine::test_generation_stats_accumulate -v
      ```
    priority: P1
    dependencies: []

  - name: fix-speculative-distribution-threshold
    prompt: |
      Fix statistical distribution preservation test threshold.

      **Failure:**
      - test_distribution_preserving_statistical: `abs(1.0 - 0.6) < 0.08` fails
      - Empirical distribution doesn't match target with 0.08 tolerance

      **Analysis:**
      This is a Monte Carlo test. Either:
      1. The rejection sampling in verify.py has a bug
      2. The test needs more samples for statistical convergence
      3. The tolerance is too tight

      **Debug:**
      ```python
      # In tests/test_speculative.py, find the test and increase samples
      # or check if the verify_speculative logic is correct
      ```

      **If rejection sampling is correct, relax tolerance:**
      - Increase num_samples from current value to 1000+
      - Or use chi-squared test instead of element-wise threshold

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_speculative.py::TestVerification::test_distribution_preserving_statistical -v
      ```
    priority: P1
    dependencies: []

  # ============================================================
  # CATEGORY 5: MIXED PRECISION (P1)
  # ============================================================

  - name: fix-mixed-precision-tests
    prompt: |
      Fix mixed precision overflow detection tests.

      **Failures:**
      - test_k32768_all_max_overflows_fp16
      - test_k32768_all_ones_no_overflow_fp16
      - test_k_parallel_fp32_no_benefit

      **Analysis:**
      These tests verify that FP32 accumulator prevents overflow for large K.
      The test logic compares FP16-accumulated vs FP32-accumulated results.

      **Common issues:**
      1. Reference computation itself overflows (see RuntimeWarning in output)
      2. Tolerance too tight for near-overflow values
      3. Test expects specific overflow behavior that differs on Metal

      **Fix approach:**
      1. Check if test's reference calculation is numerically stable
      2. Adjust test to use values that don't overflow the reference
      3. Or loosen tolerance for near-max-float values

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_mixed_precision.py -v
      ```
    priority: P1
    dependencies: []

  # ============================================================
  # CATEGORY 6: REGRESSION/LAYERS (P1)
  # ============================================================

  - name: fix-identity-weight-passthrough
    prompt: |
      Fix identity weight passthrough test - RMSE 0.97 > 0.1.

      **Failure:**
      - test_identity_weight_passthrough: RMSE 0.97 (should be < 0.1)

      **Test intent:**
      With identity-like weights (diagonal 1s, off-diagonal 0s),
      output should approximately equal input after quantization noise.

      **Debug:**
      ```python
      # The test creates identity matrix, quantizes it, and checks output
      # RMSE 0.97 suggests output is nearly unrelated to input
      ```

      **Likely causes:**
      1. Quantization destroys identity structure (FP4 can't represent 0.0 exactly)
      2. Scale computation incorrect for sparse weights
      3. Same kernel bug as FP4 M=1

      **Fix:**
      - If FP4 can't represent identity well, adjust test to use larger values
      - If kernel bug, fix underlying issue

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_regression.py::TestNumericalStability::test_identity_weight_passthrough -v
      ```
    priority: P1
    dependencies:
      - fix-fp4-m1-indexing

  - name: fix-group-size-consistency
    prompt: |
      Fix group size consistency test - relative error is inf.

      **Failures:**
      - test_group_size_consistency[32/64/128]: relative error = inf

      **Analysis:**
      `inf` relative error means division by zero or NaN output.
      The test compares output with different group sizes.

      **Debug:**
      ```bash
      PYTHONPATH=. uv run python -c "
      import mlx.core as mx
      from metal_marlin import pack_fp4_weights, quantized_linear

      M, K, N = 32, 256, 128
      x = mx.random.normal((M, K)).astype(mx.float16)
      w = mx.random.normal((K, N)).astype(mx.float16)

      for gs in [32, 64, 128]:
          packed, scales = pack_fp4_weights(w, group_size=gs)
          out = quantized_linear(x, packed, scales, group_size=gs)
          print(f'gs={gs}: has_nan={mx.any(mx.isnan(out)).item()}, has_inf={mx.any(mx.isinf(out)).item()}')
      "
      ```

      **Fix based on debug output.**

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_regression.py::TestNumericalStability::test_group_size_consistency -v
      ```
    priority: P1
    dependencies:
      - fix-fp4-m1-indexing

  - name: fix-marlin-linear-layer
    prompt: |
      Fix MarlinLinear layer producing constant output after row 1.

      **Failure:**
      - test_from_linear: output rows 2+ are nearly identical

      **Output pattern:**
      ```
      Row 0: [-0.35, -1.33, 0.67, ...]  # Different
      Row 1: [-0.03, 0.03, 0.04, ...]   # Same as below
      Row 2: [-0.03, 0.03, 0.04, ...]   # Same
      ...
      ```

      **Root cause:**
      This is the same M=1 bug manifesting differently. When processing
      multiple rows, only the first row gets computed correctly.

      **Debug:**
      The kernel likely has:
      ```metal
      if (out_row < M && out_col < N) {
          // This condition may pass for row 0 but routing breaks for rows 1+
      }
      ```

      **Fix:**
      Same as fix-fp4-m1-indexing - the tiling/indexing bug.

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_layers.py::TestMarlinLinear::test_from_linear -v
      ```
    priority: P1
    dependencies:
      - fix-fp4-m1-indexing

  # ============================================================
  # CATEGORY 7: ONNX (P2)
  # ============================================================

  - name: fix-onnx-quantized-executor
    prompt: |
      Fix ONNX quantized executor producing zeros.

      **Failure:**
      - test_quantized_vs_fp16_closeness: 87.5% elements are 0 in output

      **Output:**
      ```
      ACTUAL: [[ 7.75, -5.93, 0, 0, 0, 0, 0, 0, ...]]  # Mostly zeros
      DESIRED: [[ 7.67, -9.61, 1.93, -12.09, ...]]    # All nonzero
      ```

      **Debug:**
      ```bash
      PYTHONPATH=. uv run python -c "
      from converters.onnx_executor import ONNXExecutor
      # Create minimal ONNX graph and trace execution
      "
      ```

      **Likely cause:**
      - Output tensor not being filled correctly
      - Early termination in quantized_linear call
      - Shape mismatch causing truncation

      **Test:**
      ```bash
      PYTHONPATH=. uv run python -m pytest tests/test_onnx.py::TestMatmulQuantized::test_quantized_vs_fp16_closeness -v
      ```
    priority: P2
    dependencies:
      - fix-fp4-m1-indexing
