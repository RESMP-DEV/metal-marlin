# yaml-language-server: $schema=
# Phase 18: Multi-Model Benchmark Suite
# KL Divergence, Perplexity, Throughput for target models

tasks:
  - name: benchmark-framework
    prompt: |
      Create `metal_marlin/benchmark_models.py` - unified benchmark runner.

      ```python
      @dataclass
      class BenchmarkResult:
          model_name: str
          model_type: str  # dense, moe, moe-mtp
          params_total: int
          params_active: int  # For MoE
          precision_config: str  # "fp4-uniform" or "mixed-precision"

          # Quality metrics (compare FP4 vs FP16 reference)
          perplexity_fp16: float
          perplexity_fp4: float
          ppl_delta_pct: float  # (fp4 - fp16) / fp16 * 100

          kl_divergence_mean: float
          kl_divergence_max: float

          # Speed metrics
          throughput_tok_per_sec: float  # tokens/second
          prefill_latency_ms: float  # time to first token
          memory_gb: float  # peak GPU memory

          # Compression
          compression_ratio: float
          quantization_time_sec: float

      def benchmark_model(
          model_id: str,
          preset: str = "auto",  # auto-detect dense/moe/moe-mtp
          calibration: str = "bartowski-v3",
          samples: int = 100,
          max_length: int = 512,
      ) -> BenchmarkResult:
          """
          Full benchmark pipeline:
          1. Download FP16 model
          2. Run FP16 perplexity baseline
          3. Quantize to FP4 (mixed-precision for MoE)
          4. Run FP4 perplexity
          5. Compute KL divergence (FP16 vs FP4 distributions)
          6. Measure throughput
          """

      def benchmark_models_parallel(
          model_ids: list[str],
          output_json: str,
          **kwargs,
      ) -> list[BenchmarkResult]:
          """Run multiple models, save results to JSON."""
      ```

      CLI:
      ```bash
      python -m metal_marlin.benchmark_models \
          --models "zai-org/GLM-4.7-Flash,Qwen/Qwen3-30B-A3B" \
          --output results.json \
          --calibration bartowski-v3
      ```
    priority: P0
    dependencies: []

  - name: benchmark-glm47-flash
    prompt: |
      Benchmark GLM-4.7-Flash (MoE + MTP model).

      Model: `zai-org/GLM-4.7-Flash`
      - Architecture: MoE with MTP (Multi-Token Prediction)
      - Total params: ~9B
      - Active params: ~2B per token
      - 64 experts, 2 active per token
      - MTP heads for speculative decoding

      Tasks:
      1. Download model from HuggingFace
      2. Analyze layer structure with `mixed_precision.analyze_model_layers()`
      3. Quantize with `MixedPrecisionConfig.default_moe_mtp()`:
         - Router: FP16
         - Shared expert: FP4/g64
         - Routed experts: FP4/g128
         - MTP heads: FP4/g256
      4. Compute perplexity on Bartowski v3 calibration data
      5. Compute KL divergence vs FP16 reference
      6. Measure throughput (tok/s) on batch size 1

      Expected results to verify:
      - PPL delta should be < 5% with mixed-precision
      - KL divergence mean < 0.15
      - Compression > 3.5x

      Save results to `benchmarks/results/glm47_flash.json`
    priority: P1
    dependencies:
      - benchmark-framework

  - name: benchmark-qwen3-30b
    prompt: |
      Benchmark Qwen3-30B-A3B (MoE model).

      Model: `Qwen/Qwen3-30B-A3B`
      - Architecture: MoE
      - Total params: 30B
      - Active params: 3B per token
      - Sparse MoE with shared experts

      Tasks:
      1. Download model
      2. Quantize with `MixedPrecisionConfig.default_moe()`
      3. Benchmark PPL, KLD, throughput
      4. Compare uniform FP4 vs mixed-precision

      Save results to `benchmarks/results/qwen3_30b.json`
    priority: P1
    dependencies:
      - benchmark-framework

  - name: benchmark-qwen3-32b
    prompt: |
      Benchmark Qwen3-32B (Dense model).

      Model: `Qwen/Qwen3-32B`
      - Architecture: Dense transformer
      - Params: 32B
      - Standard Llama-style architecture

      Tasks:
      1. Download model
      2. Quantize with `MixedPrecisionConfig.default_dense()`
      3. Benchmark PPL, KLD, throughput
      4. Test multiple group sizes: 64, 128, 256

      Save results to `benchmarks/results/qwen3_32b.json`
    priority: P1
    dependencies:
      - benchmark-framework

  - name: benchmark-qwen3-3b
    prompt: |
      Benchmark Qwen3-3B (Small dense model, fast iteration).

      Model: `Qwen/Qwen3-3B`
      - Architecture: Dense transformer
      - Params: 3B
      - Good for quick validation

      Tasks:
      1. Download model
      2. Quantize to FP4/g128
      3. Full benchmark suite (should complete in <5 min)
      4. Validate benchmark framework correctness

      Save results to `benchmarks/results/qwen3_3b.json`
    priority: P0 # Run first as validation
    dependencies:
      - benchmark-framework

  - name: benchmark-nemotron-30b
    prompt: |
      Benchmark Nemotron-3-Nano-30B (MoE model from NVIDIA).

      Model: `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16`
      - Architecture: MoE (Mixture of Experts)
      - Total params: 30B
      - Active params: 3B per token
      - NVIDIA's efficient MoE variant

      Tasks:
      1. Download model from HuggingFace
      2. Analyze MoE layer structure
      3. Quantize with `MixedPrecisionConfig.default_moe()`:
         - Router: FP16
         - Shared expert: FP4/g64
         - Routed experts: FP4/g128
      4. Benchmark PPL, KLD vs FP16 reference
      5. Benchmark throughput (tok/s)

      Save results to `benchmarks/results/nemotron_30b.json`
    priority: P1
    dependencies:
      - benchmark-framework

  - name: benchmark-results-aggregator
    prompt: |
      Create `metal_marlin/benchmark_report.py` - aggregate and report results.

      ```python
      def aggregate_results(results_dir: str) -> pd.DataFrame:
          """Load all benchmark JSONs into a DataFrame."""

      def generate_markdown_report(df: pd.DataFrame) -> str:
          """Generate markdown table for README."""

      def plot_ppl_vs_compression(df: pd.DataFrame, output: str):
          """Scatter plot: X=compression ratio, Y=PPL delta %"""

      def plot_throughput_comparison(df: pd.DataFrame, output: str):
          """Bar chart: throughput by model"""
      ```

      Generate `benchmarks/RESULTS.md` with:
      - Summary table
      - Per-model detailed breakdown
      - Comparison: uniform vs mixed-precision
      - Comparison: WikiText-2 vs Bartowski v3 calibration
    priority: P2
    dependencies:
      - benchmark-glm47-flash
      - benchmark-qwen3-30b
      - benchmark-qwen3-32b
      - benchmark-qwen3-3b
