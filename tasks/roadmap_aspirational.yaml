# Metal Marlin Roadmap: Aspirational Goals
#
# This file contains longer-term goals and research directions.
# Not all tasks have concrete implementations yet - some require
# investigation and prototyping first.
#
# Categories:
# 1. Vision/Multimodal Support
# 2. Metal Kernel Optimization
# 3. Architecture-Specific Optimizations (MLA, MoE, Dense)
# 4. Emerging Architectures
# 5. Inference Pipeline Improvements

tasks:
  # =========================================================================
  # VISION & MULTIMODAL SUPPORT
  # =========================================================================

  - name: vision-encoder-quantization
    prompt: |
      Research and implement quantization for vision encoders in VLMs.

      Target models:
      - Qwen2-VL (ViT + cross-attention fusion)
      - LLaVA-1.5/1.6 (CLIP ViT-L/14)
      - InternVL2 (InternViT-6B)
      - Pixtral (custom vision encoder)

      Key challenges:
      1. Vision encoders are often more sensitive to quantization than LLMs
      2. Patch embeddings have different statistics than text embeddings
      3. Cross-attention layers bridge vision/text - critical precision

      Create `metal_marlin/vision/encoder_quant.py`:
      - Analyze vision encoder layer sensitivity
      - Implement vision-specific calibration (use image datasets, not text)
      - Support mixed precision: FP8 for vision, FP4 for LLM

      Calibration dataset: Use COCO or ImageNet subset for Hessian collection
      on vision encoder layers.
    priority: P1
    dependencies: []

  - name: vision-projector-handling
    prompt: |
      Implement proper handling for vision-language projectors.

      The projector maps vision features to LLM embedding space:
      - LLaVA: 2-layer MLP projector
      - Qwen2-VL: Perceiver resampler (cross-attention)
      - InternVL: QLLaMA-style projector

      Create `metal_marlin/vision/projector.py`:
      - Identify projector layers automatically from model config
      - Keep projectors at higher precision (FP8 or FP16)
      - Handle variable-length image token sequences

      Test with:
      - Single image input
      - Multi-image input (interleaved)
      - Video frame sequences
    priority: P1
    dependencies:
      - vision-encoder-quantization

  - name: image-preprocessing-metal
    prompt: |
      Implement Metal-accelerated image preprocessing.

      Current bottleneck: CPU-based image resize/normalize before inference.

      Create `metal_marlin/vision/preprocess.metal`:
      - GPU-accelerated image resize (bilinear/bicubic)
      - Normalization with model-specific mean/std
      - Patch extraction for ViT models
      - Dynamic resolution handling (Qwen2-VL style)

      Metal kernel considerations:
      - Use texture sampling for efficient interpolation
      - Process multiple images in batch
      - Support various input formats (RGB, BGR, grayscale)

      Expected speedup: 5-10x for preprocessing on M-series chips.
    priority: P2
    dependencies:
      - vision-encoder-quantization

  - name: multimodal-kv-cache
    prompt: |
      Extend KV cache for multimodal models.

      VLMs have unique caching requirements:
      - Image tokens are usually prepended/interleaved
      - Image token count varies with resolution
      - Cross-attention KV differs from self-attention KV

      Update `metal_marlin/paged/allocator.py`:
      - Track image vs text token boundaries
      - Support prefix caching for repeated image contexts
      - Handle dynamic image token counts

      Optimization: Cache vision encoder outputs for repeated image queries
      (same image, different questions).
    priority: P2
    dependencies:
      - vision-projector-handling

  # =========================================================================
  # METAL KERNEL OPTIMIZATION
  # =========================================================================

  - name: kernel-profiling-infrastructure
    prompt: |
      Create comprehensive Metal kernel profiling system.

      Current state: Basic timing, no deep analysis of GPU utilization.

      Create `metal_marlin/profiling/`:
      - `gpu_counters.py` - Read Metal performance counters
      - `occupancy.py` - Analyze threadgroup occupancy
      - `memory_bandwidth.py` - Measure actual vs theoretical bandwidth
      - `roofline.py` - Generate roofline model plots

      Metrics to capture:
      - ALU utilization (%)
      - Memory bandwidth utilization (%)
      - Cache hit rates (L1/L2)
      - Threadgroup occupancy
      - Stall reasons (memory, sync, ALU)

      Use Metal System Trace and GPU Performance State APIs.
      Output: JSON profiles compatible with chrome://tracing.
    priority: P0
    dependencies: []

  - name: simdgroup-matrix-optimization
    prompt: |
      Optimize kernels using Apple's simdgroup matrix operations.

      M1/M2/M3/M4 have hardware matrix units accessible via:
      - `simdgroup_matrix` types
      - `simdgroup_multiply_accumulate`

      Current kernels may not fully utilize these.

      Audit and optimize:
      1. `marlin_fp4_dequant.metal` - Use simdgroup for 8x8 blocks
      2. `fused_gemm.metal` - Leverage matrix hardware for D = A @ B + C
      3. `attention.metal` - QK^T and softmax @ V

      Reference: Apple's MPSGraph uses these for optimal GEMM.

      Target: Match or exceed MLX's matrix throughput.
    priority: P0
    dependencies:
      - kernel-profiling-infrastructure

  - name: tile-size-autotuning
    prompt: |
      Implement runtime tile size autotuning for kernels.

      Optimal tile sizes depend on:
      - GPU generation (M1 vs M2 vs M3 vs M4)
      - Problem size (small GEMM vs large GEMM)
      - Available shared memory
      - Register pressure

      Create `metal_marlin/autotuning/`:
      - `tile_search.py` - Grid search over tile configurations
      - `cache.py` - Persist optimal configs per (GPU, problem_size)
      - `heuristics.py` - Fast fallback when tuning not available

      Tile parameters to tune:
      - BLOCK_M, BLOCK_N, BLOCK_K for GEMM
      - Threadgroup size (threads per group)
      - Elements per thread
      - Shared memory allocation

      Run autotuning once on first use, cache results.
    priority: P1
    dependencies:
      - kernel-profiling-infrastructure

  - name: memory-coalescing-audit
    prompt: |
      Audit and fix memory access patterns in all kernels.

      Non-coalesced memory access is a major performance killer on GPUs.

      For each kernel, verify:
      1. Adjacent threads access adjacent memory addresses
      2. Strided access patterns are minimized
      3. Shared memory bank conflicts are avoided
      4. Texture cache is used where beneficial

      Create `metal_marlin/profiling/memory_audit.py`:
      - Static analysis of Metal shaders for access patterns
      - Runtime validation with synthetic workloads
      - Generate reports with optimization suggestions

      Priority kernels:
      - FP4 dequantization (sequential scale access)
      - Attention (strided Q/K/V access)
      - MoE dispatch (scattered expert access)
    priority: P1
    dependencies:
      - kernel-profiling-infrastructure

  - name: async-copy-optimization
    prompt: |
      Implement async memory copy for double-buffering.

      Current approach: Load → Compute → Store (sequential)
      Optimal: Load[i+1] || Compute[i] || Store[i-1] (pipelined)

      Metal supports async copies via:
      - `threadgroup_async_copy()` for threadgroup memory
      - Separate command buffers for copy vs compute

      Update kernels to use double-buffering:
      1. Declare 2x shared memory buffers
      2. Async load next tile while computing current
      3. Synchronize with `threadgroup_barrier(mem_flags::mem_threadgroup)`

      Expected improvement: 20-40% on memory-bound kernels.
    priority: P1
    dependencies:
      - simdgroup-matrix-optimization

  - name: sparse-kernel-optimization
    prompt: |
      Optimize kernels for sparse/structured sparsity patterns.

      MoE models are naturally sparse (only top-k experts active).
      Some models use explicit 2:4 or N:M sparsity.

      Create `metal_marlin/kernels/sparse/`:
      - `sparse_gemm.metal` - Sparse weight GEMM
      - `moe_dispatch.metal` - Efficient expert routing
      - `block_sparse.metal` - Block-sparse patterns

      Formats to support:
      - COO (coordinate) - flexible but slow
      - CSR (compressed row) - good for sparse GEMM
      - Block sparse - good for structured sparsity
      - 2:4 sparsity - NVIDIA style, needs Metal equivalent

      For MoE: Skip loading experts that aren't selected.
    priority: P2
    dependencies:
      - simdgroup-matrix-optimization

  # =========================================================================
  # MLA (MULTI-HEAD LATENT ATTENTION) OPTIMIZATION
  # =========================================================================

  - name: mla-architecture-analysis
    prompt: |
      Deep analysis of MLA architecture for quantization.

      MLA (used by DeepSeek-V2, GLM-4) compresses KV cache:
      - Standard MHA: KV cache = O(n_layers * seq_len * n_heads * head_dim)
      - MLA: KV cache = O(n_layers * seq_len * kv_lora_rank)

      Key components:
      1. `q_proj` - Standard Q projection
      2. `kv_a_proj_with_mqa` - Compress KV to latent space
      3. `kv_b_proj` - Decompress from latent for attention
      4. `q_a_proj` / `q_b_proj` - Optional Q compression

      Create `docs/architectures/mla.md`:
      - Diagram of MLA forward pass
      - Quantization sensitivity per layer type
      - Comparison: MLA vs GQA vs MQA memory usage
      - Recommendations for FP4/FP8/FP16 per layer

      Reference models: GLM-4.7-Flash, DeepSeek-V2
    priority: P0
    dependencies: []

  - name: mla-latent-projection-kernels
    prompt: |
      Create optimized kernels for MLA latent projections.

      MLA has unique GEMM patterns:
      - kv_a_proj: [batch, seq, hidden] → [batch, seq, kv_lora_rank]
      - kv_b_proj: [batch, seq, kv_lora_rank] → [batch, seq, n_kv_heads * head_dim]

      The latent dimension (kv_lora_rank) is typically small (512-1536).
      This means tall-skinny or short-wide matrices.

      Create `metal_marlin/kernels/mla_proj.metal`:
      - Fused kv_a + kv_b projection (skip intermediate materialization)
      - Optimized for small K dimension
      - Support quantized weights with FP16 activations

      Also fuse: RoPE can be applied in latent space before decompression.
    priority: P1
    dependencies:
      - mla-architecture-analysis

  - name: mla-kv-cache-optimization
    prompt: |
      Specialized KV cache for MLA models.

      MLA stores compressed KV representations:
      - Cache size per layer: seq_len * kv_lora_rank * dtype_size
      - Much smaller than standard: seq_len * n_heads * head_dim * dtype_size

      Create `metal_marlin/paged/mla_cache.py`:
      - Store compressed latents directly (don't decompress)
      - Decompress on-demand during attention
      - Support prefix caching with latent representations

      Memory comparison (seq_len=4096, n_heads=32, head_dim=128):
      - Standard: 4096 * 32 * 128 * 2 = 32 MB per layer
      - MLA (rank=512): 4096 * 512 * 2 = 4 MB per layer
      - 8x memory reduction!

      This is why MLA enables longer contexts on limited memory.
    priority: P1
    dependencies:
      - mla-latent-projection-kernels

  - name: mla-rope-latent-fusion
    prompt: |
      Fuse RoPE with MLA latent projections.

      Standard: hidden → kv_a_proj → latent → kv_b_proj → RoPE → KV
      Optimized: hidden → kv_a_proj → latent_RoPE → kv_b_proj → KV

      Apply RoPE in compressed space when mathematically equivalent.

      Update `metal_marlin/kernels/rope.metal`:
      - Support RoPE on small latent dimensions
      - Handle GLM's `rope_ratio` scaling
      - Fuse with projection kernel

      Not all MLA variants support this - document which do.
    priority: P2
    dependencies:
      - mla-latent-projection-kernels

  # =========================================================================
  # MOE (MIXTURE OF EXPERTS) OPTIMIZATION
  # =========================================================================

  - name: moe-routing-analysis
    prompt: |
      Analyze MoE routing patterns for optimization.

      Questions to answer:
      1. How balanced are expert loads in practice?
      2. Do certain experts get selected together frequently?
      3. Can we predict routing from early layers?

      Create `metal_marlin/analysis/moe_routing.py`:
      - Profile expert selection across dataset
      - Compute expert co-occurrence matrix
      - Identify hot/cold experts
      - Visualize routing patterns

      Use findings to:
      - Pre-load frequently used experts
      - Batch tokens going to same expert
      - Consider expert pruning (remove never-used experts)

      Test on: Qwen3-30B-A3B, Mixtral, DeepSeek-MoE
    priority: P0
    dependencies: []

  - name: moe-token-batching
    prompt: |
      Implement efficient token batching for MoE dispatch.

      Naive MoE: For each token, load top-k experts, compute, combine.
      Optimized: Group tokens by expert, batch compute.

      Create `metal_marlin/moe/token_dispatcher.py`:
      - `group_tokens_by_expert(tokens, routing_weights)` - Reorder tokens
      - `dispatch_to_experts(grouped_tokens, experts)` - Batched expert forward
      - `combine_expert_outputs(outputs, routing_weights)` - Reassemble

      Metal kernel: `moe_dispatch.metal`
      - Single kernel for dispatch + expert compute + combine
      - Minimize global memory traffic
      - Use shared memory for token routing tables

      Expected: 2-4x speedup vs naive per-token dispatch.
    priority: P0
    dependencies:
      - moe-routing-analysis

  - name: moe-expert-prefetching
    prompt: |
      Implement predictive expert prefetching.

      In autoregressive generation:
      - Token N's routing is known
      - Start loading experts for token N+1 while computing N

      Challenges:
      - Routing for N+1 depends on N's output
      - Can predict from attention patterns or use N's routing as hint

      Create `metal_marlin/moe/prefetch.py`:
      - `predict_next_experts(current_routing, attention_pattern)` - Heuristic
      - `async_load_experts(expert_ids)` - Background loading
      - `expert_cache` - LRU cache for hot experts

      Strategy: Preload top-1 expert from current token's routing.
      Most consecutive tokens share experts (locality).
    priority: P1
    dependencies:
      - moe-token-batching

  - name: moe-load-balancing-aware-quant
    prompt: |
      Quantize MoE experts based on load distribution.

      Not all experts are equal:
      - High-traffic experts: Need higher precision (more diverse inputs)
      - Low-traffic experts: Can use aggressive quantization
      - Shared expert (if any): Always high precision

      Create `metal_marlin/moe/adaptive_precision.py`:
      - Profile expert load distribution
      - Assign precision based on:
        - Traffic volume (more traffic → higher precision)
        - Input diversity (higher variance → higher precision)
        - Output importance (layer-specific)

      Output: Per-expert precision map for quantization

      This can reduce memory 20-30% vs uniform precision.
    priority: P1
    dependencies:
      - moe-routing-analysis

  - name: moe-capacity-factor-tuning
    prompt: |
      Implement runtime capacity factor adjustment.

      Capacity factor controls max tokens per expert:
      - capacity = n_tokens * capacity_factor / n_experts
      - Lower = faster but may drop tokens
      - Higher = accurate but slower

      Create `metal_marlin/moe/capacity.py`:
      - `analyze_overflow_rate()` - Measure token dropping
      - `auto_tune_capacity(target_drop_rate)` - Find optimal factor
      - `dynamic_capacity()` - Adjust per-batch

      For inference (not training), can often use capacity_factor=1.0
      since we process sequentially.
    priority: P2
    dependencies:
      - moe-token-batching

  # =========================================================================
  # DENSE MODEL OPTIMIZATION
  # =========================================================================

  - name: dense-gemm-tiling
    prompt: |
      Optimize GEMM tiling specifically for dense transformers.

      Dense models have predictable, regular GEMM shapes:
      - Q/K/V proj: [batch, seq, hidden] @ [hidden, n_heads * head_dim]
      - MLP up: [batch, seq, hidden] @ [hidden, intermediate]
      - MLP down: [batch, seq, intermediate] @ [intermediate, hidden]

      Create `metal_marlin/kernels/dense_gemm.metal`:
      - Pre-computed optimal tiles for common shapes
      - Split-K for tall-skinny matrices (prefill)
      - Strided for decode (batch_size=1)

      Special case: Llama/Qwen MLP uses gate * up, can fuse.
    priority: P1
    dependencies:
      - tile-size-autotuning

  - name: dense-layer-fusion
    prompt: |
      Fuse operations within dense transformer layers.

      Standard: RMSNorm → QKV_proj → RoPE → Attention → O_proj → Residual
      Fused: RMSNorm+QKV → RoPE+Attention → O_proj+Residual

      Create `metal_marlin/fusion/`:
      - `norm_linear.metal` - Fuse RMSNorm + Linear
      - `attention_residual.metal` - Fuse attention output + residual add
      - `gated_mlp.metal` - Fuse gate * up + activation + down

      Fusion reduces memory bandwidth:
      - Avoid writing/reading intermediate activations
      - Keep data in registers/shared memory

      Expected: 15-25% speedup on memory-bound inference.
    priority: P1
    dependencies:
      - dense-gemm-tiling

  - name: dense-prefill-optimization
    prompt: |
      Optimize prefill (prompt processing) for dense models.

      Prefill characteristics:
      - Large batch of tokens (prompt length)
      - Compute-bound (large GEMM)
      - KV cache write-heavy

      Optimizations:
      1. Parallel KV cache writing (don't serialize)
      2. Chunked prefill for very long prompts
      3. Flash Attention for O(seq) memory attention

      Create `metal_marlin/inference/prefill.py`:
      - `chunked_prefill(prompt, chunk_size)` - Process in chunks
      - `parallel_kv_write(k, v, cache)` - Concurrent cache update

      Also: Consider speculative prefill (predict and cache likely continuations).
    priority: P1
    dependencies:
      - dense-layer-fusion

  - name: dense-decode-optimization
    prompt: |
      Optimize decode (generation) for dense models.

      Decode characteristics:
      - Single token at a time
      - Memory-bound (small GEMM, large KV read)
      - Latency-critical (impacts tok/s)

      Optimizations:
      1. Persistent kernels (keep weights in cache between tokens)
      2. Fused attention (combine softmax + V matmul)
      3. Quantized KV cache (FP8 or INT8)

      Create `metal_marlin/inference/decode.py`:
      - `persistent_decode_step()` - Single kernel for full layer
      - `quantized_kv_attention()` - Attention with quantized cache

      Target: <5ms per token latency on M4 Max for 30B models.
    priority: P1
    dependencies:
      - dense-prefill-optimization

  # =========================================================================
  # EMERGING ARCHITECTURES
  # =========================================================================

  - name: differential-transformer-support
    prompt: |
      Add support for Differential Transformer architecture.

      Differential Transformer (Microsoft, 2024) modifies attention:
      - Two attention heads per "logical" head
      - Output = softmax(Q1 @ K1) @ V - λ * softmax(Q2 @ K2) @ V
      - Reduces noise, improves for long context

      Create `metal_marlin/architectures/diff_transformer.py`:
      - Parse differential attention config
      - Implement fused differential attention kernel
      - Handle λ parameter (learnable or fixed)

      Quantization considerations:
      - λ should remain FP16 (small, sensitive)
      - Q1/K1 and Q2/K2 may need different precision
    priority: P2
    dependencies: []

  - name: mamba-ssm-support
    prompt: |
      Add support for Mamba/SSM architectures.

      Mamba replaces attention with selective state spaces:
      - Linear time complexity in sequence length
      - No KV cache (state is recurrent)
      - Different compute pattern than transformers

      Create `metal_marlin/architectures/mamba.py`:
      - Selective scan kernel (`selective_scan.metal`)
      - State management for generation
      - Quantization of SSM parameters (A, B, C, D, dt)

      Models: Mamba, Mamba-2, Jamba (Mamba + Attention hybrid)

      Note: Mamba is more compute-bound, different optimization targets.
    priority: P2
    dependencies: []

  - name: rwkv-support
    prompt: |
      Add support for RWKV architecture.

      RWKV is RNN-like with transformer-like training:
      - Linear attention variant
      - Constant memory for generation
      - WKV operator is the core

      Create `metal_marlin/architectures/rwkv.py`:
      - WKV kernel (`rwkv_wkv.metal`)
      - Time mixing and channel mixing
      - State management

      Models: RWKV-5/6, Eagle (RWKV variant)

      RWKV quantization is tricky - time decay params are sensitive.
    priority: P3
    dependencies: []

  - name: megabyte-support
    prompt: |
      Research MegaByte/byte-level model support.

      Byte-level models (MegaByte, ByteT5):
      - No tokenizer needed
      - Process raw bytes
      - Hierarchical attention (global + local)

      Challenges:
      - Very long sequences (1 char = 1 token)
      - Different attention patterns
      - Patch embedding instead of token embedding

      Create `docs/architectures/byte_models.md`:
      - Document MegaByte architecture
      - Assess quantization compatibility
      - Identify Metal kernel requirements

      This is more research-oriented - byte models are emerging.
    priority: P3
    dependencies: []

  - name: hybrid-architecture-framework
    prompt: |
      Create framework for hybrid architectures.

      Modern models mix components:
      - Jamba: Mamba + Attention layers
      - StripedHyena: Hyena + Attention
      - Zamba: SSM + shared attention layers

      Create `metal_marlin/architectures/hybrid.py`:
      - Detect hybrid architecture from config
      - Route layers to appropriate kernels
      - Handle different state management per layer type

      Config format:
      ```python
      {
        "layer_types": ["attention", "mamba", "attention", "mamba", ...],
        "attention_config": {...},
        "mamba_config": {...}
      }
      ```

      Allow per-layer-type quantization config.
    priority: P2
    dependencies:
      - mamba-ssm-support

  # =========================================================================
  # INFERENCE PIPELINE IMPROVEMENTS
  # =========================================================================

  - name: speculative-decoding
    prompt: |
      Implement speculative decoding for faster generation.

      Speculative decoding:
      1. Small "draft" model generates K tokens quickly
      2. Large "target" model verifies in parallel
      3. Accept matching tokens, reject divergent

      Create `metal_marlin/speculative/`:
      - `draft_model.py` - Manage small draft model
      - `verifier.py` - Parallel verification kernel
      - `sampling.py` - Speculative sampling algorithm

      Key: Draft and target share tokenizer and produce compatible logits.

      Implementation options:
      - Self-draft: Use early exit from main model
      - Separate draft: Smaller quantized version
      - Medusa: Multiple prediction heads

      Expected: 2-3x speedup for repetitive/predictable text.
    priority: P1
    dependencies: []

  - name: continuous-batching
    prompt: |
      Implement continuous batching for serving.

      Static batching: Process batch, return all, repeat.
      Continuous: Add new requests mid-batch, return early finishers.

      Create `metal_marlin/serving/continuous_batch.py`:
      - `BatchScheduler` - Manage request queue
      - `IterationPlanner` - Plan per-iteration work
      - `KVCacheManager` - Allocate/free cache per request

      Requirements:
      - Track per-request KV cache regions
      - Handle variable sequence lengths in batch
      - Preemption for high-priority requests

      This is essential for production serving (vLLM-style).
    priority: P1
    dependencies: []

  - name: tensor-parallel-inference
    prompt: |
      Implement tensor parallelism for multi-GPU (future M-series).

      Current M-series: Single unified GPU
      Future possibility: Multi-die or external GPU

      Create `metal_marlin/distributed/`:
      - `tensor_parallel.py` - Split model across devices
      - `all_reduce.metal` - Cross-device reduction kernel
      - `pipeline_parallel.py` - Layer-wise distribution

      Split strategies:
      - Column parallel: Split weight columns, all-reduce after
      - Row parallel: Split weight rows, all-gather after

      For now: Prototype with CPU + GPU split (verification only).
    priority: P3
    dependencies: []

  - name: quantized-kv-cache
    prompt: |
      Implement quantized KV cache for memory savings.

      Standard: FP16 KV cache = 2 * n_layers * seq_len * n_kv_heads * head_dim * 2
      Quantized: FP8/INT8 = half the memory

      Create `metal_marlin/cache/quantized_kv.py`:
      - `QuantizedKVCache` - FP8 storage with FP16 compute
      - `compress_kv(k, v)` - Quantize incoming KV
      - `decompress_kv(k_q, v_q)` - Dequant for attention

      Options:
      - Per-head scaling (better accuracy)
      - Per-token scaling (simpler)
      - Asymmetric: FP8 K, INT8 V (V more robust)

      Expected: 2x longer context in same memory.
    priority: P1
    dependencies: []

  - name: prompt-caching
    prompt: |
      Implement prompt caching / prefix caching.

      Common patterns:
      - Same system prompt for all requests
      - Few-shot examples repeated
      - Chat history grows incrementally

      Create `metal_marlin/cache/prompt_cache.py`:
      - `PrefixCache` - Store KV for common prefixes
      - `hash_prefix(tokens)` - Efficient prefix matching
      - `extend_from_cache(prefix_kv, new_tokens)` - Reuse cached

      Storage options:
      - In-memory LRU cache
      - Disk persistence for server restart
      - Hierarchical: hot (GPU) / warm (RAM) / cold (disk)

      vLLM's PagedAttention has good prefix caching - reference.
    priority: P1
    dependencies:
      - continuous-batching

  - name: structured-generation
    prompt: |
      Implement constrained/structured generation.

      For JSON, regex, or grammar-constrained output:
      - Modify logits to zero out invalid tokens
      - Guide generation to match schema

      Create `metal_marlin/guided/`:
      - `json_schema.py` - JSON schema → valid token sets
      - `regex.py` - Regex → DFA → token masks
      - `grammar.py` - CFG → token constraints
      - `logit_processor.py` - Apply constraints efficiently

      Metal kernel: Parallel logit masking

      Reference: outlines, guidance, jsonformer patterns.
    priority: P2
    dependencies: []
