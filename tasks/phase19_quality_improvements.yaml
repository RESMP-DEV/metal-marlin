# yaml-language-server: $schema=
# Phase 19: Quality & Consistency Improvements
#
# Addresses:
# - BF16 support on Metal (Apple silicon natively supports BF16)
# - Full calibration dataset (no sample limits)
# - Sub-4-bit quantization (2-bit, 3-bit) for MoE non-sensitive layers
# - KL divergence measurement for proper quality assessment
# - Layer-wise quantization (memory efficient)
# - vLLM/serving engine alignment

tasks:
  # ============================================================================
  # BF16 Support
  # ============================================================================
  - name: bf16-precision-support
    prompt: |
      Add BF16 (bfloat16) precision support to Metal Marlin.

      Apple Silicon M1+ natively supports BF16. Currently we hardcode FP16 everywhere.

      Files to modify:
      - `metal_marlin/mixed_precision.py`: Add `BF16 = "bf16"` to Precision enum
      - `metal_marlin/quantize.py`: Support BF16 input weights
      - `metal_marlin/kernels.py`: Add BF16 variants of Metal shaders
      - `metal_marlin/inference.py`: Use BF16 for activations where supported

      Reference vLLM's approach:
      - https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization

      BF16 advantages over FP16:
      - Same dynamic range as FP32 (8-bit exponent vs 5-bit)
      - Better for large values common in attention
      - Native support on Apple Silicon M1+

      Update `MixedPrecisionConfig` presets to use BF16 for:
      - Embeddings (FP16 -> BF16)
      - Norms (FP16 -> BF16)  
      - lm_head (FP16 -> BF16)
      - MoE router (FP16 -> BF16)

      Test: Verify MLX accepts mx.bfloat16 dtype on Apple Silicon.
    priority: P1
    dependencies: []

  # ============================================================================
  # Sub-4-bit Quantization
  # ============================================================================
  - name: sub4bit-quantization
    prompt: |
      Add 2-bit and 3-bit quantization for MoE non-sensitive layers.

      For MoE models:
      - 64 experts, only 2 active per token
      - 62 "cold" experts can be quantized aggressively
      - Community benchmarks show 2-bit works fine for routed experts

      Create `metal_marlin/sub4bit.py` with:

      1. INT2 quantization (4 levels: -1, 0, 0.5, 1 scaled)
         - Pack 16 weights per uint32
         - Per-group scales (group_size=64 recommended)

      2. INT3 quantization (8 levels)
         - Pack 10 weights per uint32 (with 2 bits padding)
         - Better quality than INT2, still 25% smaller than INT4

      3. NF2/NF3 (NormalFloat) variants
         - Non-uniform quantization levels based on normal distribution
         - Better for transformer weights which are roughly Gaussian

      Reference implementations:
      - llama.cpp IQ2_XXS, IQ3_XXS quants (proven quality)
      - bitsandbytes NF4 approach

      Update `mixed_precision.py`:
      - Add `INT2`, `INT3`, `NF2`, `NF3` to Precision enum
      - Default MoE experts to INT3 or NF3
      - Keep shared expert at FP4/INT4

      Update `kernels.py`:
      - Add Metal shaders for INT2/INT3 dequantization
      - Optimize for M1/M2/M3 SIMD groups
    priority: P0
    dependencies: []

  - name: sub4bit-kernels
    prompt: |
      Implement Metal kernels for 2-bit and 3-bit dequantization.

      In `metal_marlin/kernels.py`, add:

      ```metal
      // INT2 dequantization: 16 weights per uint32
      kernel void dequant_int2(
          device const uint32_t* packed [[buffer(0)]],
          device const half* scales [[buffer(1)]],
          device half* output [[buffer(2)]],
          uint gid [[thread_position_in_grid]]
      ) {
          uint32_t word = packed[gid];
          half scale = scales[gid / GROUP_SIZE];
          
          for (int i = 0; i < 16; i++) {
              int2_val = (word >> (i * 2)) & 0x3;  // 0, 1, 2, 3
              // Map to {-1.0, -0.33, 0.33, 1.0}
              float dequant = (float(int2_val) - 1.5f) * 0.667f;
              output[gid * 16 + i] = half(dequant * float(scale));
          }
      }
      ```

      Also add INT3 kernel (10 weights per uint32, 2 bits unused).

      Benchmark against FP4 kernel to ensure no throughput regression.
      Target: Same tok/s as FP4 (memory bandwidth limited).
    priority: P0
    dependencies:
      - sub4bit-quantization

  # ============================================================================
  # Full Calibration Dataset
  # ============================================================================
  - name: full-calibration-dataset
    prompt: |
      Remove sample limits from calibration - use the FULL dataset.

      Current code in `benchmark_models.py` and related files limits samples:
      ```python
      texts = load_calibration_data(calibration, samples)  # samples=100 default
      ```

      This is wrong. For accurate quantization:
      1. Calibration should use ALL available samples
      2. Perplexity evaluation can use a subset (for speed)

      Files to modify:

      1. `metal_marlin/benchmark_models.py`:
         - `load_calibration_data()`: Remove `num_samples` parameter for calibration
         - Add separate `load_eval_data(num_samples=100)` for evaluation

      2. `metal_marlin/hf_loader.py`:
         - `convert_model_to_fp4()`: Accept full calibration iterator
         - Process in batches to avoid OOM

      3. Add streaming support:
         ```python
         def load_calibration_stream(dataset: str) -> Iterator[str]:
             '''Stream calibration data without loading all into memory.'''
             if dataset == "bartowski-v3":
                 from datasets import load_dataset
                 ds = load_dataset("Bartowski/calibration-v3", split="train", streaming=True)
                 for ex in ds:
                     if len(ex.get("text", "")) > 50:
                         yield ex["text"]
         ```

      Bartowski calibration v3 has ~800+ samples. Use ALL of them.
    priority: P1
    dependencies: []

  # ============================================================================
  # KL Divergence Measurement
  # ============================================================================
  - name: kl-divergence-measurement
    prompt: |
      Add proper KL divergence measurement for quantization quality.

      KL divergence measures how much the quantized model's probability 
      distribution differs from the original. This is MORE informative than
      perplexity for quantization quality assessment.

      Create `metal_marlin/eval_kl_divergence.py`:

      ```python
      def compute_kl_divergence(
          original_logits: mx.array,  # [batch, seq, vocab]
          quantized_logits: mx.array,
          temperature: float = 1.0,
      ) -> tuple[float, float, float]:
          '''
          Compute KL(P_original || P_quantized).
          
          Returns: (kl_mean, kl_max, kl_std)
          '''
          # Softmax with temperature
          p_orig = mx.softmax(original_logits / temperature, axis=-1)
          p_quant = mx.softmax(quantized_logits / temperature, axis=-1)
          
          # KL divergence: sum(p * log(p/q))
          # Add epsilon for numerical stability
          eps = 1e-10
          kl = p_orig * (mx.log(p_orig + eps) - mx.log(p_quant + eps))
          kl_per_token = kl.sum(axis=-1)  # Sum over vocab
          
          return (
              float(kl_per_token.mean()),
              float(kl_per_token.max()),
              float(kl_per_token.std()),
          )
      ```

      Integrate into `benchmark_models.py`:
      - Add `kl_mean`, `kl_max`, `kl_std` to BenchmarkResult
      - Compute KL by running both FP16 (or BF16) and quantized on same inputs
      - Report in results table

      Target KL values (from vLLM/llama.cpp experience):
      - KL < 0.01: Excellent (nearly lossless)
      - KL < 0.05: Good (minimal quality impact)
      - KL < 0.10: Acceptable (noticeable but usable)
      - KL > 0.10: Poor (significant degradation)
    priority: P0
    dependencies: []

  # ============================================================================
  # Layer-wise Quantization
  # ============================================================================
  - name: layerwise-quantization
    prompt: |
      Implement layer-wise quantization to fit in unified memory.

      Current approach loads entire model, which fails for large models.
      vLLM and llama.cpp quantize one layer at a time.

      Modify `metal_marlin/hf_loader.py`:

      ```python
      def convert_model_layerwise(
          model_path: Path,
          output_path: Path,
          precision_config: MixedPrecisionConfig,
          calibration_data: Iterator[str],
          device: str = "mps",
      ) -> dict[str, Any]:
          '''
          Quantize model layer-by-layer to minimize memory usage.
          
          For a 30B model on 64GB M3 Max:
          - Full model FP16: ~60GB (won't fit)
          - One layer FP16: ~1GB (fits easily)
          - Quantized output: ~8GB total
          '''
          config = load_config(model_path)
          tokenizer = load_tokenizer(model_path)
          
          # Prepare calibration tokens once
          calib_tokens = prepare_calibration(calibration_data, tokenizer)
          
          stats = {"layers": {}, "total_bytes": 0}
          
          # Process embeddings (keep FP16/BF16)
          embed_weights = load_safetensor_weights(model_path, "model.embed_tokens")
          save_weights(output_path / "embed.safetensors", embed_weights)
          del embed_weights
          gc.collect()
          mx.metal.clear_cache()
          
          # Process each transformer layer
          for layer_idx in range(config.num_hidden_layers):
              print(f"Processing layer {layer_idx}/{config.num_hidden_layers}")
              
              # Load single layer
              layer_weights = load_layer_weights(model_path, layer_idx)
              
              # Quantize based on precision config
              quantized = quantize_layer(
                  layer_weights, 
                  layer_idx, 
                  precision_config,
                  calib_tokens,
              )
              
              # Save and free memory
              save_weights(output_path / f"layer_{layer_idx}.safetensors", quantized)
              stats["layers"][layer_idx] = compute_layer_stats(quantized)
              
              del layer_weights, quantized
              gc.collect()
              mx.metal.clear_cache()
          
          # Process lm_head (keep FP16/BF16)
          lm_head = load_safetensor_weights(model_path, "lm_head")
          save_weights(output_path / "lm_head.safetensors", lm_head)
          
          return stats
      ```

      This approach:
      1. Peak memory = 1 layer FP16 + 1 layer quantized (~2GB for 30B model)
      2. Can quantize 70B+ models on 64GB machines
      3. Matches vLLM's approach
    priority: P0
    dependencies: []

  # ============================================================================
  # vLLM Alignment / Consistency Audit
  # ============================================================================
  - name: vllm-alignment-audit
    prompt: |
      Audit Metal Marlin against vLLM for consistency and best practices.

      Review vLLM's quantization implementation:
      - https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/layers/quantization

      Key areas to check:

      1. **Weight packing format**:
         - vLLM uses specific tensor layouts for efficient dequant
         - Verify our packed format matches or document differences

      2. **Scale handling**:
         - Per-channel vs per-group vs per-tensor
         - Zero-point handling for asymmetric quant
         - Scale dtype (FP16 vs FP32)

      3. **Calibration approach**:
         - vLLM uses activation-aware quantization (GPTQ-style)
         - We should support similar if not already

      4. **MoE handling**:
         - How vLLM handles expert routing precision
         - Shared expert vs routed expert treatment

      5. **KV cache quantization**:
         - vLLM supports FP8 KV cache
         - Should add to `kv_cache.py`

      Create `docs/vllm_comparison.md` documenting:
      - Feature parity table
      - Format compatibility (can we load vLLM quants?)
      - Performance comparison methodology

      File list to audit:
      - `kernels.py`: Dequant kernels vs vLLM CUDA kernels
      - `quantize.py`: Weight packing vs vLLM format
      - `mixed_precision.py`: Layer classification vs vLLM
      - `inference.py`: Batching approach vs vLLM
      - `kv_cache.py`: Paged attention vs vLLM PagedAttention
    priority: P1
    dependencies: []

  - name: llama-cpp-gguf-alignment
    prompt: |
      Ensure compatibility with llama.cpp GGUF quantization formats.

      llama.cpp is the de facto standard for local inference. We should:

      1. **Support loading GGUF quants directly**:
         - Already have `gguf_loader.py` - verify it handles all quant types
         - Q4_K_M, Q5_K_M, Q6_K (k-quants)
         - IQ2_XXS, IQ3_XXS, IQ4_XS (importance quants)

      2. **Match their perplexity measurement**:
         llama.cpp `perplexity` command uses specific methodology:
         - Sliding window with stride
         - BOS token handling
         - Verify our `eval_perplexity.py` matches

      3. **Adopt their calibration dataset if better**:
         - llama.cpp community has extensive benchmarks
         - Bartowski v3 is good, but verify it's comprehensive

      4. **i-quants (importance matrix) support**:
         - IQ2/IQ3/IQ4 use importance matrices for weight selection
         - This is why they achieve better quality at same bpw
         - Consider implementing similar for Metal Marlin

      Update `metal_marlin/gguf_to_marlin.py`:
      - Add IQ2/IQ3 â†’ Metal Marlin conversion
      - Preserve importance matrix if possible
      - Document quality-size tradeoffs

      Reference: https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-quants.c
    priority: P1
    dependencies:
      - sub4bit-quantization

  # ============================================================================
  # Consistency Fixes
  # ============================================================================
  - name: dtype-consistency-audit
    prompt: |
      Audit and fix dtype inconsistencies across the codebase.

      Known issues:
      1. Hardcoded `torch.float16` / `mx.float16` should be configurable
      2. Mixed use of numpy float32 intermediate values
      3. Scale storage dtype varies (FP16 vs FP32)

      Files to audit for dtype consistency:

      1. `quantize.py`:
         - Input dtype handling
         - Scale computation dtype
         - Output dtype specification

      2. `kernels.py`:
         - Metal shader types (half vs float vs bfloat)
         - Buffer dtype declarations

      3. `inference.py`:
         - Activation dtype throughout forward pass
         - Accumulation dtype (should be FP32 for accuracy)

      4. `attention.py`:
         - Q/K/V dtypes
         - Softmax accumulation dtype
         - Output projection dtype

      5. `kv_cache.py`:
         - Cache storage dtype
         - Quantized cache support

      Create `metal_marlin/dtypes.py`:
      ```python
      from dataclasses import dataclass
      from typing import Literal

      @dataclass
      class DTypeConfig:
          '''Centralized dtype configuration.'''
          weights: Literal["fp16", "bf16"] = "bf16"
          activations: Literal["fp16", "bf16"] = "bf16"
          accumulation: Literal["fp32"] = "fp32"  # Always FP32
          scales: Literal["fp16", "fp32"] = "fp16"
          kv_cache: Literal["fp16", "bf16", "fp8"] = "bf16"
      ```

      Update all files to use centralized config instead of hardcoded dtypes.
    priority: P1
    dependencies:
      - bf16-precision-support
