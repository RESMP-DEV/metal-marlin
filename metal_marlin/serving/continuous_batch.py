"""Continuous batching scheduler for vLLM-style inference.

Unlike static batching where all sequences in a batch must complete before new
requests can be processed, continuous batching allows:
- Adding new requests mid-batch
- Returning early finishers immediately
- Variable sequence lengths without padding waste
- Better GPU utilization through dynamic work assignment

Components:
- BatchScheduler: Priority-aware request queue management
- IterationPlanner: Per-iteration work planning with prefill/decode budgets
- KVCacheManager: Per-request KV cache region tracking and lifecycle

Usage:
    from metal_marlin.serving.continuous_batch import (
        BatchScheduler,
        IterationPlanner,
        KVCacheManager,
        SchedulerConfig,
    )

    kv_manager = KVCacheManager(num_blocks=512, block_size=16)
    scheduler = BatchScheduler(config, kv_manager)
    planner = IterationPlanner(config)

    # Add requests with optional priority
    scheduler.add_request(request, priority=RequestPriority.HIGH)

    # Each iteration
    plan = planner.plan_iteration(scheduler.get_runnable_requests())
    # ... execute plan.batch ...
    scheduler.step(plan.completed_request_ids)
"""

from __future__ import annotations

import heapq
from collections import deque
from collections.abc import Iterator
from dataclasses import dataclass, field
from enum import IntEnum, auto
from time import time

from ..paged.allocator import BlockAllocator
from ..paged.page_table import PageTable
from .request import GenerationRequest, RequestStatus, SchedulerOutput


class RequestPriority(IntEnum):
    """Priority levels for request scheduling.

    Higher numeric values = higher priority (processed first).
    """

    LOW = 1
    NORMAL = 2
    HIGH = 3
    REALTIME = 4  # Preempts everything except other REALTIME


class PreemptionPolicy(IntEnum):
    """How to select victims for preemption when memory is exhausted."""

    MOST_TOKENS = auto()  # Preempt sequence with most output tokens (default vLLM)
    LEAST_TOKENS = auto()  # Preempt sequence with fewest output tokens
    OLDEST = auto()  # Preempt longest-running sequence
    LOWEST_PRIORITY = auto()  # Preempt lowest priority first


@dataclass
class SchedulerConfig:
    """Configuration for the continuous batch scheduler."""

    max_num_seqs: int = 64
    max_num_batched_tokens: int = 2048
    max_prefill_tokens: int = 1024
    block_size: int = 16
    preemption_policy: PreemptionPolicy = PreemptionPolicy.LOWEST_PRIORITY
    enable_chunked_prefill: bool = True
    max_chunk_size: int = 512  # Max tokens in a single prefill chunk
    starvation_timeout_s: float = 30.0  # Promote starving low-priority requests


@dataclass
class RequestState:
    """Extended state tracking for a request in the scheduler."""

    request: GenerationRequest
    priority: RequestPriority
    enqueue_time: float
    prefill_progress: int = 0  # Tokens prefilled so far (for chunked prefill)
    kv_region_id: int | None = None  # Opaque ID from KVCacheManager
    preempt_count: int = 0  # Times this request has been preempted

    @property
    def remaining_prefill(self) -> int:
        """Tokens remaining to prefill."""
        return len(self.request.prompt_tokens) - self.prefill_progress

    @property
    def is_prefill_complete(self) -> bool:
        """Whether prefill is finished for this request."""
        return self.prefill_progress >= len(self.request.prompt_tokens)

    @property
    def effective_priority(self) -> tuple[int, float]:
        """Priority for heap ordering: (priority, -enqueue_time).

        Higher priority first, then FIFO within same priority.
        """
        return (self.priority.value, -self.enqueue_time)

    def __lt__(self, other: RequestState) -> bool:
        # For max-heap behavior with heapq (which is min-heap)
        return self.effective_priority > other.effective_priority


@dataclass
class IterationPlan:
    """Plan for a single scheduler iteration.

    Generated by IterationPlanner, consumed by the model runner.
    """

    prefill_work: list[tuple[RequestState, int, int]]  # (state, start_tok, num_toks)
    decode_work: list[RequestState]
    preempted: list[RequestState]

    # Token budgets used
    prefill_tokens_used: int = 0
    decode_tokens_used: int = 0

    @property
    def total_tokens(self) -> int:
        return self.prefill_tokens_used + self.decode_tokens_used

    @property
    def is_empty(self) -> bool:
        return not self.prefill_work and not self.decode_work

    def to_scheduler_output(self) -> SchedulerOutput:
        """Convert to legacy SchedulerOutput for compatibility with BatchedModelRunner."""
        # For chunked prefill, we include the full request but track progress separately
        prefill_requests = [state.request for state, _, _ in self.prefill_work]
        decode_requests = [state.request for state in self.decode_work]
        preempted_requests = [state.request for state in self.preempted]

        return SchedulerOutput(
            prefill_requests=prefill_requests,
            decode_requests=decode_requests,
            preempted_requests=preempted_requests,
        )


@dataclass
class KVRegion:
    """Tracks KV cache allocation for a single request."""

    region_id: int
    request_id: str
    block_indices: list[int] = field(default_factory=list)
    num_tokens: int = 0
    created_at: float = field(default_factory=time)

    @property
    def num_blocks(self) -> int:
        return len(self.block_indices)


class KVCacheManager:
    """Manages KV cache allocation on a per-request basis.

    Provides a higher-level abstraction over BlockAllocator and PageTable:
    - Allocate/free cache regions by request ID
    - Track token counts and block usage per request
    - Support preemption with optional CPU offload (future)
    - Memory pressure monitoring

    Unlike the low-level PageTable which uses `id(request)` for sequence IDs,
    KVCacheManager uses string request_id for stable identification across
    preemption/resume cycles.
    """

    def __init__(
        self,
        num_blocks: int,
        block_size: int = 16,
        *,
        allocator: BlockAllocator | None = None,
    ):
        self.num_blocks = num_blocks
        self.block_size = block_size

        # Use provided allocator or create new one
        self.allocator = allocator or BlockAllocator(num_blocks)
        self.page_table = PageTable(self.allocator, block_size)

        # Request-level tracking
        self._regions: dict[str, KVRegion] = {}  # request_id -> region
        self._next_region_id = 0

    def allocate(self, request_id: str, num_tokens: int) -> int | None:
        """Allocate KV cache region for initial prefill.

        Args:
            request_id: Unique request identifier.
            num_tokens: Number of tokens to allocate space for.

        Returns:
            Region ID on success, None if OOM.
        """
        if request_id in self._regions:
            raise ValueError(f"Region already exists for request {request_id}")

        blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        if self.allocator.num_free < blocks_needed:
            return None

        # Allocate blocks
        region_id = self._next_region_id
        self._next_region_id += 1

        # Use region_id as sequence ID for page table
        if not self.page_table.add_sequence(region_id):
            return None

        # Append tokens to allocate blocks
        if not self.page_table.append_tokens(region_id, num_tokens):
            self.page_table.remove_sequence(region_id)
            return None

        region = KVRegion(
            region_id=region_id,
            request_id=request_id,
            block_indices=self.page_table.get_block_table(region_id).copy(),
            num_tokens=num_tokens,
        )
        self._regions[request_id] = region

        return region_id

    def extend(self, request_id: str, num_tokens: int = 1) -> bool:
        """Extend KV cache region for decode phase.

        Allocates additional blocks if needed.

        Returns:
            True on success, False if OOM.
        """
        region = self._regions.get(request_id)
        if region is None:
            return False

        if not self.page_table.append_tokens(region.region_id, num_tokens):
            return False

        region.num_tokens += num_tokens
        region.block_indices = self.page_table.get_block_table(region.region_id).copy()
        return True

    def free(self, request_id: str) -> None:
        """Free all KV cache for a request."""
        region = self._regions.pop(request_id, None)
        if region:
            self.page_table.remove_sequence(region.region_id)

    def get_block_table(self, request_id: str) -> list[int] | None:
        """Get block indices for a request (for attention kernel dispatch)."""
        region = self._regions.get(request_id)
        return region.block_indices if region else None

    def get_num_tokens(self, request_id: str) -> int:
        """Get number of tokens stored for a request."""
        region = self._regions.get(request_id)
        return region.num_tokens if region else 0

    def get_slot_mapping(self, request_id: str) -> tuple[int, int] | None:
        """Get (block_idx, slot_offset) for next KV write position."""
        region = self._regions.get(request_id)
        if region is None:
            return None
        return self.page_table.get_slot_mapping(region.region_id)

    def preempt(self, request_id: str) -> dict | None:
        """Preempt a request, freeing its KV cache.

        Returns state that can be used to restore the request later.
        Currently does not support CPU offload; returns only metadata.
        """
        region = self._regions.get(request_id)
        if region is None:
            return None

        state = {
            "request_id": request_id,
            "num_tokens": region.num_tokens,
            "created_at": region.created_at,
        }

        # Free the KV cache (no CPU offload yet)
        self.free(request_id)
        return state

    @property
    def num_free_blocks(self) -> int:
        """Number of free blocks available."""
        return self.allocator.num_free

    @property
    def num_allocated_blocks(self) -> int:
        """Number of blocks currently in use."""
        return self.allocator.num_allocated

    @property
    def memory_pressure(self) -> float:
        """Memory pressure as fraction [0, 1]. Higher = more pressure."""
        if self.num_blocks == 0:
            return 1.0
        return self.num_allocated_blocks / self.num_blocks

    def can_allocate(self, num_tokens: int) -> bool:
        """Check if we can allocate space for given tokens."""
        blocks_needed = (num_tokens + self.block_size - 1) // self.block_size
        return self.allocator.num_free >= blocks_needed

    def blocks_for_tokens(self, num_tokens: int) -> int:
        """Calculate blocks needed for a token count."""
        return (num_tokens + self.block_size - 1) // self.block_size

    def __len__(self) -> int:
        """Number of active regions."""
        return len(self._regions)


class IterationPlanner:
    """Plans work distribution for each scheduler iteration.

    Responsible for:
    - Balancing prefill vs decode work within token budget
    - Chunked prefill for long prompts
    - Ensuring decode requests get guaranteed slots
    - Handling preemption when budget is exceeded
    """

    def __init__(self, config: SchedulerConfig):
        self.config = config

    def plan_iteration(
        self,
        waiting: list[RequestState],
        running: list[RequestState],
        kv_manager: KVCacheManager,
    ) -> IterationPlan:
        """Plan work for the next iteration.

        Args:
            waiting: Requests waiting for (chunked) prefill, priority-sorted.
            running: Requests actively decoding, priority-sorted.
            kv_manager: KV cache manager for memory checks.

        Returns:
            IterationPlan with prefill and decode work assignments.
        """
        plan = IterationPlan(
            prefill_work=[],
            decode_work=[],
            preempted=[],
        )

        budget = self.config.max_num_batched_tokens
        prefill_budget = min(budget, self.config.max_prefill_tokens)

        # Phase 1: Allocate decode slots for running sequences.
        # Each decode request consumes exactly 1 token of budget.
        # Priority: keep running requests going to preserve KV investment.
        for state in running:
            if budget < 1:
                # Out of budget, must preempt
                plan.preempted.append(state)
                continue

            # Check if we can extend KV cache by 1 token
            if not kv_manager.can_allocate(1):
                # Out of memory, preempt lowest-priority running request
                plan.preempted.append(state)
                continue

            plan.decode_work.append(state)
            plan.decode_tokens_used += 1
            budget -= 1

        # Phase 2: Schedule prefill work with remaining budget.
        # Use chunked prefill if enabled.
        for state in waiting:
            if budget <= 0 or prefill_budget <= 0:
                break

            remaining = state.remaining_prefill
            if remaining <= 0:
                continue

            # Determine chunk size
            if self.config.enable_chunked_prefill:
                chunk_size = min(
                    remaining,
                    prefill_budget,
                    self.config.max_chunk_size,
                )
            else:
                chunk_size = remaining
                if chunk_size > prefill_budget:
                    # Can't fit; skip to next request
                    continue

            # Check memory for this chunk
            if not state.is_prefill_complete:
                # First chunk needs full prompt allocation
                tokens_to_alloc = len(state.request.prompt_tokens)
            else:
                tokens_to_alloc = chunk_size

            if not kv_manager.can_allocate(tokens_to_alloc):
                # Not enough memory; try preempting or skip
                continue

            # Schedule this chunk
            start_tok = state.prefill_progress
            plan.prefill_work.append((state, start_tok, chunk_size))
            plan.prefill_tokens_used += chunk_size
            budget -= chunk_size
            prefill_budget -= chunk_size

            # Enforce max concurrent sequences
            total_seqs = len(plan.decode_work) + len(plan.prefill_work)
            if total_seqs >= self.config.max_num_seqs:
                break

        return plan


class BatchScheduler:
    """Priority-aware continuous batch scheduler.

    Enhanced version of FCFSScheduler with:
    - Request priorities (LOW, NORMAL, HIGH, REALTIME)
    - Configurable preemption policies
    - Starvation prevention for low-priority requests
    - Integration with KVCacheManager for memory tracking
    - Support for chunked prefill
    """

    def __init__(
        self,
        config: SchedulerConfig,
        kv_manager: KVCacheManager,
    ):
        self.config = config
        self.kv_manager = kv_manager
        self.planner = IterationPlanner(config)

        # Priority queue for waiting requests (max-heap via __lt__)
        self._waiting: list[RequestState] = []

        # Running requests (decode phase)
        self._running: list[RequestState] = []

        # Swapped/preempted requests
        self._swapped: deque[RequestState] = deque()

        # Request ID -> RequestState mapping
        self._requests: dict[str, RequestState] = {}

    def add_request(
        self,
        request: GenerationRequest,
        priority: RequestPriority = RequestPriority.NORMAL,
    ) -> None:
        """Add a new request to the scheduler.

        Args:
            request: The generation request.
            priority: Scheduling priority.
        """
        if request.request_id in self._requests:
            raise ValueError(f"Request {request.request_id} already exists")

        state = RequestState(
            request=request,
            priority=priority,
            enqueue_time=time(),
        )

        request.status = RequestStatus.PENDING
        self._requests[request.request_id] = state
        heapq.heappush(self._waiting, state)

    def abort_request(self, request_id: str) -> bool:
        """Cancel a request by ID, freeing resources.

        Returns True if the request was found and aborted.
        """
        state = self._requests.pop(request_id, None)
        if state is None:
            return False

        # Remove from queues
        if state in self._waiting:
            self._waiting.remove(state)
            heapq.heapify(self._waiting)
        elif state in self._running:
            self._running.remove(state)
        elif state in self._swapped:
            self._swapped.remove(state)

        # Free KV cache
        self.kv_manager.free(request_id)

        return True

    def schedule(self) -> SchedulerOutput:
        """Schedule work for the next iteration.

        Returns a SchedulerOutput compatible with BatchedModelRunner.
        """
        self._check_starvation()

        # Sort running by priority (highest first)
        # Note: __lt__ is defined as self > other for max-heap, so sort() alone works
        self._running.sort()

        # Get sorted waiting list (already a heap but need to preserve order)
        # Note: __lt__ gives max-heap ordering, so sorted() puts highest priority first
        waiting_sorted = sorted(self._waiting)

        # Plan iteration
        plan = self.planner.plan_iteration(
            waiting=waiting_sorted,
            running=self._running,
            kv_manager=self.kv_manager,
        )

        # Process preemptions
        for state in plan.preempted:
            self._preempt(state)

        # Allocate KV for new prefills and update progress
        for state, start_tok, num_toks in plan.prefill_work:
            if state.kv_region_id is None:
                # First chunk - allocate full prompt space
                region_id = self.kv_manager.allocate(
                    state.request.request_id,
                    len(state.request.prompt_tokens),
                )
                if region_id is None:
                    # OOM - shouldn't happen if planner checked, but be safe
                    continue
                state.kv_region_id = region_id

            # Update prefill progress
            state.prefill_progress = start_tok + num_toks

            # If prefill complete, move to running
            if state.is_prefill_complete:
                if state in self._waiting:
                    self._waiting.remove(state)
                    heapq.heapify(self._waiting)
                state.request.status = RequestStatus.RUNNING
                self._running.append(state)

            # Update block indices on request
            block_table = self.kv_manager.get_block_table(state.request.request_id)
            if block_table:
                state.request.block_indices = block_table

        # Update block indices for decode requests
        for state in plan.decode_work:
            block_table = self.kv_manager.get_block_table(state.request.request_id)
            if block_table:
                state.request.block_indices = block_table

        return plan.to_scheduler_output()

    def step(self, finished_ids: list[str] | None = None) -> list[GenerationRequest]:
        """Advance scheduler state after an iteration.

        Call this after model execution to:
        - Free KV cache for finished requests
        - Extend KV cache for continuing decode requests

        Args:
            finished_ids: Request IDs that finished this iteration.

        Returns:
            List of finished requests.
        """
        finished_ids = finished_ids or []
        finished: list[GenerationRequest] = []

        for state in list(self._running):
            if state.request.request_id in finished_ids or state.request.is_finished:
                # Request finished
                self._running.remove(state)
                self._requests.pop(state.request.request_id, None)
                self.kv_manager.free(state.request.request_id)
                state.request.status = RequestStatus.FINISHED
                finished.append(state.request)
            else:
                # Extend KV cache for next token
                success = self.kv_manager.extend(state.request.request_id, 1)
                if not success:
                    # OOM during decode - preempt
                    self._preempt(state)

        return finished

    def free_finished(self) -> list[GenerationRequest]:
        """Free finished sequences. Alias for step() with no finished IDs."""
        return self.step([])

    def step_decode(self) -> None:
        """Legacy compatibility: advance decode by 1 token.

        Prefer using step() which handles both finished detection and KV extension.
        """
        for state in list(self._running):
            if not self.kv_manager.extend(state.request.request_id, 1):
                self._preempt(state)

    def _preempt(self, state: RequestState) -> None:
        """Preempt a request, freeing its KV cache."""
        if state in self._running:
            self._running.remove(state)
        if state in self._waiting:
            self._waiting.remove(state)
            heapq.heapify(self._waiting)

        self.kv_manager.preempt(state.request.request_id)
        state.request.status = RequestStatus.PREEMPTED
        state.preempt_count += 1
        state.kv_region_id = None
        state.prefill_progress = 0  # Must re-prefill from scratch
        self._swapped.append(state)

    def _select_preempt_victim(self) -> RequestState | None:
        """Select a running request to preempt based on policy."""
        if not self._running:
            return None

        policy = self.config.preemption_policy

        if policy == PreemptionPolicy.MOST_TOKENS:
            return max(self._running, key=lambda s: len(s.request.output_tokens))
        elif policy == PreemptionPolicy.LEAST_TOKENS:
            return min(self._running, key=lambda s: len(s.request.output_tokens))
        elif policy == PreemptionPolicy.OLDEST:
            return min(self._running, key=lambda s: s.enqueue_time)
        elif policy == PreemptionPolicy.LOWEST_PRIORITY:
            return min(self._running, key=lambda s: s.effective_priority)
        else:
            return self._running[-1]  # Default: last added

    def _check_starvation(self) -> None:
        """Promote starving low-priority requests to prevent indefinite waiting."""
        now = time()
        timeout = self.config.starvation_timeout_s

        for state in self._waiting:
            age = now - state.enqueue_time
            if age > timeout and state.priority < RequestPriority.HIGH:
                state.priority = RequestPriority(min(state.priority + 1, RequestPriority.HIGH))

        # Re-heapify after priority changes
        heapq.heapify(self._waiting)

        # Also check swapped queue
        for state in self._swapped:
            age = now - state.enqueue_time
            if age > timeout * 2:  # Double timeout for swapped
                state.priority = RequestPriority.HIGH

    def resume_swapped(self, max_to_resume: int = 1) -> int:
        """Try to resume preempted requests.

        Called when memory becomes available. Swapped requests need full re-prefill.

        Returns:
            Number of requests resumed.
        """
        resumed = 0
        while self._swapped and resumed < max_to_resume:
            state = self._swapped[0]
            tokens_needed = len(state.request.prompt_tokens)

            if not self.kv_manager.can_allocate(tokens_needed):
                break

            self._swapped.popleft()
            state.request.status = RequestStatus.PENDING
            heapq.heappush(self._waiting, state)
            resumed += 1

        return resumed

    @property
    def num_waiting(self) -> int:
        return len(self._waiting)

    @property
    def num_running(self) -> int:
        return len(self._running)

    @property
    def num_swapped(self) -> int:
        return len(self._swapped)

    @property
    def has_pending_work(self) -> bool:
        return bool(self._waiting or self._running or self._swapped)

    def get_request(self, request_id: str) -> GenerationRequest | None:
        """Get request by ID."""
        state = self._requests.get(request_id)
        return state.request if state else None

    def get_stats(self) -> dict:
        """Get scheduler statistics."""
        return {
            "waiting": self.num_waiting,
            "running": self.num_running,
            "swapped": self.num_swapped,
            "kv_memory_pressure": self.kv_manager.memory_pressure,
            "kv_blocks_used": self.kv_manager.num_allocated_blocks,
            "kv_blocks_free": self.kv_manager.num_free_blocks,
        }

    def iter_requests(self) -> Iterator[tuple[str, RequestState]]:
        """Iterate over all tracked requests."""
        yield from self._requests.items()
