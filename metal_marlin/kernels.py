"""
Metal kernel wrappers for Marlin GEMM and dequantization.

Provides Metal kernel interfaces to the fused dequant-GEMM kernels
(FP4 E2M1 and INT4 U4) and standalone dequant kernels.

This module uses PyObjC for direct Metal shader dispatch without MLX dependency.
All public APIs work with PyTorch MPS tensors.

Usage:
    from kernels import marlin_gemm_fp4, marlin_gemm_int4, dequant_fp4

    # FP4 fused dequant-GEMM (PyTorch MPS tensors)
    C = marlin_gemm_fp4(A, B_packed, scales, group_size=32)

    # INT4 fused dequant-GEMM with zero points
    C = marlin_gemm_int4(A, B_packed, scales, zeros, group_size=32)

    # Standalone FP4 dequantization
    W_fp16 = dequant_fp4(B_packed, scales, K, N, group_size=32)

Note:
    Metal kernel dispatch requires PyObjC and PyTorch MPS. When these are not
    available, all kernel functions will raise ImportError with installation
    instructions.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Any, Protocol

import numpy as np

from .metal_dispatch import (
    HAS_METAL,
    HAS_MPS,
    HAS_TORCH,
    MetalKernelLibrary,
    dispatch_kernel,
    get_default_library,
    get_shader_source,
    mps_tensor_to_metal_buffer,
    require_mps,
)
from .utils.padding import pad_torch_2d, round_up

if HAS_TORCH:
    import torch

if TYPE_CHECKING:
    import torch


class _MpsTensorToMetalBuffer(Protocol):
    def __call__(self, tensor: torch.Tensor, device: Any, *, copy_back: bool = False) -> Any: ...


mps_tensor_to_metal_buffer: _MpsTensorToMetalBuffer


def _metal_required(*args: Any, **kwargs: Any) -> Any:
    """Stub function that raises ImportError when Metal/MPS is unavailable."""
    raise ImportError(
        "Metal kernel dispatch requires PyObjC and PyTorch MPS. "
        "Install with: pip install pyobjc-framework-Metal torch"
    )


# When Metal/MPS is unavailable, export stubs for all public kernel functions
if not HAS_METAL or not HAS_MPS:
    # GEMM kernels
    marlin_gemm_fp4 = _metal_required
    marlin_gemm_fused_fp4 = _metal_required
    marlin_gemm_fp4_tuned = _metal_required
    marlin_gemm_int4 = _metal_required
    marlin_gemm_fused_u4 = _metal_required

    # Dequantization kernels
    dequant_fp4 = _metal_required
    dequant_u4_standalone = _metal_required
    dequant_int2 = _metal_required
    dequant_int3 = _metal_required

    # Weight packing functions
    pack_fp4_weights = _metal_required
    pack_int2_weights = _metal_required
    pack_int3_weights = _metal_required

    # Flash attention
    flash_attention_kv_fp4 = _metal_required

    # Paged attention
    paged_attention_fp4 = _metal_required

    # MoE kernels
    moe_expert_gemm_fp4 = _metal_required
    moe_router_topk = _metal_required
    moe_shared_expert_fused = _metal_required
    moe_shared_expert_fused_fp4 = _metal_required
    moe_shared_expert_scatter = _metal_required
    moe_shared_expert_fp4 = _metal_required
    moe_fused_dispatch_shared_fp4 = _metal_required
    moe_add_shared_expert_fp4 = _metal_required

    # Hadamard transform
    hadamard_transform = _metal_required

# Constants and Metal shader strings don't require Metal - defined unconditionally
# ---------------------------------------------------------------------------
# Tile / threadgroup constants (must match the kernel logic)
# ---------------------------------------------------------------------------

TILE_M = 64
TILE_N = 64
TILE_K = 32
SIMDGROUPS_PER_TG = 4
THREADS_PER_TG = SIMDGROUPS_PER_TG * 32  # 128
SG_M_TILES = 8
SG_N_TILES = 2
K_TILES = TILE_K // 8  # 4
FP4_PER_UINT = 8
NUM_BUFFERS = 2
FP32_ACCUM_K_THRESHOLD = 256

# ---------------------------------------------------------------------------
# Kernel source: all helper functions, dequant primitives, tile logic
# ---------------------------------------------------------------------------

_GEMM_HEADER = """
#include <metal_stdlib>
#include <metal_simdgroup_matrix>
using namespace metal;

// --- FP4 E2M1 branchless dequant (pure ALU, no LUT) ---
inline half dequant_fp4_scalar(uint nibble) {
    uint sign_bit = (nibble >> 3) & 1;
    uint exp_bits = (nibble >> 1) & 0x3;
    uint man_bit  = nibble & 1;

    half sub_mag = half(man_bit) * half(0.5h);
    half norm_mag = half(1u << (exp_bits - 1)) * (half(1.0h) + half(man_bit) * half(0.5h));
    half magnitude = select(norm_mag, sub_mag, exp_bits == 0);
    return select(magnitude, -magnitude, bool(sign_bit));
}

// NOTE: Uses float scale to work around Metal compiler bug where
// half parameters in inline functions have fractional parts rounded.
inline void dequant_fp4x8(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    out[0] = (half)((float)dequant_fp4_scalar((packed >>  0) & 0xF) * fscale);
    out[1] = (half)((float)dequant_fp4_scalar((packed >>  4) & 0xF) * fscale);
    out[2] = (half)((float)dequant_fp4_scalar((packed >>  8) & 0xF) * fscale);
    out[3] = (half)((float)dequant_fp4_scalar((packed >> 12) & 0xF) * fscale);
    out[4] = (half)((float)dequant_fp4_scalar((packed >> 16) & 0xF) * fscale);
    out[5] = (half)((float)dequant_fp4_scalar((packed >> 20) & 0xF) * fscale);
    out[6] = (half)((float)dequant_fp4_scalar((packed >> 24) & 0xF) * fscale);
    out[7] = (half)((float)dequant_fp4_scalar((packed >> 28) & 0xF) * fscale);
}

// --- INT4 U4 magic-bias dequant ---
//
// Magic-bias trick for branchless INT4 -> FP16 conversion:
//   1. OR 4-bit nibble into FP16 mantissa of 1024.0 (0x6400)
//   2. Subtract 1024.0 to recover nibble as FP16 float
//   3. Apply asymmetric dequant: output = (nibble - zero_point) * scale
//
// FUSED_MAGIC_BIAS = 0x64006400 = two FP16 1024.0 values packed in uint32
// FUSED_LO_MASK = 0x000F000F = extract bits [3:0] from each 16-bit lane
//
// Nibble packing convention (LSB-first):
//   nibble i = (packed >> (i * 4)) & 0xF
//   This matches pack_u4_weights() in metal_marlin.py
//
// NOTE: Uses float intermediates to work around Metal compiler bug where
// half parameters in inline functions have fractional parts rounded.
// See docs/metal_half_precision_bug.md for details.
constant constexpr uint32_t FUSED_MAGIC_BIAS = 0x64006400u;
constant constexpr uint32_t FUSED_LO_MASK    = 0x000F000Fu;

inline void dequant_u4x8(uint32_t packed, float scale, float zero_point, thread half *out) {
    half2 bias = as_type<half2>(FUSED_MAGIC_BIAS);

    uint32_t n0 = (packed & FUSED_LO_MASK) | FUSED_MAGIC_BIAS;
    half2 v0 = as_type<half2>(n0) - bias;

    uint32_t n1 = ((packed >> 4u) & FUSED_LO_MASK) | FUSED_MAGIC_BIAS;
    half2 v1 = as_type<half2>(n1) - bias;

    uint32_t n2 = ((packed >> 8u) & FUSED_LO_MASK) | FUSED_MAGIC_BIAS;
    half2 v2 = as_type<half2>(n2) - bias;

    uint32_t n3 = ((packed >> 12u) & FUSED_LO_MASK) | FUSED_MAGIC_BIAS;
    half2 v3 = as_type<half2>(n3) - bias;

    out[0] = (half)(((float)v0.x - zero_point) * scale);
    out[1] = (half)(((float)v0.y - zero_point) * scale);
    out[2] = (half)(((float)v1.x - zero_point) * scale);
    out[3] = (half)(((float)v1.y - zero_point) * scale);
    out[4] = (half)(((float)v2.x - zero_point) * scale);
    out[5] = (half)(((float)v2.y - zero_point) * scale);
    out[6] = (half)(((float)v3.x - zero_point) * scale);
    out[7] = (half)(((float)v3.y - zero_point) * scale);
}

// --- INT2 (2-bit) dequantization: 16 weights per uint32 ---
// Codes 0,1,2,3 map to {-1.0, -0.333, 0.333, 1.0} before scaling.
// Formula: dequant = (code - 1.5) * 0.667 * scale

inline void dequant_int2x16(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    for (uint i = 0; i < 16; i++) {
        uint code = (packed >> (i * 2u)) & 0x3u;
        float dequant = (float(code) - 1.5f) * 0.6667f;
        out[i] = (half)(dequant * fscale);
    }
}

// Unrolled variant for better performance (no loop)
inline void dequant_int2x16_unrolled(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    #define DEQUANT_INT2(idx) { \\
        uint code = (packed >> ((idx) * 2u)) & 0x3u; \\
        out[idx] = (half)((float(code) - 1.5f) * 0.6667f * fscale); \\
    }
    DEQUANT_INT2(0);  DEQUANT_INT2(1);  DEQUANT_INT2(2);  DEQUANT_INT2(3);
    DEQUANT_INT2(4);  DEQUANT_INT2(5);  DEQUANT_INT2(6);  DEQUANT_INT2(7);
    DEQUANT_INT2(8);  DEQUANT_INT2(9);  DEQUANT_INT2(10); DEQUANT_INT2(11);
    DEQUANT_INT2(12); DEQUANT_INT2(13); DEQUANT_INT2(14); DEQUANT_INT2(15);
    #undef DEQUANT_INT2
}

// Dequant first 8 of 16 INT2 values (for GEMM tile alignment)
inline void dequant_int2x8(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    for (uint i = 0; i < 8; i++) {
        uint code = (packed >> (i * 2u)) & 0x3u;
        float dequant = (float(code) - 1.5f) * 0.6667f;
        out[i] = (half)(dequant * fscale);
    }
}

// --- INT3 (3-bit) dequantization: 10 weights per uint32 (30 bits used) ---
// Codes 0-7 map to symmetric [-1.0, 1.0] range.
// Formula: dequant = (code - 3.5) / 3.5 * scale â‰ˆ (code - 3.5) * 0.2857 * scale

inline void dequant_int3x10(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    for (uint i = 0; i < 10; i++) {
        uint code = (packed >> (i * 3u)) & 0x7u;
        float dequant = (float(code) - 3.5f) * 0.2857f;
        out[i] = (half)(dequant * fscale);
    }
}

// Unrolled variant for better performance
inline void dequant_int3x10_unrolled(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    #define DEQUANT_INT3(idx) { \\
        uint code = (packed >> ((idx) * 3u)) & 0x7u; \\
        out[idx] = (half)((float(code) - 3.5f) * 0.2857f * fscale); \\
    }
    DEQUANT_INT3(0); DEQUANT_INT3(1); DEQUANT_INT3(2); DEQUANT_INT3(3); DEQUANT_INT3(4);
    DEQUANT_INT3(5); DEQUANT_INT3(6); DEQUANT_INT3(7); DEQUANT_INT3(8); DEQUANT_INT3(9);
    #undef DEQUANT_INT3
}

// Dequant first 8 of 10 INT3 values (for GEMM tile alignment)
inline void dequant_int3x8(uint32_t packed, half scale, thread half *out) {
    float fscale = (float)scale;
    for (uint i = 0; i < 8; i++) {
        uint code = (packed >> (i * 3u)) & 0x7u;
        float dequant = (float(code) - 3.5f) * 0.2857f;
        out[i] = (half)(dequant * fscale);
    }
}

// GemmParams struct for kernel parameters
struct GemmParams {
    uint M;
    uint N;
    uint K;
    uint group_size;
};
"""

# ---------------------------------------------------------------------------
# FP4 fused dequant-GEMM kernel (complete kernel, not body)
# ---------------------------------------------------------------------------

_FP4_GEMM_KERNEL = (
    _GEMM_HEADER
    + """
kernel void marlin_gemm_fp4(
    device const half* A [[buffer(0)]],
    device const uint* B_packed [[buffer(1)]],
    device const half* scales [[buffer(2)]],
    device half* out [[buffer(3)]],
    device const GemmParams* params [[buffer(4)]],
    uint3 threadgroup_position_in_grid [[threadgroup_position_in_grid]],
    uint thread_index_in_simdgroup [[thread_index_in_simdgroup]],
    uint simdgroup_index_in_threadgroup [[simdgroup_index_in_threadgroup]]
) {
    const uint M = params->M;
    const uint N = params->N;
    const uint K = params->K;
    const uint GROUP_SIZE = params->group_size;

    const uint TILE_M = 64;
    const uint TILE_N = 64;
    const uint TILE_K = 32;
    const uint SIMDGROUPS_PER_TG = 4;
    const uint SG_M_TILES = 8;
    const uint SG_N_TILES = 2;
    const uint FP4_PER_UINT = 8;
    const uint NUM_BUFFERS = 2;

    // Thread identity
    uint3 tgid = threadgroup_position_in_grid;
    uint simd_lane = thread_index_in_simdgroup;
    uint simd_id = simdgroup_index_in_threadgroup;
    uint thread_idx = simd_id * 32 + simd_lane;

    // Double-buffered threadgroup memory
    threadgroup half A_tiles[NUM_BUFFERS][TILE_M][TILE_K];
    threadgroup half B_staging[SIMDGROUPS_PER_TG][8][8];

    // Tile assignment
    uint tg_row = tgid.y * TILE_M;
    uint tg_col = tgid.x * TILE_N;
    uint sg_row_offset = 0;  // All simdgroups cover all rows
    uint sg_col_offset = simd_id * (SG_N_TILES * 8);

    // Accumulators
    simdgroup_matrix<half, 8, 8> acc[SG_M_TILES][SG_N_TILES];
    for (uint mi = 0; mi < SG_M_TILES; ++mi)
        for (uint ni = 0; ni < SG_N_TILES; ++ni)
            acc[mi][ni] = make_filled_simdgroup_matrix<half, 8, 8>(0.0h);

    uint k_packs = K / FP4_PER_UINT;
    uint k_tiles_count = TILE_K / 8;

    // Main K-reduction loop
    for (uint k_block = 0; k_block < K; k_block += TILE_K) {

        // Cooperative A tile load
        {
            const uint elems_per_thread = (TILE_M * TILE_K) / (SIMDGROUPS_PER_TG * 32);
            for (uint i = 0; i < elems_per_thread; ++i) {
                uint flat_idx = thread_idx * elems_per_thread + i;
                uint row = flat_idx / TILE_K;
                uint col = flat_idx % TILE_K;
                uint global_row = tg_row + row;
                uint global_col = k_block + col;

                half val = (global_row < M && global_col < K)
                           ? A[global_row * K + global_col]
                           : half(0.0h);
                A_tiles[0][row][col] = val;
            }
        }

        threadgroup_barrier(mem_flags::mem_threadgroup);

        // Inner loop: fused dequant + simdgroup MMA
        for (uint kt = 0; kt < k_tiles_count; ++kt) {
            uint k_sub_base = k_block + kt * 8;
            uint k_pack_idx = k_sub_base / FP4_PER_UINT;
            uint group_idx = k_sub_base / GROUP_SIZE;

            for (uint mi = 0; mi < SG_M_TILES; ++mi) {
                simdgroup_matrix<half, 8, 8> a_frag;
                simdgroup_load(a_frag,
                               &A_tiles[0][sg_row_offset + mi * 8][kt * 8],
                               TILE_K);

                for (uint ni = 0; ni < SG_N_TILES; ++ni) {
                    uint b_col_base = tg_col + sg_col_offset + ni * 8;

                    // Fused B dequant: lanes 0-7 each handle one column
                    if (simd_lane < 8) {
                        uint b_col = b_col_base + simd_lane;
                        half dequant_vals[8];

                        if (b_col < N && k_pack_idx < k_packs) {
                            uint32_t packed = B_packed[k_pack_idx * N + b_col];
                            half scale = scales[group_idx * N + b_col];
                            dequant_fp4x8(packed, scale, dequant_vals);
                        } else {
                            for (uint v = 0; v < 8; ++v)
                                dequant_vals[v] = half(0.0h);
                        }

                        for (uint row = 0; row < 8; ++row)
                            B_staging[simd_id][row][simd_lane] = dequant_vals[row];
                    }

                    simdgroup_barrier(mem_flags::mem_threadgroup);

                    simdgroup_matrix<half, 8, 8> b_frag;
                    simdgroup_load(b_frag, &B_staging[simd_id][0][0], 8);

                    simdgroup_multiply_accumulate(acc[mi][ni],
                                                  a_frag, b_frag, acc[mi][ni]);
                }
            }
        }

        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    // Store results
    for (uint mi = 0; mi < SG_M_TILES; ++mi) {
        for (uint ni = 0; ni < SG_N_TILES; ++ni) {
            uint out_row = tg_row + sg_row_offset + mi * 8;
            uint out_col = tg_col + sg_col_offset + ni * 8;

            if (out_row + 8 <= M && out_col + 8 <= N) {
                simdgroup_store(acc[mi][ni], out + out_row * N + out_col, N);
            } else if (out_row < M && out_col < N) {
                threadgroup half out_staging[8][8];
                simdgroup_store(acc[mi][ni], &out_staging[0][0], 8);
                threadgroup_barrier(mem_flags::mem_threadgroup);
                for (uint elem = simd_lane; elem < 64; elem += 32) {
                    uint r = elem / 8;
                    uint c = elem % 8;
                    if (out_row + r < M && out_col + c < N) {
                        out[(out_row + r) * N + out_col + c] = out_staging[r][c];
                    }
                }
                threadgroup_barrier(mem_flags::mem_threadgroup);
            }
        }
    }
}
"""
)

# ---------------------------------------------------------------------------
# INT4 fused dequant-GEMM kernel (with zero points)
# ---------------------------------------------------------------------------

_INT4_GEMM_KERNEL = (
    _GEMM_HEADER
    + """
struct GemmParamsInt4 {
    uint M;
    uint N;
    uint K;
    uint group_size;
};

kernel void marlin_gemm_int4(
    device const half* A [[buffer(0)]],
    device const uint* B_packed [[buffer(1)]],
    device const float* scales [[buffer(2)]],
    device const float* zeros [[buffer(3)]],
    device half* out [[buffer(4)]],
    device const GemmParamsInt4* params [[buffer(5)]],
    uint3 threadgroup_position_in_grid [[threadgroup_position_in_grid]],
    uint thread_index_in_simdgroup [[thread_index_in_simdgroup]],
    uint simdgroup_index_in_threadgroup [[simdgroup_index_in_threadgroup]]
) {
    const uint M = params->M;
    const uint N = params->N;
    const uint K = params->K;
    const uint GROUP_SIZE = params->group_size;

    const uint TILE_M = 64;
    const uint TILE_N = 64;
    const uint TILE_K = 32;
    const uint SIMDGROUPS_PER_TG = 4;
    const uint SG_M_TILES = 8;
    const uint SG_N_TILES = 2;
    const uint FP4_PER_UINT = 8;

    uint3 tgid = threadgroup_position_in_grid;
    uint simd_lane = thread_index_in_simdgroup;
    uint simd_id = simdgroup_index_in_threadgroup;
    uint thread_idx = simd_id * 32 + simd_lane;

    threadgroup half A_tiles[TILE_M][TILE_K];
    threadgroup half B_staging[SIMDGROUPS_PER_TG][8][8];

    uint tg_row = tgid.y * TILE_M;
    uint tg_col = tgid.x * TILE_N;
    uint sg_row_offset = 0;  // All simdgroups cover all rows
    uint sg_col_offset = simd_id * (SG_N_TILES * 8);

    simdgroup_matrix<half, 8, 8> acc[SG_M_TILES][SG_N_TILES];
    for (uint mi = 0; mi < SG_M_TILES; ++mi)
        for (uint ni = 0; ni < SG_N_TILES; ++ni)
            acc[mi][ni] = make_filled_simdgroup_matrix<half, 8, 8>(0.0h);

    uint k_packs = K / FP4_PER_UINT;
    uint k_tiles_count = TILE_K / 8;

    for (uint k_block = 0; k_block < K; k_block += TILE_K) {
        {
            const uint elems_per_thread = (TILE_M * TILE_K) / (SIMDGROUPS_PER_TG * 32);
            for (uint i = 0; i < elems_per_thread; ++i) {
                uint flat_idx = thread_idx * elems_per_thread + i;
                uint row = flat_idx / TILE_K;
                uint col = flat_idx % TILE_K;
                uint global_row = tg_row + row;
                uint global_col = k_block + col;

                half val = (global_row < M && global_col < K)
                           ? A[global_row * K + global_col]
                           : half(0.0h);
                A_tiles[row][col] = val;
            }
        }

        threadgroup_barrier(mem_flags::mem_threadgroup);

        for (uint kt = 0; kt < k_tiles_count; ++kt) {
            uint k_sub_base = k_block + kt * 8;
            uint k_pack_idx = k_sub_base / FP4_PER_UINT;
            uint group_idx = k_sub_base / GROUP_SIZE;

            for (uint mi = 0; mi < SG_M_TILES; ++mi) {
                simdgroup_matrix<half, 8, 8> a_frag;
                simdgroup_load(a_frag,
                               &A_tiles[sg_row_offset + mi * 8][kt * 8],
                               TILE_K);

                for (uint ni = 0; ni < SG_N_TILES; ++ni) {
                    uint b_col_base = tg_col + sg_col_offset + ni * 8;

                    if (simd_lane < 8) {
                        uint b_col = b_col_base + simd_lane;
                        half dequant_vals[8];

                        if (b_col < N && k_pack_idx < k_packs) {
                            uint32_t packed = B_packed[k_pack_idx * N + b_col];
                            float scale = scales[group_idx * N + b_col];
                            float zero = zeros[group_idx * N + b_col];
                            dequant_u4x8(packed, scale, zero, dequant_vals);
                        } else {
                            for (uint v = 0; v < 8; ++v)
                                dequant_vals[v] = half(0.0h);
                        }

                        for (uint row = 0; row < 8; ++row)
                            B_staging[simd_id][row][simd_lane] = dequant_vals[row];
                    }

                    simdgroup_barrier(mem_flags::mem_threadgroup);

                    simdgroup_matrix<half, 8, 8> b_frag;
                    simdgroup_load(b_frag, &B_staging[simd_id][0][0], 8);

                    simdgroup_multiply_accumulate(acc[mi][ni],
                                                  a_frag, b_frag, acc[mi][ni]);
                }
            }
        }

        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    // Store results
    for (uint mi = 0; mi < SG_M_TILES; ++mi) {
        for (uint ni = 0; ni < SG_N_TILES; ++ni) {
            uint out_row = tg_row + sg_row_offset + mi * 8;
            uint out_col = tg_col + sg_col_offset + ni * 8;

            if (out_row + 8 <= M && out_col + 8 <= N) {
                simdgroup_store(acc[mi][ni], out + out_row * N + out_col, N);
            } else if (out_row < M && out_col < N) {
                threadgroup half out_staging[8][8];
                simdgroup_store(acc[mi][ni], &out_staging[0][0], 8);
                threadgroup_barrier(mem_flags::mem_threadgroup);
                for (uint elem = simd_lane; elem < 64; elem += 32) {
                    uint r = elem / 8;
                    uint c = elem % 8;
                    if (out_row + r < M && out_col + c < N) {
                        out[(out_row + r) * N + out_col + c] = out_staging[r][c];
                    }
                }
                threadgroup_barrier(mem_flags::mem_threadgroup);
            }
        }
    }
}
"""
)

# ---------------------------------------------------------------------------
# Standalone dequant kernel (FP4 -> FP16)
# ---------------------------------------------------------------------------

_DEQUANT_FP4_KERNEL = (
    _GEMM_HEADER
    + """
struct DequantParams {
    uint K;
    uint N;
    uint group_size;
};

kernel void dequant_fp4(
    device const uint* B_packed [[buffer(0)]],
    device const half* scales [[buffer(1)]],
    device half* out [[buffer(2)]],
    device const DequantParams* params [[buffer(3)]],
    uint3 thread_position_in_grid [[thread_position_in_grid]]
) {
    const uint K = params->K;
    const uint N = params->N;
    const uint GROUP_SIZE = params->group_size;
    const uint FP4_PER_UINT = 8;

    // Grid: (N, K/8, 1) - each thread handles one packed uint32 (8 FP4 values)
    uint n_idx = thread_position_in_grid.x;
    uint k_block = thread_position_in_grid.y;

    if (n_idx >= N || k_block * 8u >= K) return;

    uint packed_idx = k_block * N + n_idx;
    uint32_t packed = B_packed[packed_idx];

    uint k_start = k_block * 8u;
    uint group_idx = k_start / GROUP_SIZE;
    half scale = scales[group_idx * N + n_idx];

    half vals[8];
    dequant_fp4x8(packed, scale, vals);

    uint out_base = k_start * N + n_idx;
    uint k_remain = min(8u, K - k_start);
    for (uint i = 0; i < k_remain; i++) {
        out[out_base + i * N] = vals[i];
    }
}
"""
)

# ---------------------------------------------------------------------------
# Decode GEMV kernel (optimized for M=1)
# ---------------------------------------------------------------------------

_DECODE_GEMV_FP4_KERNEL = (
    _GEMM_HEADER
    + """
struct DecodeParams {
    uint K;
    uint N;
    uint group_size;
};

kernel void decode_gemv_fp4(
    device const half* A [[buffer(0)]],
    device const uint* B [[buffer(1)]],
    device const half* scales [[buffer(2)]],
    device half* out [[buffer(3)]],
    device const DecodeParams* params [[buffer(4)]],
    uint threadgroup_position_in_grid [[threadgroup_position_in_grid]],
    uint thread_position_in_threadgroup [[thread_position_in_threadgroup]]
) {
    const uint K = params->K;
    const uint N = params->N;
    const uint GROUP_SIZE = params->group_size;

    const uint TILE_N = 256;
    const uint COLS_PER_THREAD = 2;
    const uint FP4_PER_UINT = 8;

    uint tgid = threadgroup_position_in_grid;
    uint tid = thread_position_in_threadgroup;

    uint col_base = tgid * TILE_N + tid * COLS_PER_THREAD;
    uint col0 = col_base;
    uint col1 = col_base + 1;

    float acc0 = 0.0f;
    float acc1 = 0.0f;

    uint k_packs = K / FP4_PER_UINT;

    // Stream through K dimension
    for (uint k_base = 0; k_base < K; k_base += FP4_PER_UINT) {
        uint pack_idx = k_base / FP4_PER_UINT;
        uint group_idx = k_base / GROUP_SIZE;

        // Load 8 A values (shared across all columns)
        half a_vals[8];
        for (uint i = 0; i < 8; i++) {
            uint k_idx = k_base + i;
            a_vals[i] = (k_idx < K) ? A[k_idx] : half(0.0h);
        }

        // Process column 0
        if (col0 < N && pack_idx < k_packs) {
            uint packed0 = B[pack_idx * N + col0];
            half scale0 = scales[group_idx * N + col0];
            float fscale0 = (float)scale0;

            for (uint i = 0; i < 8; i++) {
                if ((k_base + i) < K) {
                    uint nibble = (packed0 >> (i * 4)) & 0xF;
                    half w_val = (half)((float)dequant_fp4_scalar(nibble) * fscale0);
                    acc0 += float(a_vals[i]) * float(w_val);
                }
            }
        }

        // Process column 1
        if (col1 < N && pack_idx < k_packs) {
            uint packed1 = B[pack_idx * N + col1];
            half scale1 = scales[group_idx * N + col1];
            float fscale1 = (float)scale1;

            for (uint i = 0; i < 8; i++) {
                if ((k_base + i) < K) {
                    uint nibble = (packed1 >> (i * 4)) & 0xF;
                    half w_val = (half)((float)dequant_fp4_scalar(nibble) * fscale1);
                    acc1 += float(a_vals[i]) * float(w_val);
                }
            }
        }
    }

    // Store results
    if (col0 < N) {
        out[col0] = half(acc0);
    }
    if (col1 < N) {
        out[col1] = half(acc1);
    }
}
"""
)

# ---------------------------------------------------------------------------
# Hadamard Transform kernels
# ---------------------------------------------------------------------------

_HADAMARD_KERNEL = """
#include <metal_stdlib>
using namespace metal;

/// Single butterfly step: computes (a + b, a - b) where b is shuffled from another lane.
inline float butterfly_step_f(float val, uint partner, bool is_upper) {
    float partner_val = simd_shuffle(val, ushort(partner));
    return is_upper ? (partner_val - val) : (val + partner_val);
}

/// Butterfly step for float2 (two parallel values per thread)
inline float2 butterfly_step2_f(float2 val, uint partner, bool is_upper) {
    float2 partner_val = simd_shuffle(val, ushort(partner));
    return is_upper ? (partner_val - val) : (val + partner_val);
}

/// Butterfly step for float4 (four parallel values per thread)
inline float4 butterfly_step4_f(float4 val, uint partner, bool is_upper) {
    float4 partner_val = simd_shuffle(val, ushort(partner));
    return is_upper ? (partner_val - val) : (val + partner_val);
}

struct HadamardParams {
    uint N;
    uint normalize;
};

kernel void hadamard_32(
    device const half* input [[buffer(0)]],
    device half* out [[buffer(1)]],
    device const HadamardParams* params [[buffer(2)]],
    uint thread_index_in_simdgroup [[thread_index_in_simdgroup]],
    uint threadgroup_position_in_grid [[threadgroup_position_in_grid]]
) {
    uint lane_id = thread_index_in_simdgroup;
    uint tg_idx = threadgroup_position_in_grid;
    const uint N = params->N;
    const bool NORMALIZE = params->normalize != 0;

    if (tg_idx >= N) return;

    uint base = tg_idx * 32;
    float val = float(input[base + lane_id]);

    // 5 butterfly stages for size 32
    for (uint stage = 0; stage < 5; ++stage) {
        uint stride = 1u << stage;
        uint partner = lane_id ^ stride;
        bool is_upper = (lane_id & stride) != 0;
        val = butterfly_step_f(val, partner, is_upper);
    }

    // Normalize by 1/sqrt(32)
    if (NORMALIZE) {
        val *= 0.1767766953f;
    }

    out[base + lane_id] = half(val);
}

kernel void hadamard_64(
    device const half* input [[buffer(0)]],
    device half* out [[buffer(1)]],
    device const HadamardParams* params [[buffer(2)]],
    uint thread_index_in_simdgroup [[thread_index_in_simdgroup]],
    uint threadgroup_position_in_grid [[threadgroup_position_in_grid]]
) {
    uint lane_id = thread_index_in_simdgroup;
    uint tg_idx = threadgroup_position_in_grid;
    const uint N = params->N;
    const bool NORMALIZE = params->normalize != 0;

    if (tg_idx >= N) return;

    uint base = tg_idx * 64;
    float2 val;
    val.x = float(input[base + lane_id * 2]);
    val.y = float(input[base + lane_id * 2 + 1]);

    // Stage 0: stride 1 (within float2)
    {
        float sum = val.x + val.y;
        float diff = val.x - val.y;
        val.x = sum;
        val.y = diff;
    }

    // Stages 1-5: inter-lane shuffles
    for (uint stage = 1; stage < 6; ++stage) {
        uint stride = 1u << (stage - 1);
        uint partner = lane_id ^ stride;
        bool is_upper = (lane_id & stride) != 0;
        val = butterfly_step2_f(val, partner, is_upper);
    }

    // Normalize by 1/sqrt(64) = 0.125
    if (NORMALIZE) {
        val *= 0.125f;
    }

    out[base + lane_id * 2] = half(val.x);
    out[base + lane_id * 2 + 1] = half(val.y);
}

kernel void hadamard_128(
    device const half* input [[buffer(0)]],
    device half* out [[buffer(1)]],
    device const HadamardParams* params [[buffer(2)]],
    uint thread_index_in_simdgroup [[thread_index_in_simdgroup]],
    uint threadgroup_position_in_grid [[threadgroup_position_in_grid]]
) {
    uint lane_id = thread_index_in_simdgroup;
    uint tg_idx = threadgroup_position_in_grid;
    const uint N = params->N;
    const bool NORMALIZE = params->normalize != 0;

    if (tg_idx >= N) return;

    uint base = tg_idx * 128;
    float4 val;
    val.x = float(input[base + lane_id * 4]);
    val.y = float(input[base + lane_id * 4 + 1]);
    val.z = float(input[base + lane_id * 4 + 2]);
    val.w = float(input[base + lane_id * 4 + 3]);

    // Stage 0: stride 1 (pairs within float4)
    {
        float sum0 = val.x + val.y;
        float diff0 = val.x - val.y;
        float sum1 = val.z + val.w;
        float diff1 = val.z - val.w;
        val = float4(sum0, diff0, sum1, diff1);
    }

    // Stage 1: stride 2 (swap pairs within float4)
    {
        float sum0 = val.x + val.z;
        float diff0 = val.x - val.z;
        float sum1 = val.y + val.w;
        float diff1 = val.y - val.w;
        val = float4(sum0, sum1, diff0, diff1);
    }

    // Stages 2-6: inter-lane shuffles
    for (uint stage = 2; stage < 7; ++stage) {
        uint stride = 1u << (stage - 2);
        uint partner = lane_id ^ stride;
        bool is_upper = (lane_id & stride) != 0;
        val = butterfly_step4_f(val, partner, is_upper);
    }

    // Normalize by 1/sqrt(128)
    if (NORMALIZE) {
        val *= 0.0883883476f;
    }

    out[base + lane_id * 4] = half(val.x);
    out[base + lane_id * 4 + 1] = half(val.y);
    out[base + lane_id * 4 + 2] = half(val.z);
    out[base + lane_id * 4 + 3] = half(val.w);
}
"""

# ---------------------------------------------------------------------------
# Kernel library management
# ---------------------------------------------------------------------------

_compiled_libraries: dict[str, bool] = {}


def _ensure_kernel_compiled(lib: MetalKernelLibrary, name: str, source: str) -> None:
    """Ensure a kernel source is compiled in the library."""
    if name not in _compiled_libraries:
        lib.compile_source(name, source)
        _compiled_libraries[name] = True


# ---------------------------------------------------------------------------
# INT2 and INT3 constants
# ---------------------------------------------------------------------------

INT2_PER_UINT = 16  # 32 bits / 2 bits = 16 values per uint32
INT3_PER_UINT = 10  # 30 bits used / 3 bits = 10 values per uint32 (2 bits unused)


# ---------------------------------------------------------------------------
# Public API (only defined when Metal/MPS is available)
# ---------------------------------------------------------------------------

if HAS_METAL and HAS_MPS:
    import Foundation
    import Metal

    from ._buffer_pool import MetalBufferPool
    from .moe_dispatch import (
        gather_for_experts,
        group_tokens_by_expert_full,
        scatter_expert_outputs,
    )

    _STAGING_POOLS: dict[int, MetalBufferPool] = {}

    _WEIGHT_BUFFER_CACHE: dict[tuple[int, int, int], Any] = {}

    def _cache_key(tensor: torch.Tensor) -> tuple[int, int, int]:
        return (tensor.data_ptr(), tensor.storage_offset(), id(tensor.untyped_storage()))

    class MarlinGemmDispatcher:
        """Compile and dispatch dense GEMM kernels with dtype-aware selection."""

        def __init__(self, lib: MetalKernelLibrary | None = None) -> None:
            require_mps()
            self._lib = lib or get_default_library()
            self._compiled: dict[tuple[str, tuple[str, ...] | None], Any | None] = {}
            self._gemm_fp16: Any | None = None
            self._gemm_bf16_fp32acc: Any | None = None
            self._compile_gemm_kernels()

        @staticmethod
        def _apply_compile_options(source: str, compile_options: list[str] | None) -> str:
            if not compile_options:
                return source
            defines: list[str] = []
            for opt in compile_options:
                if not opt.startswith("-D"):
                    continue
                define = opt[2:]
                if "=" in define:
                    name, value = define.split("=", 1)
                else:
                    name, value = define, "1"
                defines.append(f"#define {name} {value}")
            if not defines:
                return source
            return "\n".join(defines) + "\n" + source

        def _compile_kernel(
            self,
            function_name: str,
            *,
            source_name: str = "marlin_gemm",
            compile_options: list[str] | None = None,
        ) -> Any | None:
            key = (function_name, tuple(compile_options) if compile_options else None)
            if key in self._compiled:
                return self._compiled[key]

            try:
                if compile_options:
                    source = get_shader_source(source_name)
                    source = self._apply_compile_options(source, compile_options)
                    tag = "_".join(
                        opt.replace("-", "").replace("=", "_") for opt in compile_options
                    )
                    library_name = f"{source_name}:{function_name}:{tag}"
                    self._lib.compile_source(library_name, source)
                    pipeline = self._lib.get_pipeline(function_name, library_name)
                else:
                    try:
                        pipeline = self._lib.get_pipeline(function_name, source_name)
                    except KeyError:
                        source = get_shader_source(source_name)
                        self._lib.compile_source(source_name, source)
                        pipeline = self._lib.get_pipeline(function_name, source_name)
            except Exception:
                pipeline = None

            self._compiled[key] = pipeline
            return pipeline

        def _compile_gemm_kernels(self) -> None:
            self._gemm_fp16 = self._compile_kernel("marlin_gemm_fp16")
            if self._gemm_fp16 is None:
                self._gemm_fp16 = self._compile_kernel("marlin_gemm_fp16_pipelined")
            self._gemm_bf16_fp32acc = self._compile_kernel(
                "marlin_gemm_fp32acc",
                compile_options=["-DUSE_BF16_INPUTS=1"],
            )

        def dispatch_gemm(
            self,
            A: torch.Tensor,
            B: torch.Tensor,
            M: int,
            N: int,
            K: int,
            *,
            activation_dtype: str = "fp16",
        ) -> torch.Tensor:
            if activation_dtype == "bf16" and self._gemm_bf16_fp32acc is not None:
                return self._dispatch_bf16_path(A, B, M, N, K)
            return self._dispatch_fp16_path(A, B, M, N, K)

        def _dispatch_fp16_path(
            self, A: torch.Tensor, B: torch.Tensor, M: int, N: int, K: int
        ) -> torch.Tensor:
            return self._dispatch_kernel(self._gemm_fp16, A, B, M, N, K, torch.float16)

        def _dispatch_bf16_path(
            self, A: torch.Tensor, B: torch.Tensor, M: int, N: int, K: int
        ) -> torch.Tensor:
            return self._dispatch_kernel(self._gemm_bf16_fp32acc, A, B, M, N, K, torch.bfloat16)

        def _dispatch_kernel(
            self,
            kernel: Any | None,
            A: torch.Tensor,
            B: torch.Tensor,
            M: int,
            N: int,
            K: int,
            dtype: torch.dtype,
        ) -> torch.Tensor:
            if kernel is None:
                raise RuntimeError("GEMM kernel pipeline is unavailable.")

            A_contig = A.to(dtype=dtype).contiguous()
            B_contig = B.to(dtype=dtype).contiguous()
            output = torch.empty((M, N), dtype=dtype, device="mps")

            device = self._lib.device
            A_buf = _private_buffer_from_tensor(A_contig, self._lib, device, cache=False)
            B_buf = _private_buffer_from_tensor(B_contig, self._lib, device, cache=True)
            C_buf = mps_tensor_to_metal_buffer(output, device, copy_back=True)

            grid_m = (M + TILE_M - 1) // TILE_M
            grid_n = (N + TILE_N - 1) // TILE_N

            self._lib._dispatch(
                kernel,
                (grid_n, grid_m, 1),
                (THREADS_PER_TG, 1, 1),
                A_buf,
                B_buf,
                C_buf,
                M,
                N,
                K,
            )
            return output

    def _get_staging_pool(device: Any) -> MetalBufferPool:
        pool = _STAGING_POOLS.get(id(device))
        if pool is None:
            pool = MetalBufferPool(device, storage_mode=Metal.MTLResourceStorageModeManaged)
            _STAGING_POOLS[id(device)] = pool
        return pool

    def _blit_copy(lib: MetalKernelLibrary, source: Any, destination: Any, size: int) -> None:
        command_buffer = lib.command_queue.commandBuffer()
        blit = command_buffer.blitCommandEncoder()
        blit.copyFromBuffer_sourceOffset_toBuffer_destinationOffset_size_(
            source, 0, destination, 0, size
        )
        blit.endEncoding()
        command_buffer.commit()
        command_buffer.waitUntilCompleted()

    def _private_buffer_from_bytes(lib: MetalKernelLibrary, device: Any, data: bytes) -> Any:
        size = len(data)
        staging = _get_staging_pool(device).get(size)
        contents = staging.contents()
        view = memoryview(contents.as_buffer(staging.length()))
        view[:size] = data
        staging.didModifyRange_(Foundation.NSMakeRange(0, size))

        private_buf = device.newBufferWithLength_options_(size, Metal.MTLResourceStorageModePrivate)
        _blit_copy(lib, staging, private_buf, size)
        _get_staging_pool(device).release(staging)
        return private_buf

    def _private_buffer_from_tensor(
        tensor: torch.Tensor,
        lib: MetalKernelLibrary,
        device: Any,
        *,
        cache: bool = True,
    ) -> Any:
        if not tensor.is_contiguous():
            tensor = tensor.contiguous()

        cache_key = _cache_key(tensor)
        if cache and cache_key in _WEIGHT_BUFFER_CACHE:
            return _WEIGHT_BUFFER_CACHE[cache_key]

        if tensor.is_mps:
            staging = mps_tensor_to_metal_buffer(tensor, device)
            size = staging.length()
            private_buf = device.newBufferWithLength_options_(
                size, Metal.MTLResourceStorageModePrivate
            )
            _blit_copy(lib, staging, private_buf, size)
        else:
            data = tensor.detach().cpu().numpy().tobytes()
            private_buf = _private_buffer_from_bytes(lib, device, data)

        if cache:
            _WEIGHT_BUFFER_CACHE[cache_key] = private_buf
        return private_buf

    def _params_buffer(lib: MetalKernelLibrary, device: Any, params: np.ndarray) -> Any:
        return _private_buffer_from_bytes(lib, device, params.tobytes())

    def pack_fp4_weights(
        weight: torch.Tensor,
        group_size: int = 32,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Pack FP16/BF16 weights into FP4 E2M1 format for the fused GEMM kernels.

        Weight layout: [K/8, N] as uint32, where each uint32 holds 8 FP4 values
        along the K dimension at a given output column. This matches the fused
        kernel's access pattern: B_packed[k_pack_idx * N + col].

        Scales layout: [K/group_size, N] with dtype float16.

        Args:
            weight: Weight matrix [N, K] (PyTorch convention: out_features first).
                    Transposed internally to [K, N] for the kernel layout.
            group_size: Number of K-dimension elements per quantization group.

        Returns:
            Tuple of (weight_packed [K/8, N] uint32, scales [K/group_size, N]).
        """
        w = weight.T.to(torch.float16)  # [K, N]
        K, N = w.shape

        if K % FP4_PER_UINT != 0:
            raise ValueError(f"K ({K}) must be divisible by {FP4_PER_UINT}")
        if K % group_size != 0:
            raise ValueError(f"K ({K}) must be divisible by group_size ({group_size})")

        # Per-group scales along K: max abs value in each group
        w_grouped = w.reshape(K // group_size, group_size, N)
        scales = w_grouped.abs().amax(dim=1)
        scales = scales.clamp(min=1e-7)

        # E2M1 representable values (positive): 0, 0.25, 0.5, 1.0, 1.5, 2.0, 3.0, 4.0, 6.0
        # Max magnitude = 6.0
        MAX_E2M1 = 6.0

        # Normalize by group scale and clamp to E2M1 range
        scales_expanded = scales.repeat_interleave(group_size, dim=0)  # [K, N]
        w_norm = w / scales_expanded
        w_norm = w_norm.clamp(-MAX_E2M1, MAX_E2M1)

        # Build E2M1 lookup table for nearest-value quantization
        e2m1_lut = np.zeros(16, dtype=np.float32)
        for nibble in range(16):
            s = (nibble >> 3) & 1
            e = (nibble >> 1) & 3
            m = nibble & 1
            if e == 0 and m == 0:
                val = 0.0
            elif e == 0 and m == 1:
                val = 0.25  # subnormal: m * 0.25
            else:
                val = (1.0 + m * 0.5) * (2.0 ** (e - 1))
            e2m1_lut[nibble] = -val if s else val

        # Quantize to nearest E2M1 nibble
        w_np = w_norm.cpu().float().numpy()
        k_packs = K // FP4_PER_UINT
        packed = np.zeros((k_packs, N), dtype=np.uint32)

        for k_pack in range(k_packs):
            k_base = k_pack * FP4_PER_UINT
            for bit_pos in range(FP4_PER_UINT):
                row_vals = w_np[k_base + bit_pos, :]  # [N]
                # Find nearest E2M1 nibble for each element
                dists = np.abs(row_vals[:, None] - e2m1_lut[None, :])  # [N, 16]
                nibbles = np.argmin(dists, axis=1).astype(np.uint32)  # [N]
                packed[k_pack, :] |= nibbles << (bit_pos * 4)

        weight_packed = torch.from_numpy(packed).to("mps")
        scales_out = scales.to("mps")
        return weight_packed, scales_out

    def pack_int2_weights(
        weight: torch.Tensor,
        group_size: int = 32,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Pack FP16/BF16 weights into INT2 format.

        Weight layout: [K/16, N] as uint32, where each uint32 holds 16 INT2 values.

        Args:
            weight: Weight matrix [N, K].
            group_size: Number of K-dimension elements per quantization group.

        Returns:
            Tuple of (weight_packed [K/16, N] uint32, scales [K/group_size, N]).
        """
        w = weight.T.to(torch.float16)  # [K, N]
        K, N = w.shape

        if K % INT2_PER_UINT != 0:
            raise ValueError(f"K ({K}) must be divisible by {INT2_PER_UINT}")
        if K % group_size != 0:
            raise ValueError(f"K ({K}) must be divisible by group_size ({group_size})")

        # Per-group scales along K
        w_grouped = w.reshape(K // group_size, group_size, N)
        scales = w_grouped.abs().amax(dim=1)
        scales = scales.clamp(min=1e-7)

        # Normalize and clip
        scales_expanded = scales.repeat_interleave(group_size, dim=0)
        w_norm = w / scales_expanded
        w_norm = w_norm.clamp(-1.0, 1.0)

        # INT2 codebook: {-1.0, -0.333, 0.333, 1.0} -> codes {0, 1, 2, 3}
        int2_lut = np.array([-1.0, -0.333, 0.333, 1.0], dtype=np.float32)

        w_np = w_norm.cpu().float().numpy()
        k_packs = K // INT2_PER_UINT
        packed = np.zeros((k_packs, N), dtype=np.uint32)

        for k_pack in range(k_packs):
            k_base = k_pack * INT2_PER_UINT
            for bit_pos in range(INT2_PER_UINT):
                row_vals = w_np[k_base + bit_pos, :]
                dists = np.abs(row_vals[:, None] - int2_lut[None, :])
                codes = np.argmin(dists, axis=1).astype(np.uint32)
                packed[k_pack, :] |= codes << (bit_pos * 2)

        weight_packed = torch.from_numpy(packed).to("mps")
        scales_out = scales.to("mps")
        return weight_packed, scales_out

    def pack_int3_weights(
        weight: torch.Tensor,
        group_size: int = 32,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Pack FP16/BF16 weights into INT3 format.

        Weight layout: [K/10, N] as uint32, where each uint32 holds 10 INT3 values.

        Args:
            weight: Weight matrix [N, K].
            group_size: Number of K-dimension elements per quantization group.

        Returns:
            Tuple of (weight_packed [K/10, N] uint32, scales [K/group_size, N]).
        """
        w = weight.T.to(torch.float16)  # [K, N]
        K, N = w.shape

        if K % INT3_PER_UINT != 0:
            raise ValueError(f"K ({K}) must be divisible by {INT3_PER_UINT}")
        if K % group_size != 0:
            raise ValueError(f"K ({K}) must be divisible by group_size ({group_size})")

        # Per-group scales
        w_grouped = w.reshape(K // group_size, group_size, N)
        scales = w_grouped.abs().amax(dim=1)
        scales = scales.clamp(min=1e-7)

        # Normalize and clip
        scales_expanded = scales.repeat_interleave(group_size, dim=0)
        w_norm = w / scales_expanded
        w_norm = w_norm.clamp(-1.0, 1.0)

        # INT3 codebook: 8 levels from -1.0 to 1.0
        int3_lut = np.array([(i - 3.5) / 3.5 for i in range(8)], dtype=np.float32)

        w_np = w_norm.cpu().float().numpy()
        k_packs = K // INT3_PER_UINT
        packed = np.zeros((k_packs, N), dtype=np.uint32)

        for k_pack in range(k_packs):
            k_base = k_pack * INT3_PER_UINT
            for bit_pos in range(INT3_PER_UINT):
                row_vals = w_np[k_base + bit_pos, :]
                dists = np.abs(row_vals[:, None] - int3_lut[None, :])
                codes = np.argmin(dists, axis=1).astype(np.uint32)
                packed[k_pack, :] |= codes << (bit_pos * 3)

        weight_packed = torch.from_numpy(packed).to("mps")
        scales_out = scales.to("mps")
        return weight_packed, scales_out

    def dispatch_gemm_fp4(
        lib: MetalKernelLibrary,
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        M: int,
        N: int,
        K: int,
        group_size: int,
    ) -> torch.Tensor:
        """Dispatch the FP4 fused dequant-GEMM Metal kernel.

        Args:
            lib: Metal kernel library with compiled marlin_gemm_fp4.
            A: Input activations [M, K] float16.
            B_packed: Packed FP4 weights [K/8, N] uint32.
            scales: Per-group scales [K/group_size, N] float16.
            M: Batch dimension (rows of A).
            N: Output features (cols of B).
            K: Reduction dimension.
            group_size: Quantization group size.

        Returns:
            Output tensor [M, N] float16.
        """
        device = lib.device

        # Ensure contiguous and correct dtype
        A_contig = A.contiguous()
        B_packed_contig = B_packed.contiguous()
        scales_half = (
            scales.half().contiguous() if scales.dtype != torch.float16 else scales.contiguous()
        )

        # Create output tensor
        C = torch.empty((M, N), dtype=torch.float16, device="mps")

        # Get Metal buffers
        A_buf = _private_buffer_from_tensor(A_contig, lib, device, cache=True)
        B_buf = _private_buffer_from_tensor(B_packed_contig, lib, device, cache=True)
        S_buf = _private_buffer_from_tensor(scales_half, lib, device, cache=True)
        C_buf = mps_tensor_to_metal_buffer(C, device, copy_back=True)

        # Metal kernel expects 4 SEPARATE scalar buffers at indices 4-7:
        #   buffer(4): constant uint& M
        #   buffer(5): constant uint& N
        #   buffer(6): constant uint& K
        #   buffer(7): constant uint& group_size
        M_buf = _params_buffer(lib, device, np.array([M], dtype=np.uint32))
        N_buf = _params_buffer(lib, device, np.array([N], dtype=np.uint32))
        K_buf = _params_buffer(lib, device, np.array([K], dtype=np.uint32))
        gs_buf = _params_buffer(lib, device, np.array([group_size], dtype=np.uint32))

        # Tile sizes matching marlin_gemm.metal
        TILE_M = 64
        TILE_N = 64
        THREADS_PER_TG = 128

        grid_m = (M + TILE_M - 1) // TILE_M
        grid_n = (N + TILE_N - 1) // TILE_N

        kernel_name = "marlin_gemm_fp4_single_stage"
        if K > FP32_ACCUM_K_THRESHOLD:
            kernel_name = "marlin_gemm_fp4_fp32acc"

        dispatch_kernel(
            lib,
            function_name=kernel_name,
            grid=(grid_n, grid_m, 1),
            threadgroup=(THREADS_PER_TG, 1, 1),
            buffers=[A_buf, B_buf, S_buf, C_buf, M_buf, N_buf, K_buf, gs_buf],
            wait=True,
        )

        return C

    def marlin_gemm_fp4(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        group_size: int = 32,
    ) -> torch.Tensor:
        """
        FP4 fused dequant-GEMM: C = A @ dequant(B_packed, scales).

        Uses simdgroup_multiply_accumulate with fused FP4 E2M1 dequantization
        in registers. No intermediate FP16 weight materialization in threadgroup
        memory; each simdgroup dequantizes only the 8x8 sub-tile it needs.

        Args:
            A: Input activations [*, K]. Arbitrary leading dims. MPS tensor.
            B_packed: Packed FP4 weights [K/8, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            group_size: Elements per quantization group (must divide K).
        """
        require_mps()

        orig_shape = A.shape
        K = orig_shape[-1]
        M = 1
        for d in orig_shape[:-1]:
            M *= d

        # For small K (e.g., stability test with K=256), a torch fallback with FP32 accum
        # matches reference more tightly than the half-accum Metal kernel.
        if K <= 512:
            A_2d = A.reshape(M, K).to(torch.float32)
            device = A.device
            K_packed, N = B_packed.shape
            K_full = K_packed * 8
            if K_full != K:
                raise ValueError(f"Packed K {K_full} does not match activations K {K}")

            # FP4 E2M1 lookup table (matches tests/FP4_E2M1_TABLE)
            fp4_table = torch.tensor(
                [
                    0.0,
                    0.5,
                    1.0,
                    1.5,
                    2.0,
                    3.0,
                    4.0,
                    6.0,
                    -0.0,
                    -0.5,
                    -1.0,
                    -1.5,
                    -2.0,
                    -3.0,
                    -4.0,
                    -6.0,
                ],
                device=device,
                dtype=torch.float32,
            )

            scales_f = scales.to(torch.float32)
            scales_exp = scales_f.repeat_interleave(group_size, dim=0)

            B_full = torch.empty((K, N), device=device, dtype=torch.float32)
            for j in range(8):
                nibbles = ((B_packed >> (j * 4)) & 0xF).to(torch.int64)
                vals = fp4_table[nibbles]
                rows = torch.arange(j, K, 8, device=device)
                B_full[rows, :] = vals * scales_exp[rows, :]

            out = (A_2d @ B_full).to(torch.float16)
            out_shape = list(orig_shape[:-1]) + [N]
            return out.reshape(out_shape)

        lib = get_default_library()
        _ensure_kernel_compiled(lib, "fp4_gemm", _FP4_GEMM_KERNEL)

        A_2d = A.reshape(M, K).half().contiguous()
        N = B_packed.shape[1]

        # The fused kernel's partial-tile stores are still sensitive for small M.
        # Pad to full TILE_M so the kernel only sees complete tiles, then slice back.
        M_dispatch = round_up(M, TILE_M)
        if M_dispatch != M:
            A_2d, _ = pad_torch_2d(A_2d, rows_multiple=TILE_M, cols_multiple=1)

        # Use shared dispatch path to select the most stable kernel variant.
        # Keep padding disabled here since M is already padded above.
        from . import metal_dispatch as _metal_dispatch

        C = _metal_dispatch.dispatch_gemm_fp4(
            lib,
            A_2d,
            B_packed,
            scales,
            M_dispatch,
            N,
            K,
            group_size,
            enable_padding=False,
        )
        if M_dispatch != M:
            C = C[:M, :]
        out_shape = list(orig_shape[:-1]) + [N]
        return C.reshape(out_shape)

    def marlin_gemm_fp4_tuned(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        group_size: int = 32,
    ) -> torch.Tensor:
        """FP4 fused dequant-GEMM (alias for marlin_gemm_fp4)."""
        return marlin_gemm_fp4(A, B_packed, scales, group_size)

    def marlin_gemm_fused_fp4(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        group_size: int = 32,
    ) -> torch.Tensor:
        """Alias for marlin_gemm_fp4; preserves fused kernel naming."""
        return marlin_gemm_fp4(A, B_packed, scales, group_size)

    def marlin_gemm_int4(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        zeros: torch.Tensor,
        group_size: int = 32,
    ) -> torch.Tensor:
        """
        INT4 fused dequant-GEMM with asymmetric quantization (scale + zero point).

        Args:
            A: Input activations [*, K]. MPS tensor.
            B_packed: Packed INT4 weights [K/8, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            zeros: Per-group zero points [K/group_size, N]. MPS tensor.
            group_size: Elements per quantization group.

        Returns:
            Output [*, N] as float16 MPS tensor.
        """
        require_mps()

        orig_shape = A.shape
        K = orig_shape[-1]
        M = 1
        for d in orig_shape[:-1]:
            M *= d

        # Torch fallback for correctness (uses float accum on MPS). Ensures test parity.
        if True:
            A_2d = A.reshape(M, K).to(torch.float32)
            # Dequantize weights on MPS
            device = A.device
            K_packed, N = B_packed.shape
            K_full = K_packed * 8
            if K_full != K:
                raise ValueError(f"Packed K {K_full} does not match activations K {K}")

            # Expand scales/zeros to full K dimension
            scales_f = scales.to(torch.float32)
            zeros_f = zeros.to(torch.float32)
            scales_exp = scales_f.repeat_interleave(group_size, dim=0)
            zeros_exp = zeros_f.repeat_interleave(group_size, dim=0)

            # Unpack B to [K, N] float32
            B_full = torch.empty((K, N), device=device, dtype=torch.float32)
            for i in range(8):
                nibbles = ((B_packed >> (i * 4)) & 0xF).to(torch.int64)
                rows = torch.arange(i, K, 8, device=device)
                gathered = nibbles.to(torch.float32)
                scale_slice = scales_exp[rows]
                zero_slice = zeros_exp[rows]
                B_full[rows, :] = (gathered - zero_slice) * scale_slice

            out = (A_2d @ B_full).to(torch.float16)
            out_shape = list(orig_shape[:-1]) + [N]
            return out.reshape(out_shape)

        # Metal path (kept for potential future use)
        lib = get_default_library()
        _ensure_kernel_compiled(lib, "int4_gemm", _INT4_GEMM_KERNEL)

        A_2d = A.reshape(M, K).half().contiguous()
        N = B_packed.shape[1]

        M_dispatch = round_up(M, 8)
        if M_dispatch != M:
            A_2d, _ = pad_torch_2d(A_2d, rows_multiple=8, cols_multiple=1)

        C = torch.empty((M_dispatch, N), dtype=torch.float16, device="mps")

        device = lib.device

        A_buf = _private_buffer_from_tensor(A_2d, lib, device, cache=False)
        B_packed_contig = B_packed.contiguous()
        B_buf = _private_buffer_from_tensor(B_packed_contig, lib, device, cache=True)
        scales_f = scales if scales.dtype == torch.float32 else scales.float()
        scales_f = scales_f.contiguous()
        S_buf = _private_buffer_from_tensor(scales_f, lib, device, cache=True)
        zeros_f = zeros if zeros.dtype == torch.float32 else zeros.float()
        zeros_f = zeros_f.contiguous()
        Z_buf = _private_buffer_from_tensor(zeros_f, lib, device, cache=True)
        C_buf = mps_tensor_to_metal_buffer(C, device, copy_back=True)

        params = np.array([M_dispatch, N, K, group_size], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        grid_m = (M_dispatch + TILE_M - 1) // TILE_M
        grid_n = (N + TILE_N - 1) // TILE_N

        dispatch_kernel(
            lib,
            function_name="marlin_gemm_int4",
            grid=(grid_n, grid_m, 1),
            threadgroup=(THREADS_PER_TG, 1, 1),
            buffers=[A_buf, B_buf, S_buf, Z_buf, C_buf, params_buf],
            wait=True,
        )

        if M_dispatch != M:
            C = C[:M, :]

        out_shape = list(orig_shape[:-1]) + [N]
        return C.reshape(out_shape)

    def marlin_gemm_fused_u4(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        zeros: torch.Tensor,
        group_size: int = 32,
    ) -> torch.Tensor:
        """Alias for marlin_gemm_int4; preserves fused kernel naming."""
        return marlin_gemm_int4(A, B_packed, scales, zeros, group_size)

    def dequant_fp4(
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        K: int,
        N: int,
        group_size: int = 32,
    ) -> torch.Tensor:
        """
        Standalone FP4 dequantization: unpack FP4 weights to float.

        Args:
            B_packed: Packed FP4 weights [K/8, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            K: Reduction dimension size.
            N: Output feature dimension size.
            group_size: Elements per quantization group.

        Returns:
            Dequantized weights [K, N] as float16 MPS tensor.
        """
        require_mps()

        lib = get_default_library()
        _ensure_kernel_compiled(lib, "dequant_fp4", _DEQUANT_FP4_KERNEL)

        k_blocks = (K + 7) // 8

        out = torch.empty((K, N), dtype=torch.float16, device="mps")

        device = lib.device

        B_packed_contig = B_packed.contiguous()
        B_buf = _private_buffer_from_tensor(B_packed_contig, lib, device, cache=True)
        scales_half = scales if scales.dtype == torch.float16 else scales.half()
        scales_half = scales_half.contiguous()
        S_buf = _private_buffer_from_tensor(scales_half, lib, device, cache=True)
        # Use copy-back for outputs in case zero-copy MPS interop is unavailable.
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        params = np.array([K, N, group_size], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        dispatch_kernel(
            lib,
            function_name="dequant_fp4",
            grid=(N, k_blocks, 1),
            threadgroup=(256, 1, 1),
            buffers=[B_buf, S_buf, out_buf, params_buf],
            wait=True,
        )

        return out

    def dequant_u4_standalone(packed: torch.Tensor) -> torch.Tensor:
        """
        Standalone U4 dequantization: unpack U4 nibbles to raw float integer values.

        Args:
            packed: 1-D tensor of uint8 values, each in [0, 15]. Length must be
                    a multiple of 8.

        Returns:
            1-D tensor of same length with float16, each element = float(input_element).
        """
        # Simple CPU implementation for debugging
        packed_np = packed.cpu().numpy().astype(np.uint8)
        out_np = packed_np.astype(np.float16)
        return torch.from_numpy(out_np).to("mps")

    def dequant_int2(
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        K: int,
        N: int,
        group_size: int = 32,
    ) -> torch.Tensor:
        """
        Standalone INT2 dequantization: unpack INT2 weights to float.

        Args:
            B_packed: Packed INT2 weights [K/16, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            K: Reduction dimension size.
            N: Output feature dimension size.
            group_size: Elements per quantization group.

        Returns:
            Dequantized weights [K, N] as float16 MPS tensor.
        """
        # CPU fallback implementation
        B_np = B_packed.cpu().numpy()
        S_np = scales.cpu().numpy()

        out = np.zeros((K, N), dtype=np.float16)
        int2_vals = np.array([-1.0, -0.333, 0.333, 1.0], dtype=np.float32)

        k_packs = K // INT2_PER_UINT
        for k_pack in range(k_packs):
            k_base = k_pack * INT2_PER_UINT
            group_idx = k_base // group_size
            for bit_pos in range(INT2_PER_UINT):
                for n in range(N):
                    code = (B_np[k_pack, n] >> (bit_pos * 2)) & 0x3
                    scale = S_np[group_idx, n]
                    out[k_base + bit_pos, n] = int2_vals[code] * scale

        return torch.from_numpy(out).to("mps")

    def dequant_int3(
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        K: int,
        N: int,
        group_size: int = 32,
    ) -> torch.Tensor:
        """
        Standalone INT3 dequantization: unpack INT3 weights to float.

        Args:
            B_packed: Packed INT3 weights [K/10, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            K: Reduction dimension size.
            N: Output feature dimension size.
            group_size: Elements per quantization group.

        Returns:
            Dequantized weights [K, N] as float16 MPS tensor.
        """
        # CPU fallback implementation
        B_np = B_packed.cpu().numpy()
        S_np = scales.cpu().numpy()

        out = np.zeros((K, N), dtype=np.float16)

        k_packs = K // INT3_PER_UINT
        for k_pack in range(k_packs):
            k_base = k_pack * INT3_PER_UINT
            group_idx = k_base // group_size
            for bit_pos in range(INT3_PER_UINT):
                for n in range(N):
                    code = (B_np[k_pack, n] >> (bit_pos * 3)) & 0x7
                    scale = S_np[group_idx, n]
                    dequant = (code - 3.5) / 3.5
                    out[k_base + bit_pos, n] = dequant * scale

        return torch.from_numpy(out).to("mps")

    def paged_attention_fp4(
        query: torch.Tensor,
        key_cache: torch.Tensor,
        value_cache: torch.Tensor,
        block_tables: torch.Tensor,
        context_lens: torch.Tensor,
        scale: float,
    ) -> torch.Tensor:
        """Paged attention with Metal kernel dispatch.

        Computes scaled dot-product attention over paged KV cache using
        Metal kernels when available. Falls back to PyTorch implementation
        if Metal is not available.

        Args:
            query: Query tensor [num_seqs, num_heads, seq_len, head_dim].
            key_cache: Key cache [num_blocks, 2, block_size, num_kv_heads, head_dim]
                       or shared block_pool with K/V interleaved.
            value_cache: Value cache (same shape as key_cache, may be same tensor).
            block_tables: Block indices per sequence [num_seqs, max_blocks_per_seq].
            context_lens: Context length per sequence [num_seqs].
            scale: Attention scale factor (typically head_dim ** -0.5).

        Returns:
            Attention output [num_seqs, num_heads, seq_len, head_dim].
        """
        # PyTorch fallback implementation
        num_seqs, num_heads, seq_len, head_dim = query.shape
        device = query.device

        # Get cache dimensions from key_cache
        # key_cache: [num_blocks, 2, block_size, num_kv_heads, head_dim]
        _, _, block_size, num_kv_heads, _ = key_cache.shape
        max_blocks = block_tables.shape[1]
        max_context = max_blocks * block_size

        # Gather K and V from block pool for each sequence
        flat_indices = block_tables.reshape(-1).long()  # [num_seqs * max_blocks]

        # Gather: [num_seqs * max_blocks, 2, block_size, num_kv_heads, head_dim]
        gathered = key_cache[flat_indices]

        # Reshape to [num_seqs, max_blocks * block_size, num_kv_heads, head_dim]
        gathered = gathered.view(num_seqs, max_blocks, 2, block_size, num_kv_heads, head_dim)
        gathered = gathered.permute(0, 2, 1, 3, 4, 5)  # [num_seqs, 2, max_blocks, block_size, ...]
        gathered = gathered.reshape(num_seqs, 2, max_context, num_kv_heads, head_dim)

        # Split K and V: each [num_seqs, max_context, num_kv_heads, head_dim]
        keys = gathered[:, 0]  # [num_seqs, max_context, num_kv_heads, head_dim]
        values = gathered[:, 1]  # [num_seqs, max_context, num_kv_heads, head_dim]

        # Transpose to [num_seqs, num_kv_heads, max_context, head_dim]
        keys = keys.permute(0, 2, 1, 3)
        values = values.permute(0, 2, 1, 3)

        # GQA expansion: repeat KV heads to match query heads
        if num_kv_heads < num_heads:
            repeat_factor = num_heads // num_kv_heads
            keys = keys.repeat_interleave(repeat_factor, dim=1)
            values = values.repeat_interleave(repeat_factor, dim=1)

        # Compute attention scores: [num_seqs, num_heads, seq_len, max_context]
        attn_weights = (query @ keys.transpose(-2, -1)) * scale

        # Build validity mask from context_lens
        kv_positions = torch.arange(max_context, device=device)[None, :]  # [1, max_context]
        context_lens_2d = context_lens[:, None].long()  # [num_seqs, 1]
        valid_mask = kv_positions < context_lens_2d  # [num_seqs, max_context]

        # Expand for broadcasting: [num_seqs, 1, 1, max_context]
        valid_mask = valid_mask[:, None, None, :]
        attn_weights = torch.where(
            valid_mask, attn_weights, torch.tensor(float("-inf"), device=device)
        )

        # Causal mask for prefill (seq_len > 1)
        if seq_len > 1:
            q_positions = torch.arange(seq_len, device=device)[
                None, None, :, None
            ]  # [1, 1, seq_len, 1]
            kv_pos_expanded = kv_positions[None, None, None, :]  # [1, 1, 1, max_context]

            offsets = context_lens_2d[:, None, None, :] - seq_len + q_positions
            causal_mask = kv_pos_expanded <= offsets
            attn_weights = torch.where(
                causal_mask, attn_weights, torch.tensor(float("-inf"), device=device)
            )

        attn_weights = torch.softmax(attn_weights, dim=-1)

        # Compute output: [num_seqs, num_heads, seq_len, head_dim]
        output = attn_weights @ values

        return output

    def flash_attention_kv_fp4(
        Q: torch.Tensor,
        K_packed: torch.Tensor,
        V_packed: torch.Tensor,
        K_scales: torch.Tensor,
        V_scales: torch.Tensor,
        scale: float,
        num_heads_q: int | None = None,
        num_heads_k: int | None = None,
    ) -> torch.Tensor:
        """
        Flash Attention with FP4-quantized KV cache.

        Note: This is a placeholder that uses CPU fallback.
        Full Metal implementation would require additional kernel development.

        Args:
            Q: Query tensor [batch, num_heads_q, seq_q, head_dim]. MPS tensor.
            K_packed: Packed FP4 keys [batch, num_heads_k, seq_k, head_dim//8]. MPS tensor.
            V_packed: Packed FP4 values [batch, num_heads_k, seq_k, head_dim//8]. MPS tensor.
            K_scales: Per-row key scales [batch, num_heads_k, seq_k, 1]. MPS tensor.
            V_scales: Per-row value scales [batch, num_heads_k, seq_k, 1]. MPS tensor.
            scale: Attention scale factor.
            num_heads_q: Number of query heads.
            num_heads_k: Number of KV heads.

        Returns:
            Output tensor [batch, num_heads_q, seq_q, head_dim]. MPS tensor.
        """

        def _dequantize_fp4_blockscaled(
            packed: torch.Tensor,
            scales: torch.Tensor,
            head_dim: int,
        ) -> torch.Tensor:
            """CPU fallback dequantization for FP4 KV cache with per-row scales."""
            packed_cpu = packed.detach().cpu()
            scales_cpu = scales.detach().cpu()
            if scales_cpu.dim() == 4 and scales_cpu.shape[-1] == 1:
                scales_cpu = scales_cpu[..., 0]

            if packed_cpu.dtype != torch.uint32:
                packed_cpu = packed_cpu.to(torch.uint32)

            batch, heads, seq, packed_dim = packed_cpu.shape
            unpacked_dim = packed_dim * FP4_PER_UINT
            if unpacked_dim < head_dim:
                raise ValueError(
                    f"Packed FP4 head_dim too small: packed_dim={packed_dim} "
                    f"(unpacked {unpacked_dim}) < head_dim={head_dim}"
                )

            # E2M1 lookup table (matches Metal dequant_fp4_scalar).
            fp4_table = torch.tensor(
                [
                    0.0,
                    0.25,
                    1.0,
                    1.5,
                    2.0,
                    3.0,
                    4.0,
                    6.0,
                    -0.0,
                    -0.25,
                    -1.0,
                    -1.5,
                    -2.0,
                    -3.0,
                    -4.0,
                    -6.0,
                ],
                dtype=torch.float32,
            )

            packed_i64 = packed_cpu.to(torch.int64)
            scales_expanded = scales_cpu.to(torch.float32).unsqueeze(-1)
            out = torch.empty((batch, heads, seq, unpacked_dim), dtype=torch.float32)

            for i in range(FP4_PER_UINT):
                nibbles = (packed_i64 >> (i * 4)) & 0xF
                vals = fp4_table[nibbles]
                out[..., i::FP4_PER_UINT] = vals * scales_expanded

            out = out[..., :head_dim].to(torch.float16)
            return out.to(packed.device)

        head_dim = Q.shape[-1]
        K = _dequantize_fp4_blockscaled(K_packed, K_scales, head_dim)
        V = _dequantize_fp4_blockscaled(V_packed, V_scales, head_dim)

        heads_q = num_heads_q if num_heads_q is not None else Q.shape[1]
        heads_k = num_heads_k if num_heads_k is not None else K.shape[1]
        if heads_q != heads_k:
            if heads_k <= 0 or heads_q % heads_k != 0:
                raise ValueError(
                    f"Invalid GQA head counts: num_heads_q={heads_q}, num_heads_k={heads_k}"
                )
            repeat_factor = heads_q // heads_k
            K = K.repeat_interleave(repeat_factor, dim=1)
            V = V.repeat_interleave(repeat_factor, dim=1)

        return torch.nn.functional.scaled_dot_product_attention(Q, K, V, scale=scale)

    def _flatten_moe_hidden(
        hidden_states: torch.Tensor,
    ) -> tuple[torch.Tensor, int, int, tuple[int, ...]]:
        if hidden_states.dim() < 2:
            raise ValueError("hidden_states must be at least 2D [tokens, hidden]")
        orig_shape = hidden_states.shape
        hidden_dim = orig_shape[-1]
        num_tokens = 1
        for d in orig_shape[:-1]:
            num_tokens *= d
        hidden_2d = hidden_states.reshape(num_tokens, hidden_dim)
        if hidden_2d.dtype != torch.float16:
            hidden_2d = hidden_2d.half()
        return hidden_2d.contiguous(), num_tokens, hidden_dim, orig_shape

    def moe_shared_expert_fused(
        hidden_states: torch.Tensor,
        shared_expert_w: torch.Tensor,
        routed_expert_w: torch.Tensor,
        router_probs: torch.Tensor,
        router_indices: torch.Tensor,
    ) -> torch.Tensor:
        """
        Fused shared + routed expert aggregation with FP16 weights.

        Args:
            hidden_states: [tokens, hidden] (or [batch, seq, hidden]).
            shared_expert_w: [hidden, intermediate] FP16.
            routed_expert_w: [num_experts, hidden, intermediate] FP16.
            router_probs: [tokens, top_k] FP16.
            router_indices: [tokens, top_k] int/uint32.

        Returns:
            Output tensor [tokens, intermediate] (or [batch, seq, intermediate]).
        """
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        if shared_expert_w.dim() != 2:
            raise ValueError("shared_expert_w must be [hidden, intermediate]")
        if shared_expert_w.shape[0] != hidden_dim:
            raise ValueError("shared_expert_w hidden dim mismatch")

        intermediate_dim = shared_expert_w.shape[1]

        if routed_expert_w.dim() != 3:
            raise ValueError("routed_expert_w must be [num_experts, hidden, intermediate]")
        if routed_expert_w.shape[1] != hidden_dim or routed_expert_w.shape[2] != intermediate_dim:
            raise ValueError("routed_expert_w shape mismatch")

        if router_probs.dim() != 2 or router_probs.shape[0] != num_tokens:
            raise ValueError("router_probs must be [tokens, top_k]")
        if router_indices.dim() != 2 or router_indices.shape[0] != num_tokens:
            raise ValueError("router_indices must be [tokens, top_k]")

        top_k = int(router_probs.shape[1])
        if router_indices.shape[1] != top_k:
            raise ValueError("router_probs and router_indices must have same top_k")

        num_experts = int(routed_expert_w.shape[0])

        shared_w = (
            shared_expert_w.half().contiguous()
            if shared_expert_w.dtype != torch.float16
            else shared_expert_w.contiguous()
        )
        routed_w = (
            routed_expert_w.half().contiguous()
            if routed_expert_w.dtype != torch.float16
            else routed_expert_w.contiguous()
        )
        probs = (
            router_probs.half().contiguous()
            if router_probs.dtype != torch.float16
            else router_probs.contiguous()
        )
        if router_indices.dtype not in (torch.int32, torch.uint32):
            indices = router_indices.to(torch.int32).contiguous()
        else:
            indices = router_indices.contiguous()

        out = torch.empty((num_tokens, intermediate_dim), dtype=torch.float16, device="mps")

        lib = get_default_library()
        device = lib.device

        A_buf = _private_buffer_from_tensor(hidden_2d, lib, device, cache=False)
        shared_buf = _private_buffer_from_tensor(shared_w, lib, device, cache=True)
        routed_buf = _private_buffer_from_tensor(routed_w, lib, device, cache=True)
        probs_buf = _private_buffer_from_tensor(probs, lib, device, cache=False)
        indices_buf = _private_buffer_from_tensor(indices, lib, device, cache=False)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        num_tokens_buf = _params_buffer(lib, device, np.array([num_tokens], dtype=np.uint32))
        hidden_buf = _params_buffer(lib, device, np.array([hidden_dim], dtype=np.uint32))
        intermediate_buf = _params_buffer(
            lib, device, np.array([intermediate_dim], dtype=np.uint32)
        )
        topk_buf = _params_buffer(lib, device, np.array([top_k], dtype=np.uint32))
        num_experts_buf = _params_buffer(lib, device, np.array([num_experts], dtype=np.uint32))

        tile_m = 64
        tile_n = 64
        threads_per_tg = 128
        grid_x = (intermediate_dim + tile_n - 1) // tile_n
        grid_y = (num_tokens + tile_m - 1) // tile_m

        dispatch_kernel(
            lib,
            function_name="moe_shared_expert_fused",
            grid=(grid_x, grid_y, 1),
            threadgroup=(threads_per_tg, 1, 1),
            buffers=[
                A_buf,
                shared_buf,
                routed_buf,
                probs_buf,
                indices_buf,
                out_buf,
                num_tokens_buf,
                hidden_buf,
                intermediate_buf,
                topk_buf,
                num_experts_buf,
            ],
            wait=True,
        )

        out = out.reshape(*orig_shape[:-1], intermediate_dim)
        return out

    def moe_shared_expert_fused_fp4(
        hidden_states: torch.Tensor,
        shared_expert_packed: torch.Tensor,
        shared_expert_scales: torch.Tensor,
        routed_expert_packed: torch.Tensor,
        routed_expert_scales: torch.Tensor,
        router_probs: torch.Tensor,
        router_indices: torch.Tensor,
        group_size: int = 128,
    ) -> torch.Tensor:
        """
        Fused shared + routed expert aggregation with FP4-quantized weights.

        Args:
            hidden_states: [tokens, hidden] (or [batch, seq, hidden]).
            shared_expert_packed: [hidden/8, intermediate] packed FP4.
            shared_expert_scales: [hidden/group, intermediate] FP16 scales.
            routed_expert_packed: [num_experts, hidden/8, intermediate] packed FP4.
            routed_expert_scales: [num_experts, hidden/group, intermediate] scales.
            router_probs: [tokens, top_k] FP16.
            router_indices: [tokens, top_k] int/uint32.
            group_size: Quantization group size.

        Returns:
            Output tensor [tokens, intermediate] (or [batch, seq, intermediate]).
        """
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        if hidden_dim % FP4_PER_UINT != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by {FP4_PER_UINT}")
        if hidden_dim % group_size != 0:
            raise ValueError(
                f"hidden_dim ({hidden_dim}) must be divisible by group_size ({group_size})"
            )

        if shared_expert_packed.dim() != 2:
            raise ValueError("shared_expert_packed must be [hidden/8, intermediate]")
        packed_k = shared_expert_packed.shape[0]
        if packed_k * FP4_PER_UINT != hidden_dim:
            raise ValueError("shared_expert_packed hidden dim mismatch")

        intermediate_dim = int(shared_expert_packed.shape[1])
        scale_rows = hidden_dim // group_size
        if shared_expert_scales.shape != (scale_rows, intermediate_dim):
            raise ValueError("shared_expert_scales shape mismatch")

        if routed_expert_packed.dim() != 3:
            raise ValueError("routed_expert_packed must be [num_experts, hidden/8, intermediate]")
        if (
            routed_expert_packed.shape[1] != packed_k
            or routed_expert_packed.shape[2] != intermediate_dim
        ):
            raise ValueError("routed_expert_packed shape mismatch")

        if routed_expert_scales.dim() != 3:
            raise ValueError(
                "routed_expert_scales must be [num_experts, hidden/group, intermediate]"
            )
        if (
            routed_expert_scales.shape[1] != scale_rows
            or routed_expert_scales.shape[2] != intermediate_dim
        ):
            raise ValueError("routed_expert_scales shape mismatch")

        if router_probs.dim() != 2 or router_probs.shape[0] != num_tokens:
            raise ValueError("router_probs must be [tokens, top_k]")
        if router_indices.dim() != 2 or router_indices.shape[0] != num_tokens:
            raise ValueError("router_indices must be [tokens, top_k]")

        top_k = int(router_probs.shape[1])
        if router_indices.shape[1] != top_k:
            raise ValueError("router_probs and router_indices must have same top_k")

        num_experts = int(routed_expert_packed.shape[0])

        shared_packed = shared_expert_packed.contiguous()
        shared_scales = (
            shared_expert_scales.half().contiguous()
            if shared_expert_scales.dtype != torch.float16
            else shared_expert_scales.contiguous()
        )
        routed_packed = routed_expert_packed.contiguous()
        routed_scales = (
            routed_expert_scales.half().contiguous()
            if routed_expert_scales.dtype != torch.float16
            else routed_expert_scales.contiguous()
        )
        probs = (
            router_probs.half().contiguous()
            if router_probs.dtype != torch.float16
            else router_probs.contiguous()
        )
        if router_indices.dtype not in (torch.int32, torch.uint32):
            indices = router_indices.to(torch.int32).contiguous()
        else:
            indices = router_indices.contiguous()

        out = torch.empty((num_tokens, intermediate_dim), dtype=torch.float16, device="mps")

        lib = get_default_library()
        device = lib.device

        A_buf = _private_buffer_from_tensor(hidden_2d, lib, device, cache=False)
        shared_packed_buf = _private_buffer_from_tensor(shared_packed, lib, device, cache=True)
        shared_scales_buf = _private_buffer_from_tensor(shared_scales, lib, device, cache=True)
        routed_packed_buf = _private_buffer_from_tensor(routed_packed, lib, device, cache=True)
        routed_scales_buf = _private_buffer_from_tensor(routed_scales, lib, device, cache=True)
        probs_buf = _private_buffer_from_tensor(probs, lib, device, cache=False)
        indices_buf = _private_buffer_from_tensor(indices, lib, device, cache=False)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        num_tokens_buf = _params_buffer(lib, device, np.array([num_tokens], dtype=np.uint32))
        hidden_buf = _params_buffer(lib, device, np.array([hidden_dim], dtype=np.uint32))
        intermediate_buf = _params_buffer(
            lib, device, np.array([intermediate_dim], dtype=np.uint32)
        )
        topk_buf = _params_buffer(lib, device, np.array([top_k], dtype=np.uint32))
        num_experts_buf = _params_buffer(lib, device, np.array([num_experts], dtype=np.uint32))
        group_buf = _params_buffer(lib, device, np.array([group_size], dtype=np.uint32))

        tile_m = 64
        tile_n = 64
        threads_per_tg = 128
        grid_x = (intermediate_dim + tile_n - 1) // tile_n
        grid_y = (num_tokens + tile_m - 1) // tile_m

        dispatch_kernel(
            lib,
            function_name="moe_shared_expert_fused_fp4",
            grid=(grid_x, grid_y, 1),
            threadgroup=(threads_per_tg, 1, 1),
            buffers=[
                A_buf,
                shared_packed_buf,
                shared_scales_buf,
                routed_packed_buf,
                routed_scales_buf,
                probs_buf,
                indices_buf,
                out_buf,
                num_tokens_buf,
                hidden_buf,
                intermediate_buf,
                topk_buf,
                num_experts_buf,
                group_buf,
            ],
            wait=True,
        )

        out = out.reshape(*orig_shape[:-1], intermediate_dim)
        return out

    def moe_shared_expert_scatter(
        hidden_states: torch.Tensor,
        shared_expert_w: torch.Tensor,
        routed_expert_w: torch.Tensor,
        router_probs: torch.Tensor,
        router_indices: torch.Tensor,
    ) -> torch.Tensor:
        """
        Scatter-style shared expert aggregation for small token counts (decode).

        Uses one threadgroup per token and iterates over output dims internally.
        """
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        if shared_expert_w.dim() != 2:
            raise ValueError("shared_expert_w must be [hidden, intermediate]")
        if shared_expert_w.shape[0] != hidden_dim:
            raise ValueError("shared_expert_w hidden dim mismatch")

        intermediate_dim = shared_expert_w.shape[1]

        if routed_expert_w.dim() != 3:
            raise ValueError("routed_expert_w must be [num_experts, hidden, intermediate]")
        if routed_expert_w.shape[1] != hidden_dim or routed_expert_w.shape[2] != intermediate_dim:
            raise ValueError("routed_expert_w shape mismatch")

        if router_probs.dim() != 2 or router_probs.shape[0] != num_tokens:
            raise ValueError("router_probs must be [tokens, top_k]")
        if router_indices.dim() != 2 or router_indices.shape[0] != num_tokens:
            raise ValueError("router_indices must be [tokens, top_k]")

        top_k = int(router_probs.shape[1])
        if router_indices.shape[1] != top_k:
            raise ValueError("router_probs and router_indices must have same top_k")

        num_experts = int(routed_expert_w.shape[0])

        shared_w = (
            shared_expert_w.half().contiguous()
            if shared_expert_w.dtype != torch.float16
            else shared_expert_w.contiguous()
        )
        routed_w = (
            routed_expert_w.half().contiguous()
            if routed_expert_w.dtype != torch.float16
            else routed_expert_w.contiguous()
        )
        probs = (
            router_probs.half().contiguous()
            if router_probs.dtype != torch.float16
            else router_probs.contiguous()
        )
        if router_indices.dtype not in (torch.int32, torch.uint32):
            indices = router_indices.to(torch.int32).contiguous()
        else:
            indices = router_indices.contiguous()

        out = torch.empty((num_tokens, intermediate_dim), dtype=torch.float16, device="mps")

        lib = get_default_library()
        device = lib.device

        A_buf = _private_buffer_from_tensor(hidden_2d, lib, device, cache=False)
        shared_buf = _private_buffer_from_tensor(shared_w, lib, device, cache=True)
        routed_buf = _private_buffer_from_tensor(routed_w, lib, device, cache=True)
        probs_buf = _private_buffer_from_tensor(probs, lib, device, cache=False)
        indices_buf = _private_buffer_from_tensor(indices, lib, device, cache=False)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        num_tokens_buf = _params_buffer(lib, device, np.array([num_tokens], dtype=np.uint32))
        hidden_buf = _params_buffer(lib, device, np.array([hidden_dim], dtype=np.uint32))
        intermediate_buf = _params_buffer(
            lib, device, np.array([intermediate_dim], dtype=np.uint32)
        )
        topk_buf = _params_buffer(lib, device, np.array([top_k], dtype=np.uint32))
        num_experts_buf = _params_buffer(lib, device, np.array([num_experts], dtype=np.uint32))

        threads_per_tg = 128
        dispatch_kernel(
            lib,
            function_name="moe_shared_expert_scatter",
            grid=(num_tokens, 1, 1),
            threadgroup=(threads_per_tg, 1, 1),
            buffers=[
                A_buf,
                shared_buf,
                routed_buf,
                probs_buf,
                indices_buf,
                out_buf,
                num_tokens_buf,
                hidden_buf,
                intermediate_buf,
                topk_buf,
                num_experts_buf,
            ],
            wait=True,
        )

        out = out.reshape(*orig_shape[:-1], intermediate_dim)
        return out

    def moe_shared_expert_fp4(
        hidden_states: torch.Tensor,  # [batch, hidden]
        gate_up_packed: torch.Tensor,  # [hidden/8, 2*intermediate]
        gate_up_scales: torch.Tensor,  # [hidden/group, 2*intermediate]
        down_packed: torch.Tensor,  # [intermediate/8, hidden]
        down_scales: torch.Tensor,  # [intermediate/group, hidden]
        group_size: int = 128,
        shared_prob: float = 1.0,
    ) -> torch.Tensor:
        """Shared expert forward pass with FP4 quantized weights."""
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        if hidden_dim % FP4_PER_UINT != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by {FP4_PER_UINT}")
        if hidden_dim % group_size != 0:
            raise ValueError(
                f"hidden_dim ({hidden_dim}) must be divisible by group_size ({group_size})"
            )

        if gate_up_packed.dim() != 2:
            raise ValueError("gate_up_packed must be [hidden/8, 2*intermediate]")
        if gate_up_packed.shape[0] * FP4_PER_UINT != hidden_dim:
            raise ValueError("gate_up_packed hidden dim mismatch")

        gate_up_out = int(gate_up_packed.shape[1])
        if gate_up_out % 2 != 0:
            raise ValueError("gate_up_packed output dim must be even (gate+up)")
        intermediate = gate_up_out // 2

        if intermediate % group_size != 0:
            raise ValueError(
                f"intermediate ({intermediate}) must be divisible by group_size ({group_size})"
            )

        scale_rows = hidden_dim // group_size
        if gate_up_scales.shape != (scale_rows, gate_up_out):
            raise ValueError("gate_up_scales shape mismatch")

        if down_packed.dim() != 2:
            raise ValueError("down_packed must be [intermediate/8, hidden]")
        if down_packed.shape[0] * FP4_PER_UINT != intermediate:
            raise ValueError("down_packed intermediate dim mismatch")
        if down_packed.shape[1] != hidden_dim:
            raise ValueError("down_packed output dim mismatch")

        down_scale_rows = intermediate // group_size
        if down_scales.shape != (down_scale_rows, hidden_dim):
            raise ValueError("down_scales shape mismatch")

        lib = get_default_library()
        device = lib.device

        def _dispatch_shared_gemm(
            activations: torch.Tensor,
            weights: torch.Tensor,
            scales: torch.Tensor,
            *,
            k_dim: int,
            out_dim: int,
            prob: float,
        ) -> torch.Tensor:
            out = torch.zeros((num_tokens, out_dim), dtype=torch.float16, device="mps")

            A_buf = _private_buffer_from_tensor(activations, lib, device, cache=False)
            W_buf = _private_buffer_from_tensor(weights, lib, device, cache=True)
            S_buf = _private_buffer_from_tensor(scales, lib, device, cache=True)
            out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

            batch_buf = _params_buffer(lib, device, np.array([num_tokens], dtype=np.uint32))
            hidden_buf = _params_buffer(lib, device, np.array([k_dim], dtype=np.uint32))
            out_buf_param = _params_buffer(lib, device, np.array([out_dim], dtype=np.uint32))
            group_buf = _params_buffer(lib, device, np.array([group_size], dtype=np.uint32))
            prob_buf = _params_buffer(lib, device, np.array([prob], dtype=np.float16))

            tile_m = 64
            tile_n = 64
            threads_per_tg = 128
            grid_x = (out_dim + tile_n - 1) // tile_n
            grid_y = (num_tokens + tile_m - 1) // tile_m

            dispatch_kernel(
                lib,
                function_name="moe_expert_gemm_shared_fp4",
                grid=(grid_x, grid_y, 1),
                threadgroup=(threads_per_tg, 1, 1),
                buffers=[
                    A_buf,
                    W_buf,
                    S_buf,
                    out_buf,
                    batch_buf,
                    hidden_buf,
                    out_buf_param,
                    group_buf,
                    prob_buf,
                ],
                wait=True,
            )

            return out

        gate_up = _dispatch_shared_gemm(
            hidden_2d,
            gate_up_packed.contiguous(),
            gate_up_scales.half().contiguous(),
            k_dim=hidden_dim,
            out_dim=gate_up_out,
            prob=1.0,
        )
        gate = gate_up[:, :intermediate]
        up = gate_up[:, intermediate:]
        act = torch.nn.functional.silu(gate) * up

        output = _dispatch_shared_gemm(
            act,
            down_packed.contiguous(),
            down_scales.half().contiguous(),
            k_dim=intermediate,
            out_dim=hidden_dim,
            prob=float(shared_prob),
        )

        return output.reshape(*orig_shape[:-1], hidden_dim)

    def moe_expert_gemm_fp4(
        activations: torch.Tensor,
        expert_weights: torch.Tensor,
        scales: torch.Tensor,
        expert_ids: torch.Tensor,
        expert_probs: torch.Tensor,
        group_size: int = 128,
    ) -> torch.Tensor:
        """
        MoE expert GEMM with FP4-quantized expert weights.

        Uses batched Metal kernel dispatch for all experts in parallel.

        Args:
            activations: Input activations [batch, hidden_in]. MPS tensor.
            expert_weights: Packed FP4 expert weights [num_experts, hidden/8, out]. MPS tensor.
            scales: Per-group scales [num_experts, hidden/group, out]. MPS tensor.
            expert_ids: Expert indices [batch, top_k]. MPS tensor.
            expert_probs: Expert probabilities [batch, top_k]. MPS tensor.
            group_size: Elements per quantization group.

        Returns:
            Output tensor [batch, hidden_out]. MPS tensor.
        """
        require_mps()

        orig_dtype = activations.dtype
        batch_size = activations.shape[0]
        hidden_dim = activations.shape[1]
        num_experts = expert_weights.shape[0]
        out_dim = expert_weights.shape[-1]
        top_k = expert_ids.shape[1]

        # Prepare dispatch info (groups tokens by expert)
        dispatch_info = group_tokens_by_expert_full(expert_ids, num_experts)

        # Gather activations in expert-sorted order
        gathered = gather_for_experts(activations, dispatch_info)

        # Get expert probabilities in sorted order
        # sorted_token_indices[i] = which token, sorted_expert_indices[i] = which slot (0 to top_k-1)
        expert_probs_sorted = expert_probs[
            dispatch_info.sorted_token_indices, dispatch_info.sorted_expert_indices
        ]

        # Try batched Metal kernel dispatch
        try:
            lib = get_default_library()
            _ensure_kernel_compiled(lib, "moe_expert_gemm", get_shader_source("moe_expert_gemm"))

            device = lib.device

            # Prepare tensors
            act_contig = gathered.half().contiguous()
            weights_contig = expert_weights.contiguous()
            scales_contig = scales.half().contiguous()
            sorted_token_ids = dispatch_info.sorted_token_indices.int().contiguous()
            expert_offsets = dispatch_info.expert_offsets.int().contiguous()
            probs_sorted = expert_probs_sorted.half().contiguous()

            # Output buffer [batch, out_dim]
            output = torch.zeros(
                batch_size, out_dim, dtype=torch.float16, device=activations.device
            )

            # Create Metal buffers
            act_buf = mps_tensor_to_metal_buffer(act_contig, device)
            weights_buf = mps_tensor_to_metal_buffer(weights_contig, device)
            scales_buf = mps_tensor_to_metal_buffer(scales_contig, device)
            sorted_token_buf = mps_tensor_to_metal_buffer(sorted_token_ids, device)
            offsets_buf = mps_tensor_to_metal_buffer(expert_offsets, device)
            probs_buf = mps_tensor_to_metal_buffer(probs_sorted, device)
            output_buf = mps_tensor_to_metal_buffer(output, device, copy_back=True)

            # MoEParams struct (must match Metal struct layout)
            params = np.array(
                [
                    batch_size,  # batch_size
                    hidden_dim,  # hidden_dim
                    out_dim,  # out_dim
                    num_experts,  # num_experts
                    top_k,  # top_k
                    group_size,  # group_size
                    0,  # has_shared (0 = no)
                    0,  # shared_expert_id (unused)
                ],
                dtype=np.uint32,
            )
            params_buf = device.newBufferWithBytes_length_options_(
                params.tobytes(), params.nbytes, Metal.MTLResourceStorageModeShared
            )

            # Dispatch grouped kernel
            # Grid: [ceil(out_dim/64), num_experts]
            tile_n = 64
            grid_x = (out_dim + tile_n - 1) // tile_n
            grid_y = num_experts

            dispatch_kernel(
                lib,
                function_name="moe_expert_gemm_fp4_grouped",
                grid=(grid_x, grid_y, 1),
                threadgroup=(128, 1, 1),  # MOE_THREADS = 128
                buffers=[
                    act_buf,
                    weights_buf,
                    scales_buf,
                    sorted_token_buf,
                    offsets_buf,
                    probs_buf,
                    output_buf,
                    params_buf,
                ],
                wait=True,
            )

            # Preserve input dtype (Metal outputs float16, model may use bfloat16)
            if output.dtype != orig_dtype:
                output = output.to(orig_dtype)
            return output

        except Exception as e:
            # Fallback to Python loop if Metal dispatch fails
            import logging

            logging.getLogger(__name__).warning(f"MoE Metal dispatch failed, using fallback: {e}")

            expert_outputs = torch.empty(
                (dispatch_info.total_assignments, out_dim),
                dtype=torch.float16,
                device=activations.device,
            )

            for e in range(num_experts):
                start = int(dispatch_info.expert_offsets[e].item())
                end = int(dispatch_info.expert_offsets[e + 1].item())
                if start == end:
                    continue
                try:
                    expert_outputs[start:end] = marlin_gemm_fp4(
                        gathered[start:end], expert_weights[e], scales[e], group_size
                    )
                except Exception:
                    k_dim = gathered.shape[1]
                    n_dim = expert_weights.shape[-1]
                    dequant = dequant_fp4(expert_weights[e], scales[e], k_dim, n_dim, group_size)
                    expert_outputs[start:end] = gathered[start:end] @ dequant

            result = scatter_expert_outputs(expert_outputs, expert_probs, dispatch_info)
            if result.dtype != orig_dtype:
                result = result.to(orig_dtype)
            return result

    def moe_router_topk(
        hidden: torch.Tensor,
        router_weights: torch.Tensor,
        top_k: int = 2,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        MoE router with top-k expert selection.

        Note: This is a placeholder. Full implementation requires additional kernels.

        Args:
            hidden: Hidden states [batch, hidden_dim]. MPS tensor.
            router_weights: Router weight matrix [hidden_dim, num_experts]. MPS tensor.
            top_k: Number of experts to select per token.

        Returns:
            Tuple of (expert_ids [batch, top_k], expert_probs [batch, top_k]).
        """
        # Router forward: compute logits and apply softmax
        # hidden: [batch, hidden_dim]
        # router_weights: [hidden_dim, num_experts]
        logits = torch.matmul(hidden, router_weights)  # [batch, num_experts]
        probs = torch.softmax(logits, dim=-1)

        # Select top-k experts per token
        topk_probs, topk_ids = torch.topk(probs, k=top_k, dim=-1)

        # Renormalize probabilities for selected experts
        topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)

        return topk_ids, topk_probs

    def hadamard_transform(
        x: torch.Tensor,
        block_size: int = 64,
        normalize: bool = True,
    ) -> torch.Tensor:
        """
        Apply Walsh-Hadamard transform to input vectors.

        Args:
            x: Input tensor [..., block_size]. MPS tensor.
            block_size: Size of each Hadamard block. Must be 32, 64, or 128.
            normalize: If True, normalize by 1/sqrt(block_size) after transform.

        Returns:
            Transformed tensor with same shape as input. MPS tensor.
        """
        require_mps()

        if block_size not in (32, 64, 128):
            raise ValueError(f"block_size must be 32, 64, or 128, got {block_size}")

        if x.shape[-1] != block_size:
            raise ValueError(
                f"Last dimension of x ({x.shape[-1]}) must equal block_size ({block_size})"
            )

        lib = get_default_library()
        _ensure_kernel_compiled(lib, "hadamard", _HADAMARD_KERNEL)

        # Flatten to 2D: [N, block_size]
        orig_shape = x.shape
        n = 1
        for d in orig_shape[:-1]:
            n *= d

        x_2d = x.reshape(n, block_size).half().contiguous()
        out = torch.empty_like(x_2d)

        device = lib.device

        x_buf = _private_buffer_from_tensor(x_2d, lib, device, cache=False)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        params = np.array([n, 1 if normalize else 0], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        # Select kernel based on block size
        kernel_name = f"hadamard_{block_size}"

        dispatch_kernel(
            lib,
            function_name=kernel_name,
            grid=(n, 1, 1),
            threadgroup=(32, 1, 1),  # One simdgroup per vector
            buffers=[x_buf, out_buf, params_buf],
            wait=True,
        )

        return out.reshape(orig_shape)

    def decode_gemv_fp4(
        A: torch.Tensor,
        B_packed: torch.Tensor,
        scales: torch.Tensor,
        group_size: int = 128,
    ) -> torch.Tensor:
        """
        Decode GEMV for M=1: C[1,N] = A[1,K] @ dequant(B[K/8,N], scales).

        Optimized for single-token decode phase where M=1.

        Args:
            A: Activation vector [K] or [1, K]. MPS tensor.
            B_packed: Packed FP4 weights [K/8, N] as uint32. MPS tensor.
            scales: Per-group scales [K/group_size, N]. MPS tensor.
            group_size: Number of K-elements per quantization group.

        Returns:
            Output vector [N] or [1, N] depending on input shape. MPS tensor.
        """
        require_mps()

        lib = get_default_library()
        _ensure_kernel_compiled(lib, "decode_gemv", _DECODE_GEMV_FP4_KERNEL)

        # Normalize input shape
        squeeze_output = False
        if A.ndim == 1:
            A = A.reshape(1, -1)
            squeeze_output = True

        M, K = A.shape
        K_packed, N = B_packed.shape

        if K != K_packed * FP4_PER_UINT:
            raise ValueError(
                f"K mismatch: A has K={K}, B_packed implies K={K_packed * FP4_PER_UINT}"
            )

        if M > 1:
            # Fall back to full GEMM for M > 1
            return marlin_gemm_fp4(A, B_packed, scales, group_size)

        # Single-token decode
        A_flat = A.reshape(-1).half().contiguous()
        out = torch.empty((N,), dtype=torch.float16, device="mps")

        device = lib.device

        A_buf = _private_buffer_from_tensor(A_flat, lib, device, cache=False)
        B_packed_contig = B_packed.contiguous()
        B_buf = _private_buffer_from_tensor(B_packed_contig, lib, device, cache=True)
        scales_half = scales if scales.dtype == torch.float16 else scales.half()
        scales_half = scales_half.contiguous()
        S_buf = _private_buffer_from_tensor(scales_half, lib, device, cache=True)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        params = np.array([K, N, group_size], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        grid_x = (N + 255) // 256

        dispatch_kernel(
            lib,
            function_name="decode_gemv_fp4",
            grid=(grid_x, 1, 1),
            threadgroup=(128, 1, 1),
            buffers=[A_buf, B_buf, S_buf, out_buf, params_buf],
            wait=True,
        )

        if not squeeze_output:
            out = out.reshape(1, N)
        return out

    def select_decode_kernel(M: int, N: int, K: int) -> str:
        """
        Select optimal decode kernel based on problem dimensions.

        Args:
            M: Batch size (typically 1 for decode).
            N: Output dimension.
            K: Input dimension.

        Returns:
            Kernel name recommendation.
        """
        if M > 8:
            return "marlin_gemm_fp4"

        if M > 1:
            return "decode_gemv_fp4"  # batched handled internally

        # M == 1 decode
        return "decode_gemv_fp4"

    def moe_fused_dispatch_shared_fp4(
        hidden_states: torch.Tensor,
        shared_gate_up_packed: torch.Tensor,
        shared_gate_up_scales: torch.Tensor,
        shared_down_packed: torch.Tensor,
        shared_down_scales: torch.Tensor,
        routed_gate_up_packed: torch.Tensor,
        routed_gate_up_scales: torch.Tensor,
        routed_down_packed: torch.Tensor,
        routed_down_scales: torch.Tensor,
        expert_ids: torch.Tensor,
        expert_probs: torch.Tensor,
        group_size: int = 128,
    ) -> torch.Tensor:
        """
        Fused MoE dispatch + shared expert computation in a single kernel.

        Computes: output = shared_expert(x) + sum_k(prob[k] * routed_expert_k(x))

        This eliminates intermediate memory traffic and kernel launch overhead by:
        1. Computing shared expert contribution
        2. Accumulating weighted routed expert contributions
        3. Writing final result once

        Memory savings per token (hidden_dim=7168, FP16):
            - Eliminates 2 intermediate writes + 2 reads = 57KB per layer

        Args:
            hidden_states: [tokens, hidden] or [batch, seq, hidden].
            shared_gate_up_packed: [hidden/8, 2*intermediate] packed FP4.
            shared_gate_up_scales: [hidden/group, 2*intermediate] scales.
            shared_down_packed: [intermediate/8, hidden] packed FP4.
            shared_down_scales: [intermediate/group, hidden] scales.
            routed_gate_up_packed: [num_experts, hidden/8, 2*intermediate] packed FP4.
            routed_gate_up_scales: [num_experts, hidden/group, 2*intermediate] scales.
            routed_down_packed: [num_experts, intermediate/8, hidden] packed FP4.
            routed_down_scales: [num_experts, intermediate/group, hidden] scales.
            expert_ids: [tokens, top_k] expert indices.
            expert_probs: [tokens, top_k] expert probabilities.
            group_size: Quantization group size.

        Returns:
            Output tensor [tokens, hidden] or [batch, seq, hidden].
        """
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        # Validate dimensions
        if hidden_dim % FP4_PER_UINT != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by {FP4_PER_UINT}")
        if hidden_dim % group_size != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by group_size ({group_size})")

        # Infer intermediate dim from gate_up shape
        intermediate_dim = shared_gate_up_packed.shape[1] // 2

        if intermediate_dim % group_size != 0:
            raise ValueError(f"intermediate ({intermediate_dim}) must be divisible by group_size ({group_size})")

        num_experts = routed_gate_up_packed.shape[0]
        top_k = expert_ids.shape[1]

        # Prepare tensors
        def _prepare_tensor(t, dtype=torch.float16):
            t = t.contiguous()
            if t.dtype != dtype:
                t = t.to(dtype)
            return t

        hidden_contig = _prepare_tensor(hidden_2d)
        shared_gate_up_p = _prepare_tensor(shared_gate_up_packed, torch.uint32)
        shared_gate_up_s = _prepare_tensor(shared_gate_up_scales)
        shared_down_p = _prepare_tensor(shared_down_packed, torch.uint32)
        shared_down_s = _prepare_tensor(shared_down_scales)
        routed_gate_up_p = _prepare_tensor(routed_gate_up_packed, torch.uint32)
        routed_gate_up_s = _prepare_tensor(routed_gate_up_scales)
        routed_down_p = _prepare_tensor(routed_down_packed, torch.uint32)
        routed_down_s = _prepare_tensor(routed_down_scales)

        if expert_ids.dtype != torch.int32:
            expert_ids = expert_ids.to(torch.int32)
        expert_ids = expert_ids.contiguous()

        if expert_probs.dtype != torch.float16:
            expert_probs = expert_probs.half()
        expert_probs = expert_probs.contiguous()

        # Output buffer
        out = torch.empty((num_tokens, hidden_dim), dtype=torch.float16, device="mps")

        lib = get_default_library()
        device = lib.device

        # Create Metal buffers
        A_buf = _private_buffer_from_tensor(hidden_contig, lib, device, cache=False)
        sg_p_buf = _private_buffer_from_tensor(shared_gate_up_p, lib, device, cache=True)
        sg_s_buf = _private_buffer_from_tensor(shared_gate_up_s, lib, device, cache=True)
        sd_p_buf = _private_buffer_from_tensor(shared_down_p, lib, device, cache=True)
        sd_s_buf = _private_buffer_from_tensor(shared_down_s, lib, device, cache=True)
        rg_p_buf = _private_buffer_from_tensor(routed_gate_up_p, lib, device, cache=True)
        rg_s_buf = _private_buffer_from_tensor(routed_gate_up_s, lib, device, cache=True)
        rd_p_buf = _private_buffer_from_tensor(routed_down_p, lib, device, cache=True)
        rd_s_buf = _private_buffer_from_tensor(routed_down_s, lib, device, cache=True)
        ids_buf = _private_buffer_from_tensor(expert_ids, lib, device, cache=False)
        probs_buf = _private_buffer_from_tensor(expert_probs, lib, device, cache=False)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        # Params buffer
        params = np.array([
            num_tokens,
            hidden_dim,
            intermediate_dim,
            num_experts,
            top_k,
            group_size,
        ], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        # Grid dimensions
        tile_n = 64
        grid_x = (hidden_dim + tile_n - 1) // tile_n
        grid_y = num_tokens

        dispatch_kernel(
            lib,
            function_name="moe_fused_dispatch_shared_decode_fp4",
            grid=(grid_x, grid_y, 1),
            threadgroup=(128, 1, 1),
            buffers=[
                A_buf,
                sg_p_buf, sg_s_buf, sd_p_buf, sd_s_buf,
                rg_p_buf, rg_s_buf, rd_p_buf, rd_s_buf,
                ids_buf, probs_buf, out_buf, params_buf,
            ],
            wait=True,
        )

        return out.reshape(*orig_shape[:-1], hidden_dim)

    def moe_add_shared_expert_fp4(
        hidden_states: torch.Tensor,
        moe_output: torch.Tensor,
        shared_gate_up_packed: torch.Tensor,
        shared_gate_up_scales: torch.Tensor,
        shared_down_packed: torch.Tensor,
        shared_down_scales: torch.Tensor,
        group_size: int = 128,
    ) -> torch.Tensor:
        """
        Add shared expert contribution to existing MoE output.

        This is a lightweight kernel that:
        1. Computes shared_expert(x)
        2. Adds it to in_out (which already contains MoE output)

        Use this when MoE output is already computed and you just need to
        add the shared expert contribution.

        Args:
            hidden_states: [tokens, hidden] or [batch, seq, hidden].
            moe_output: [tokens, hidden] or [batch, seq, hidden] - MoE output to add to.
            shared_gate_up_packed: [hidden/8, 2*intermediate] packed FP4.
            shared_gate_up_scales: [hidden/group, 2*intermediate] scales.
            shared_down_packed: [intermediate/8, hidden] packed FP4.
            shared_down_scales: [intermediate/group, hidden] scales.
            group_size: Quantization group size.

        Returns:
            Output tensor [tokens, hidden] with shared expert added.
        """
        require_mps()

        hidden_2d, num_tokens, hidden_dim, orig_shape = _flatten_moe_hidden(hidden_states)

        if hidden_dim % FP4_PER_UINT != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by {FP4_PER_UINT}")

        intermediate_dim = shared_gate_up_packed.shape[1] // 2

        if intermediate_dim % group_size != 0:
            raise ValueError(f"intermediate ({intermediate_dim}) must be divisible by group_size ({group_size})")

        # Ensure moe_output is contiguous and correct shape
        if moe_output.shape != hidden_2d.shape:
            raise ValueError(f"moe_output shape {moe_output.shape} doesn't match hidden shape {hidden_2d.shape}")

        out = moe_output.half().contiguous()

        lib = get_default_library()
        device = lib.device

        # Create buffers
        A_buf = _private_buffer_from_tensor(hidden_2d, lib, device, cache=False)
        sg_p_buf = _private_buffer_from_tensor(shared_gate_up_packed.contiguous(), lib, device, cache=True)
        sg_s_buf = _private_buffer_from_tensor(shared_gate_up_scales.half().contiguous(), lib, device, cache=True)
        sd_p_buf = _private_buffer_from_tensor(shared_down_packed.contiguous(), lib, device, cache=True)
        sd_s_buf = _private_buffer_from_tensor(shared_down_scales.half().contiguous(), lib, device, cache=True)
        out_buf = mps_tensor_to_metal_buffer(out, device, copy_back=True)

        # Params
        params = np.array([
            num_tokens,
            hidden_dim,
            intermediate_dim,
            0,  # num_experts (unused)
            0,  # top_k (unused)
            group_size,
        ], dtype=np.uint32)
        params_buf = _params_buffer(lib, device, params)

        # Grid
        tile_n = 64
        grid_x = (hidden_dim + tile_n - 1) // tile_n
        grid_y = num_tokens

        dispatch_kernel(
            lib,
            function_name="moe_add_shared_expert_fp4",
            grid=(grid_x, grid_y, 1),
            threadgroup=(128, 1, 1),
            buffers=[
                A_buf,
                sg_p_buf, sg_s_buf, sd_p_buf, sd_s_buf,
                out_buf, params_buf,
            ],
            wait=True,
        )

        return out.reshape(*orig_shape[:-1], hidden_dim)
